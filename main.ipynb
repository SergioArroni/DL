{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023 \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETX = \"./data/prep/HotelReservationsPreparedCleanX.csv\"\n",
    "DATASETY = \"./data/prep/HotelReservationsY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(DATASETX)\n",
    "df_y = pd.read_csv(DATASETY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  \n",
       "0                    -0.6  \n",
       "1                    -1.0  \n",
       "2                    -1.0  \n",
       "3                    -1.0  \n",
       "4                    -1.0  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booking_status\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               0\n",
       "4               1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  booking_status  \n",
       "0                    -0.6               1  \n",
       "1                    -1.0               1  \n",
       "2                    -1.0               1  \n",
       "3                    -1.0               0  \n",
       "4                    -1.0               1  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_x, df_y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.665914</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.677778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.674944</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.647407</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.268623</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.576667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.503386</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.575444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "16377          -0.5            -1.0             -1.000000          -0.764706   \n",
       "24639           0.0            -1.0             -0.428571          -0.764706   \n",
       "21974           0.0            -1.0             -0.428571          -1.000000   \n",
       "9205            0.0            -0.8             -1.000000          -0.647059   \n",
       "33303           0.0            -1.0             -0.428571          -0.411765   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "16377                0.0                        -1.0                 0.0   \n",
       "24639               -1.0                        -1.0                 0.0   \n",
       "21974               -1.0                         1.0                 1.0   \n",
       "9205                -1.0                        -1.0                 0.0   \n",
       "33303               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "16377  -0.665914       0.454545      0.133333                 -1.0   \n",
       "24639  -0.674944      -0.454545      0.466667                  0.0   \n",
       "21974  -0.268623      -0.090909      0.666667                  0.0   \n",
       "9205   -0.503386      -0.454545      0.733333                  0.0   \n",
       "33303  -0.399549       0.090909      0.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "16377            -1.0                          -1.0   \n",
       "24639            -1.0                          -1.0   \n",
       "21974            -1.0                          -1.0   \n",
       "9205             -1.0                          -1.0   \n",
       "33303            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "16377                                  -1.0           -0.677778   \n",
       "24639                                  -1.0           -0.647407   \n",
       "21974                                  -1.0           -0.576667   \n",
       "9205                                   -1.0           -0.643333   \n",
       "33303                                  -1.0           -0.575444   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "16377                    -1.0               1  \n",
       "24639                    -0.6               1  \n",
       "21974                    -0.6               0  \n",
       "9205                     -0.6               1  \n",
       "33303                    -0.6               1  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df, random_state=seed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, temp_set = train_test_split(df, train_size=TRAIN_SIZE, random_state=seed)\n",
    "\n",
    "validation_set, test_set = train_test_split(temp_set, train_size=VALIDATION_SIZE / (VALIDATION_SIZE + TEST_SIZE), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('booking_status', axis=1)\n",
    "y_train = train_set['booking_status']\n",
    "\n",
    "X_val = validation_set.drop('booking_status', axis=1)\n",
    "y_val = validation_set['booking_status']\n",
    "\n",
    "X_test = test_set.drop('booking_status', axis=1)\n",
    "y_test = test_set['booking_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.462754</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.663333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.602709</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.304259</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.663148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.688488</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.740778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31249</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.128668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.552963</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "25003          -0.5            -1.0             -1.000000          -0.764706   \n",
       "34921          -0.5            -0.6             -0.714286          -0.529412   \n",
       "13102           0.0            -1.0             -1.000000          -0.647059   \n",
       "4487            0.0            -1.0             -1.000000          -0.647059   \n",
       "31249           0.5            -1.0             -1.000000          -0.529412   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "25003               -1.0                        -1.0                 0.0   \n",
       "34921               -1.0                        -1.0                -1.0   \n",
       "13102               -1.0                        -1.0                 1.0   \n",
       "4487                -1.0                        -1.0                 0.0   \n",
       "31249               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "25003  -0.462754      -0.454545     -0.066667                  0.0   \n",
       "34921  -0.602709      -0.272727      0.000000                  0.0   \n",
       "13102  -0.399549      -0.090909     -0.533333                  0.0   \n",
       "4487   -0.688488       0.636364      0.600000                  0.0   \n",
       "31249  -0.128668       1.000000      0.800000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "25003            -1.0                          -1.0   \n",
       "34921            -1.0                          -1.0   \n",
       "13102            -1.0                          -1.0   \n",
       "4487             -1.0                          -1.0   \n",
       "31249            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "25003                                  -1.0           -0.663333   \n",
       "34921                                  -1.0           -0.304259   \n",
       "13102                                  -1.0           -0.663148   \n",
       "4487                                   -1.0           -0.740778   \n",
       "31249                                  -1.0           -0.552963   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "25003                    -1.0               1  \n",
       "34921                    -0.6               0  \n",
       "13102                    -1.0               0  \n",
       "4487                     -1.0               0  \n",
       "31249                    -0.6               0  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23403</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.673148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12427</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.959368</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557562</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.376667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.729120</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.496667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.092551</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.745185</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "23403           0.0            -1.0             -1.000000          -0.882353   \n",
       "12427          -0.5            -1.0             -0.714286          -1.000000   \n",
       "8230            0.5            -1.0             -1.000000          -0.764706   \n",
       "15344           0.0            -1.0             -1.000000          -0.647059   \n",
       "35059           0.0            -1.0             -1.000000          -0.529412   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "23403               -1.0                        -1.0                -1.0   \n",
       "12427                1.0                        -1.0                 0.0   \n",
       "8230                -1.0                        -1.0                 1.0   \n",
       "15344               -1.0                        -1.0                 0.0   \n",
       "35059               -1.0                        -1.0                 0.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "23403  -1.000000      -1.000000     -0.600000                  0.0   \n",
       "12427  -0.959368      -0.636364     -0.600000                  0.0   \n",
       "8230   -0.557562      -0.272727      0.066667                  0.0   \n",
       "15344  -0.729120       0.454545      0.733333                  0.0   \n",
       "35059  -0.092551       0.818182     -1.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "23403            -1.0                          -1.0   \n",
       "12427            -1.0                          -1.0   \n",
       "8230             -1.0                          -1.0   \n",
       "15344            -1.0                          -1.0   \n",
       "35059            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "23403                                  -1.0           -0.673148   \n",
       "12427                                  -1.0           -0.666667   \n",
       "8230                                   -1.0           -0.376667   \n",
       "15344                                  -1.0           -0.496667   \n",
       "35059                                  -1.0           -0.745185   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "23403                    -1.0               1  \n",
       "12427                    -0.6               1  \n",
       "8230                     -1.0               0  \n",
       "15344                    -1.0               0  \n",
       "35059                     0.2               1  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.855530</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33371</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.720090</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.610000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.562077</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.759333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.720090</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.820000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.760722</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "20955           0.0            -1.0             -0.714286          -0.764706   \n",
       "33371           0.0            -1.0             -0.428571          -0.882353   \n",
       "22008           0.0            -1.0             -0.714286          -0.647059   \n",
       "35320           0.0            -1.0             -0.714286          -0.882353   \n",
       "25986           0.0            -1.0             -0.714286          -0.882353   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "20955               -1.0                        -1.0                 0.0   \n",
       "33371               -1.0                        -1.0                 0.0   \n",
       "22008               -1.0                        -1.0                 0.0   \n",
       "35320                1.0                        -1.0                 0.0   \n",
       "25986               -1.0                        -1.0                 0.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "20955  -0.855530       0.636364      0.200000                  0.0   \n",
       "33371  -0.720090       0.090909      0.066667                  0.0   \n",
       "22008  -0.562077      -0.636364      0.533333                  0.0   \n",
       "35320  -0.720090      -0.818182     -0.066667                  0.0   \n",
       "25986  -0.760722       0.636364      0.866667                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "20955            -1.0                          -1.0   \n",
       "33371            -1.0                          -1.0   \n",
       "22008            -1.0                          -1.0   \n",
       "35320            -1.0                          -1.0   \n",
       "25986            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "20955                                  -1.0           -0.643333   \n",
       "33371                                  -1.0           -0.610000   \n",
       "22008                                  -1.0           -0.759333   \n",
       "35320                                  -1.0           -0.820000   \n",
       "25986                                  -1.0           -0.603333   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "20955                    -0.6               1  \n",
       "33371                    -0.6               1  \n",
       "22008                    -0.6               1  \n",
       "35320                    -1.0               1  \n",
       "25986                    -0.6               0  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (29016, 16)\n",
      "Test:  (3627, 16)\n",
      "Validation:  (3627, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: \", X_train.shape)\n",
    "print(\"Test: \", X_test.shape)\n",
    "print(\"Validation: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = X_train.shape[1]\n",
    "OUTPUTS = 1\n",
    "NUM_TRAINING_EXAMPLES = int(round(X_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int (round (X_val.shape[0]/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.01\n",
    "batch_size = 512\n",
    "tasa_dropout = 0.4\n",
    "n_neurons_per_hlayer = [1024, 512, 256, 124, 62, 31, 16, 8, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"DeepFeedforward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepFeedforward\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_77 (Dense)            (None, 1024)              16384     \n",
      "                                                                 \n",
      " batch_normalization_70 (Ba  (None, 1024)              4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_70 (Activation)  (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 512)               524288    \n",
      "                                                                 \n",
      " batch_normalization_71 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_71 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 256)               131072    \n",
      "                                                                 \n",
      " batch_normalization_72 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_72 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 124)               31744     \n",
      "                                                                 \n",
      " batch_normalization_73 (Ba  (None, 124)               496       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_73 (Activation)  (None, 124)               0         \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 124)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 62)                7688      \n",
      "                                                                 \n",
      " batch_normalization_74 (Ba  (None, 62)                248       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_74 (Activation)  (None, 62)                0         \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 62)                0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 31)                1922      \n",
      "                                                                 \n",
      " batch_normalization_75 (Ba  (None, 31)                124       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_75 (Activation)  (None, 31)                0         \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 31)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                496       \n",
      "                                                                 \n",
      " batch_normalization_76 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_76 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 8)                 128       \n",
      "                                                                 \n",
      " batch_normalization_77 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_77 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 4)                 32        \n",
      "                                                                 \n",
      " batch_normalization_78 (Ba  (None, 4)                 16        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_78 (Activation)  (None, 4)                 0         \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 2)                 8         \n",
      "                                                                 \n",
      " batch_normalization_79 (Ba  (None, 2)                 8         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 721921 (2.75 MB)\n",
      "Trainable params: 717843 (2.74 MB)\n",
      "Non-trainable params: 4078 (15.93 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.InputLayer(input_shape=(INPUTS,), batch_size=None))\n",
    "\n",
    "for neurons in n_neurons_per_hlayer:\n",
    "  model.add(keras.layers.Dense(neurons, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "  model.add(keras.layers.BatchNormalization())\n",
    "  model.add(keras.layers.Activation(\"elu\"))\n",
    "  model.add(tf.keras.layers.Dropout(tasa_dropout))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('model.hdf5', monitor='val_binary_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_binary_accuracy', factor=0.1, patience=45, min_lr=0.0001, verbose=1)\n",
    "early_stop = EarlyStopping('val_binary_accuracy', patience=101, verbose=1)\n",
    "callbacks = [model_checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=lr)\n",
    "#opt = SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"binary_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - ETA: 0s - loss: 0.6117 - binary_accuracy: 0.6703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.73284, saving model to model.hdf5\n",
      "57/57 [==============================] - 7s 54ms/step - loss: 0.6117 - binary_accuracy: 0.6703 - val_loss: 2.2553 - val_binary_accuracy: 0.7328\n",
      "Epoch 2/1000\n",
      " 1/57 [..............................] - ETA: 2s - loss: 0.5248 - binary_accuracy: 0.7520"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/57 [============================>.] - ETA: 0s - loss: 0.5282 - binary_accuracy: 0.7366\n",
      "Epoch 2: val_binary_accuracy improved from 0.73284 to 0.79487, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.5282 - binary_accuracy: 0.7368 - val_loss: 0.8300 - val_binary_accuracy: 0.7949\n",
      "Epoch 3/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.5156 - binary_accuracy: 0.7450\n",
      "Epoch 3: val_binary_accuracy improved from 0.79487 to 0.79597, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.5156 - binary_accuracy: 0.7450 - val_loss: 0.6117 - val_binary_accuracy: 0.7960\n",
      "Epoch 4/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.5062 - binary_accuracy: 0.7548\n",
      "Epoch 4: val_binary_accuracy improved from 0.79597 to 0.80618, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.5063 - binary_accuracy: 0.7550 - val_loss: 0.4384 - val_binary_accuracy: 0.8062\n",
      "Epoch 5/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4942 - binary_accuracy: 0.7645\n",
      "Epoch 5: val_binary_accuracy improved from 0.80618 to 0.81693, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.4938 - binary_accuracy: 0.7648 - val_loss: 0.4232 - val_binary_accuracy: 0.8169\n",
      "Epoch 6/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4923 - binary_accuracy: 0.7698\n",
      "Epoch 6: val_binary_accuracy improved from 0.81693 to 0.81748, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.4921 - binary_accuracy: 0.7702 - val_loss: 0.4121 - val_binary_accuracy: 0.8175\n",
      "Epoch 7/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4849 - binary_accuracy: 0.7738\n",
      "Epoch 7: val_binary_accuracy did not improve from 0.81748\n",
      "57/57 [==============================] - 3s 52ms/step - loss: 0.4849 - binary_accuracy: 0.7738 - val_loss: 0.4416 - val_binary_accuracy: 0.7736\n",
      "Epoch 8/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4816 - binary_accuracy: 0.7769\n",
      "Epoch 8: val_binary_accuracy did not improve from 0.81748\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4812 - binary_accuracy: 0.7773 - val_loss: 0.4159 - val_binary_accuracy: 0.8062\n",
      "Epoch 9/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4787 - binary_accuracy: 0.7784\n",
      "Epoch 9: val_binary_accuracy improved from 0.81748 to 0.82685, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.4784 - binary_accuracy: 0.7790 - val_loss: 0.3919 - val_binary_accuracy: 0.8269\n",
      "Epoch 10/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4767 - binary_accuracy: 0.7822\n",
      "Epoch 10: val_binary_accuracy did not improve from 0.82685\n",
      "57/57 [==============================] - 3s 53ms/step - loss: 0.4767 - binary_accuracy: 0.7822 - val_loss: 0.4542 - val_binary_accuracy: 0.7968\n",
      "Epoch 11/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4740 - binary_accuracy: 0.7810\n",
      "Epoch 11: val_binary_accuracy did not improve from 0.82685\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.4739 - binary_accuracy: 0.7811 - val_loss: 0.3949 - val_binary_accuracy: 0.8197\n",
      "Epoch 12/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4757 - binary_accuracy: 0.7802\n",
      "Epoch 12: val_binary_accuracy did not improve from 0.82685\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.4757 - binary_accuracy: 0.7802 - val_loss: 0.4545 - val_binary_accuracy: 0.8235\n",
      "Epoch 13/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4661 - binary_accuracy: 0.7885\n",
      "Epoch 13: val_binary_accuracy did not improve from 0.82685\n",
      "57/57 [==============================] - 3s 52ms/step - loss: 0.4664 - binary_accuracy: 0.7885 - val_loss: 0.4004 - val_binary_accuracy: 0.8249\n",
      "Epoch 14/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4646 - binary_accuracy: 0.7886\n",
      "Epoch 14: val_binary_accuracy did not improve from 0.82685\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4646 - binary_accuracy: 0.7886 - val_loss: 0.4082 - val_binary_accuracy: 0.8178\n",
      "Epoch 15/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4643 - binary_accuracy: 0.7872\n",
      "Epoch 15: val_binary_accuracy improved from 0.82685 to 0.82768, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4643 - binary_accuracy: 0.7872 - val_loss: 0.4074 - val_binary_accuracy: 0.8277\n",
      "Epoch 16/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4663 - binary_accuracy: 0.7873\n",
      "Epoch 16: val_binary_accuracy improved from 0.82768 to 0.83182, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4663 - binary_accuracy: 0.7873 - val_loss: 0.4025 - val_binary_accuracy: 0.8318\n",
      "Epoch 17/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4627 - binary_accuracy: 0.7890\n",
      "Epoch 17: val_binary_accuracy did not improve from 0.83182\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4628 - binary_accuracy: 0.7887 - val_loss: 0.3980 - val_binary_accuracy: 0.8291\n",
      "Epoch 18/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4612 - binary_accuracy: 0.7892\n",
      "Epoch 18: val_binary_accuracy did not improve from 0.83182\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4612 - binary_accuracy: 0.7892 - val_loss: 0.4128 - val_binary_accuracy: 0.7985\n",
      "Epoch 19/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4614 - binary_accuracy: 0.7887\n",
      "Epoch 19: val_binary_accuracy did not improve from 0.83182\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4615 - binary_accuracy: 0.7884 - val_loss: 0.3949 - val_binary_accuracy: 0.8172\n",
      "Epoch 20/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4618 - binary_accuracy: 0.7908\n",
      "Epoch 20: val_binary_accuracy did not improve from 0.83182\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4623 - binary_accuracy: 0.7911 - val_loss: 0.4868 - val_binary_accuracy: 0.8161\n",
      "Epoch 21/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4558 - binary_accuracy: 0.7941\n",
      "Epoch 21: val_binary_accuracy improved from 0.83182 to 0.83788, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.4558 - binary_accuracy: 0.7941 - val_loss: 0.3646 - val_binary_accuracy: 0.8379\n",
      "Epoch 22/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4542 - binary_accuracy: 0.7919\n",
      "Epoch 22: val_binary_accuracy did not improve from 0.83788\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.4542 - binary_accuracy: 0.7919 - val_loss: 0.3813 - val_binary_accuracy: 0.8315\n",
      "Epoch 23/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4554 - binary_accuracy: 0.7950\n",
      "Epoch 23: val_binary_accuracy did not improve from 0.83788\n",
      "57/57 [==============================] - 3s 57ms/step - loss: 0.4554 - binary_accuracy: 0.7950 - val_loss: 0.3672 - val_binary_accuracy: 0.8346\n",
      "Epoch 24/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4550 - binary_accuracy: 0.7921\n",
      "Epoch 24: val_binary_accuracy did not improve from 0.83788\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.4550 - binary_accuracy: 0.7921 - val_loss: 0.3710 - val_binary_accuracy: 0.8365\n",
      "Epoch 25/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4545 - binary_accuracy: 0.7924\n",
      "Epoch 25: val_binary_accuracy did not improve from 0.83788\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4545 - binary_accuracy: 0.7924 - val_loss: 0.3911 - val_binary_accuracy: 0.8235\n",
      "Epoch 26/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4567 - binary_accuracy: 0.7917\n",
      "Epoch 26: val_binary_accuracy improved from 0.83788 to 0.84312, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4567 - binary_accuracy: 0.7917 - val_loss: 0.3595 - val_binary_accuracy: 0.8431\n",
      "Epoch 27/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4479 - binary_accuracy: 0.7969\n",
      "Epoch 27: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4479 - binary_accuracy: 0.7969 - val_loss: 0.3702 - val_binary_accuracy: 0.8346\n",
      "Epoch 28/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4512 - binary_accuracy: 0.7938\n",
      "Epoch 28: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4511 - binary_accuracy: 0.7939 - val_loss: 0.3544 - val_binary_accuracy: 0.8431\n",
      "Epoch 29/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4480 - binary_accuracy: 0.7964\n",
      "Epoch 29: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4480 - binary_accuracy: 0.7964 - val_loss: 0.3644 - val_binary_accuracy: 0.8423\n",
      "Epoch 30/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4505 - binary_accuracy: 0.7943\n",
      "Epoch 30: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4505 - binary_accuracy: 0.7943 - val_loss: 0.3589 - val_binary_accuracy: 0.8384\n",
      "Epoch 31/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4483 - binary_accuracy: 0.7973\n",
      "Epoch 31: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4478 - binary_accuracy: 0.7974 - val_loss: 0.3704 - val_binary_accuracy: 0.8337\n",
      "Epoch 32/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4456 - binary_accuracy: 0.7986\n",
      "Epoch 32: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4460 - binary_accuracy: 0.7982 - val_loss: 0.3868 - val_binary_accuracy: 0.8263\n",
      "Epoch 33/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4445 - binary_accuracy: 0.7976\n",
      "Epoch 33: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4444 - binary_accuracy: 0.7974 - val_loss: 0.3867 - val_binary_accuracy: 0.8241\n",
      "Epoch 34/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4453 - binary_accuracy: 0.7965\n",
      "Epoch 34: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4457 - binary_accuracy: 0.7960 - val_loss: 0.3597 - val_binary_accuracy: 0.8417\n",
      "Epoch 35/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4441 - binary_accuracy: 0.7954\n",
      "Epoch 35: val_binary_accuracy did not improve from 0.84312\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4441 - binary_accuracy: 0.7954 - val_loss: 0.3688 - val_binary_accuracy: 0.8318\n",
      "Epoch 36/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4416 - binary_accuracy: 0.7986\n",
      "Epoch 36: val_binary_accuracy improved from 0.84312 to 0.84395, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4415 - binary_accuracy: 0.7987 - val_loss: 0.3494 - val_binary_accuracy: 0.8439\n",
      "Epoch 37/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4445 - binary_accuracy: 0.7976\n",
      "Epoch 37: val_binary_accuracy did not improve from 0.84395\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4445 - binary_accuracy: 0.7976 - val_loss: 0.4166 - val_binary_accuracy: 0.8208\n",
      "Epoch 38/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4382 - binary_accuracy: 0.8011\n",
      "Epoch 38: val_binary_accuracy improved from 0.84395 to 0.85167, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4380 - binary_accuracy: 0.8012 - val_loss: 0.3471 - val_binary_accuracy: 0.8517\n",
      "Epoch 39/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4368 - binary_accuracy: 0.8000\n",
      "Epoch 39: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4368 - binary_accuracy: 0.7999 - val_loss: 0.3460 - val_binary_accuracy: 0.8478\n",
      "Epoch 40/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4422 - binary_accuracy: 0.7999\n",
      "Epoch 40: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4422 - binary_accuracy: 0.7999 - val_loss: 0.3598 - val_binary_accuracy: 0.8332\n",
      "Epoch 41/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4368 - binary_accuracy: 0.7985\n",
      "Epoch 41: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4368 - binary_accuracy: 0.7985 - val_loss: 0.3491 - val_binary_accuracy: 0.8478\n",
      "Epoch 42/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4388 - binary_accuracy: 0.7974\n",
      "Epoch 42: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4386 - binary_accuracy: 0.7972 - val_loss: 0.3612 - val_binary_accuracy: 0.8379\n",
      "Epoch 43/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4353 - binary_accuracy: 0.8011\n",
      "Epoch 43: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4353 - binary_accuracy: 0.8011 - val_loss: 0.3417 - val_binary_accuracy: 0.8489\n",
      "Epoch 44/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4361 - binary_accuracy: 0.8008\n",
      "Epoch 44: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4361 - binary_accuracy: 0.8008 - val_loss: 0.3419 - val_binary_accuracy: 0.8473\n",
      "Epoch 45/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4372 - binary_accuracy: 0.7987\n",
      "Epoch 45: val_binary_accuracy did not improve from 0.85167\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4374 - binary_accuracy: 0.7985 - val_loss: 0.3467 - val_binary_accuracy: 0.8442\n",
      "Epoch 46/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4369 - binary_accuracy: 0.8009\n",
      "Epoch 46: val_binary_accuracy improved from 0.85167 to 0.85553, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4371 - binary_accuracy: 0.8008 - val_loss: 0.3331 - val_binary_accuracy: 0.8555\n",
      "Epoch 47/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4307 - binary_accuracy: 0.8010\n",
      "Epoch 47: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4313 - binary_accuracy: 0.8011 - val_loss: 0.3490 - val_binary_accuracy: 0.8481\n",
      "Epoch 48/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4291 - binary_accuracy: 0.8037\n",
      "Epoch 48: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4291 - binary_accuracy: 0.8037 - val_loss: 0.3575 - val_binary_accuracy: 0.8439\n",
      "Epoch 49/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4358 - binary_accuracy: 0.7987\n",
      "Epoch 49: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4356 - binary_accuracy: 0.7987 - val_loss: 0.3557 - val_binary_accuracy: 0.8445\n",
      "Epoch 50/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4342 - binary_accuracy: 0.8016\n",
      "Epoch 50: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4342 - binary_accuracy: 0.8016 - val_loss: 0.3385 - val_binary_accuracy: 0.8506\n",
      "Epoch 51/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4291 - binary_accuracy: 0.8034\n",
      "Epoch 51: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4291 - binary_accuracy: 0.8034 - val_loss: 0.3450 - val_binary_accuracy: 0.8508\n",
      "Epoch 52/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4307 - binary_accuracy: 0.8030\n",
      "Epoch 52: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4307 - binary_accuracy: 0.8029 - val_loss: 0.3798 - val_binary_accuracy: 0.8337\n",
      "Epoch 53/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4320 - binary_accuracy: 0.8009\n",
      "Epoch 53: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4320 - binary_accuracy: 0.8009 - val_loss: 0.3403 - val_binary_accuracy: 0.8456\n",
      "Epoch 54/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4301 - binary_accuracy: 0.8056\n",
      "Epoch 54: val_binary_accuracy did not improve from 0.85553\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4301 - binary_accuracy: 0.8056 - val_loss: 0.3447 - val_binary_accuracy: 0.8500\n",
      "Epoch 55/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4289 - binary_accuracy: 0.8037\n",
      "Epoch 55: val_binary_accuracy improved from 0.85553 to 0.85801, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.4289 - binary_accuracy: 0.8037 - val_loss: 0.3330 - val_binary_accuracy: 0.8580\n",
      "Epoch 56/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4279 - binary_accuracy: 0.8049\n",
      "Epoch 56: val_binary_accuracy did not improve from 0.85801\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4279 - binary_accuracy: 0.8049 - val_loss: 0.3752 - val_binary_accuracy: 0.8302\n",
      "Epoch 57/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4279 - binary_accuracy: 0.8044\n",
      "Epoch 57: val_binary_accuracy did not improve from 0.85801\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4279 - binary_accuracy: 0.8044 - val_loss: 0.3363 - val_binary_accuracy: 0.8500\n",
      "Epoch 58/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4268 - binary_accuracy: 0.8050\n",
      "Epoch 58: val_binary_accuracy improved from 0.85801 to 0.85994, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4263 - binary_accuracy: 0.8054 - val_loss: 0.3303 - val_binary_accuracy: 0.8599\n",
      "Epoch 59/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4219 - binary_accuracy: 0.8058\n",
      "Epoch 59: val_binary_accuracy improved from 0.85994 to 0.86132, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4217 - binary_accuracy: 0.8059 - val_loss: 0.3237 - val_binary_accuracy: 0.8613\n",
      "Epoch 60/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4282 - binary_accuracy: 0.8046\n",
      "Epoch 60: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4282 - binary_accuracy: 0.8046 - val_loss: 0.3390 - val_binary_accuracy: 0.8525\n",
      "Epoch 61/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4230 - binary_accuracy: 0.8084\n",
      "Epoch 61: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4223 - binary_accuracy: 0.8087 - val_loss: 0.3328 - val_binary_accuracy: 0.8594\n",
      "Epoch 62/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4280 - binary_accuracy: 0.8053\n",
      "Epoch 62: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4280 - binary_accuracy: 0.8053 - val_loss: 0.3305 - val_binary_accuracy: 0.8530\n",
      "Epoch 63/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4237 - binary_accuracy: 0.8054\n",
      "Epoch 63: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4233 - binary_accuracy: 0.8058 - val_loss: 0.3606 - val_binary_accuracy: 0.8473\n",
      "Epoch 64/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4238 - binary_accuracy: 0.8069\n",
      "Epoch 64: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4238 - binary_accuracy: 0.8069 - val_loss: 0.3343 - val_binary_accuracy: 0.8547\n",
      "Epoch 65/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4218 - binary_accuracy: 0.8069\n",
      "Epoch 65: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.4219 - binary_accuracy: 0.8065 - val_loss: 0.3547 - val_binary_accuracy: 0.8431\n",
      "Epoch 66/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4191 - binary_accuracy: 0.8078\n",
      "Epoch 66: val_binary_accuracy did not improve from 0.86132\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4191 - binary_accuracy: 0.8078 - val_loss: 0.3351 - val_binary_accuracy: 0.8530\n",
      "Epoch 67/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4185 - binary_accuracy: 0.8090\n",
      "Epoch 67: val_binary_accuracy improved from 0.86132 to 0.86435, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4185 - binary_accuracy: 0.8090 - val_loss: 0.3256 - val_binary_accuracy: 0.8644\n",
      "Epoch 68/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4204 - binary_accuracy: 0.8061\n",
      "Epoch 68: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4204 - binary_accuracy: 0.8061 - val_loss: 0.3351 - val_binary_accuracy: 0.8511\n",
      "Epoch 69/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4178 - binary_accuracy: 0.8082\n",
      "Epoch 69: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4179 - binary_accuracy: 0.8083 - val_loss: 0.3319 - val_binary_accuracy: 0.8561\n",
      "Epoch 70/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4177 - binary_accuracy: 0.8110\n",
      "Epoch 70: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4179 - binary_accuracy: 0.8107 - val_loss: 0.3306 - val_binary_accuracy: 0.8555\n",
      "Epoch 71/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4156 - binary_accuracy: 0.8104\n",
      "Epoch 71: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4159 - binary_accuracy: 0.8100 - val_loss: 0.3244 - val_binary_accuracy: 0.8594\n",
      "Epoch 72/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4196 - binary_accuracy: 0.8089\n",
      "Epoch 72: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4196 - binary_accuracy: 0.8089 - val_loss: 0.3189 - val_binary_accuracy: 0.8605\n",
      "Epoch 73/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4190 - binary_accuracy: 0.8099\n",
      "Epoch 73: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4190 - binary_accuracy: 0.8098 - val_loss: 0.3151 - val_binary_accuracy: 0.8621\n",
      "Epoch 74/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4139 - binary_accuracy: 0.8124\n",
      "Epoch 74: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4134 - binary_accuracy: 0.8127 - val_loss: 0.3274 - val_binary_accuracy: 0.8594\n",
      "Epoch 75/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4197 - binary_accuracy: 0.8088\n",
      "Epoch 75: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4199 - binary_accuracy: 0.8085 - val_loss: 0.3452 - val_binary_accuracy: 0.8508\n",
      "Epoch 76/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4135 - binary_accuracy: 0.8105\n",
      "Epoch 76: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4135 - binary_accuracy: 0.8108 - val_loss: 0.3253 - val_binary_accuracy: 0.8602\n",
      "Epoch 77/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4168 - binary_accuracy: 0.8103\n",
      "Epoch 77: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4168 - binary_accuracy: 0.8103 - val_loss: 0.3256 - val_binary_accuracy: 0.8644\n",
      "Epoch 78/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4170 - binary_accuracy: 0.8080\n",
      "Epoch 78: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4170 - binary_accuracy: 0.8080 - val_loss: 0.3172 - val_binary_accuracy: 0.8638\n",
      "Epoch 79/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4131 - binary_accuracy: 0.8117\n",
      "Epoch 79: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4140 - binary_accuracy: 0.8111 - val_loss: 0.3256 - val_binary_accuracy: 0.8569\n",
      "Epoch 80/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4122 - binary_accuracy: 0.8121\n",
      "Epoch 80: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4122 - binary_accuracy: 0.8121 - val_loss: 0.3177 - val_binary_accuracy: 0.8605\n",
      "Epoch 81/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4133 - binary_accuracy: 0.8111\n",
      "Epoch 81: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4133 - binary_accuracy: 0.8111 - val_loss: 0.3152 - val_binary_accuracy: 0.8602\n",
      "Epoch 82/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4154 - binary_accuracy: 0.8099\n",
      "Epoch 82: val_binary_accuracy did not improve from 0.86435\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4153 - binary_accuracy: 0.8099 - val_loss: 0.3308 - val_binary_accuracy: 0.8558\n",
      "Epoch 83/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4130 - binary_accuracy: 0.8109\n",
      "Epoch 83: val_binary_accuracy improved from 0.86435 to 0.86849, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4130 - binary_accuracy: 0.8109 - val_loss: 0.3201 - val_binary_accuracy: 0.8685\n",
      "Epoch 84/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4118 - binary_accuracy: 0.8114\n",
      "Epoch 84: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4118 - binary_accuracy: 0.8114 - val_loss: 0.3319 - val_binary_accuracy: 0.8591\n",
      "Epoch 85/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4151 - binary_accuracy: 0.8120\n",
      "Epoch 85: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4151 - binary_accuracy: 0.8120 - val_loss: 0.3175 - val_binary_accuracy: 0.8666\n",
      "Epoch 86/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4134 - binary_accuracy: 0.8097\n",
      "Epoch 86: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4130 - binary_accuracy: 0.8101 - val_loss: 0.3172 - val_binary_accuracy: 0.8668\n",
      "Epoch 87/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4095 - binary_accuracy: 0.8135\n",
      "Epoch 87: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4105 - binary_accuracy: 0.8130 - val_loss: 0.3234 - val_binary_accuracy: 0.8619\n",
      "Epoch 88/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4092 - binary_accuracy: 0.8141\n",
      "Epoch 88: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4099 - binary_accuracy: 0.8138 - val_loss: 0.3199 - val_binary_accuracy: 0.8616\n",
      "Epoch 89/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4092 - binary_accuracy: 0.8105\n",
      "Epoch 89: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4095 - binary_accuracy: 0.8108 - val_loss: 0.3116 - val_binary_accuracy: 0.8671\n",
      "Epoch 90/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4082 - binary_accuracy: 0.8119\n",
      "Epoch 90: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4082 - binary_accuracy: 0.8119 - val_loss: 0.3163 - val_binary_accuracy: 0.8613\n",
      "Epoch 91/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4057 - binary_accuracy: 0.8150\n",
      "Epoch 91: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4057 - binary_accuracy: 0.8150 - val_loss: 0.3086 - val_binary_accuracy: 0.8652\n",
      "Epoch 92/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4107 - binary_accuracy: 0.8118\n",
      "Epoch 92: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4107 - binary_accuracy: 0.8118 - val_loss: 0.3215 - val_binary_accuracy: 0.8616\n",
      "Epoch 93/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4048 - binary_accuracy: 0.8140\n",
      "Epoch 93: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4049 - binary_accuracy: 0.8139 - val_loss: 0.3261 - val_binary_accuracy: 0.8575\n",
      "Epoch 94/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4113 - binary_accuracy: 0.8115\n",
      "Epoch 94: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4109 - binary_accuracy: 0.8116 - val_loss: 0.3196 - val_binary_accuracy: 0.8657\n",
      "Epoch 95/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4087 - binary_accuracy: 0.8121\n",
      "Epoch 95: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4087 - binary_accuracy: 0.8121 - val_loss: 0.3241 - val_binary_accuracy: 0.8635\n",
      "Epoch 96/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4097 - binary_accuracy: 0.8118\n",
      "Epoch 96: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.4097 - binary_accuracy: 0.8118 - val_loss: 0.3162 - val_binary_accuracy: 0.8649\n",
      "Epoch 97/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4049 - binary_accuracy: 0.8146\n",
      "Epoch 97: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4049 - binary_accuracy: 0.8146 - val_loss: 0.3140 - val_binary_accuracy: 0.8619\n",
      "Epoch 98/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4047 - binary_accuracy: 0.8167\n",
      "Epoch 98: val_binary_accuracy did not improve from 0.86849\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4048 - binary_accuracy: 0.8166 - val_loss: 0.3106 - val_binary_accuracy: 0.8660\n",
      "Epoch 99/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4104 - binary_accuracy: 0.8125\n",
      "Epoch 99: val_binary_accuracy improved from 0.86849 to 0.87014, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.4104 - binary_accuracy: 0.8125 - val_loss: 0.3126 - val_binary_accuracy: 0.8701\n",
      "Epoch 100/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4113 - binary_accuracy: 0.8107\n",
      "Epoch 100: val_binary_accuracy did not improve from 0.87014\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.4113 - binary_accuracy: 0.8107 - val_loss: 0.3112 - val_binary_accuracy: 0.8599\n",
      "Epoch 101/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3996 - binary_accuracy: 0.8165\n",
      "Epoch 101: val_binary_accuracy did not improve from 0.87014\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3996 - binary_accuracy: 0.8164 - val_loss: 0.3078 - val_binary_accuracy: 0.8597\n",
      "Epoch 102/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4049 - binary_accuracy: 0.8144\n",
      "Epoch 102: val_binary_accuracy did not improve from 0.87014\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4049 - binary_accuracy: 0.8144 - val_loss: 0.3060 - val_binary_accuracy: 0.8671\n",
      "Epoch 103/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4012 - binary_accuracy: 0.8167\n",
      "Epoch 103: val_binary_accuracy did not improve from 0.87014\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4012 - binary_accuracy: 0.8167 - val_loss: 0.3163 - val_binary_accuracy: 0.8613\n",
      "Epoch 104/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4102 - binary_accuracy: 0.8138\n",
      "Epoch 104: val_binary_accuracy did not improve from 0.87014\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4102 - binary_accuracy: 0.8138 - val_loss: 0.3123 - val_binary_accuracy: 0.8690\n",
      "Epoch 105/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3986 - binary_accuracy: 0.8187\n",
      "Epoch 105: val_binary_accuracy improved from 0.87014 to 0.87097, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3990 - binary_accuracy: 0.8185 - val_loss: 0.3046 - val_binary_accuracy: 0.8710\n",
      "Epoch 106/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4030 - binary_accuracy: 0.8155\n",
      "Epoch 106: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4030 - binary_accuracy: 0.8155 - val_loss: 0.3016 - val_binary_accuracy: 0.8701\n",
      "Epoch 107/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4025 - binary_accuracy: 0.8167\n",
      "Epoch 107: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4025 - binary_accuracy: 0.8167 - val_loss: 0.3025 - val_binary_accuracy: 0.8663\n",
      "Epoch 108/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4009 - binary_accuracy: 0.8159\n",
      "Epoch 108: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4009 - binary_accuracy: 0.8158 - val_loss: 0.3444 - val_binary_accuracy: 0.8473\n",
      "Epoch 109/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4035 - binary_accuracy: 0.8142\n",
      "Epoch 109: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4035 - binary_accuracy: 0.8141 - val_loss: 0.3053 - val_binary_accuracy: 0.8701\n",
      "Epoch 110/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3976 - binary_accuracy: 0.8210\n",
      "Epoch 110: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3976 - binary_accuracy: 0.8210 - val_loss: 0.3051 - val_binary_accuracy: 0.8682\n",
      "Epoch 111/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4012 - binary_accuracy: 0.8170\n",
      "Epoch 111: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4012 - binary_accuracy: 0.8172 - val_loss: 0.3261 - val_binary_accuracy: 0.8588\n",
      "Epoch 112/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3955 - binary_accuracy: 0.8198\n",
      "Epoch 112: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3954 - binary_accuracy: 0.8198 - val_loss: 0.3053 - val_binary_accuracy: 0.8710\n",
      "Epoch 113/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4006 - binary_accuracy: 0.8171\n",
      "Epoch 113: val_binary_accuracy did not improve from 0.87097\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.4006 - binary_accuracy: 0.8171 - val_loss: 0.3400 - val_binary_accuracy: 0.8528\n",
      "Epoch 114/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3975 - binary_accuracy: 0.8189\n",
      "Epoch 114: val_binary_accuracy improved from 0.87097 to 0.87345, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3975 - binary_accuracy: 0.8189 - val_loss: 0.3047 - val_binary_accuracy: 0.8734\n",
      "Epoch 115/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3946 - binary_accuracy: 0.8183\n",
      "Epoch 115: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3946 - binary_accuracy: 0.8183 - val_loss: 0.3105 - val_binary_accuracy: 0.8632\n",
      "Epoch 116/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3971 - binary_accuracy: 0.8181\n",
      "Epoch 116: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3970 - binary_accuracy: 0.8182 - val_loss: 0.3041 - val_binary_accuracy: 0.8690\n",
      "Epoch 117/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4018 - binary_accuracy: 0.8178\n",
      "Epoch 117: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.4015 - binary_accuracy: 0.8178 - val_loss: 0.3077 - val_binary_accuracy: 0.8632\n",
      "Epoch 118/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3970 - binary_accuracy: 0.8194\n",
      "Epoch 118: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3970 - binary_accuracy: 0.8194 - val_loss: 0.3113 - val_binary_accuracy: 0.8660\n",
      "Epoch 119/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3952 - binary_accuracy: 0.8183\n",
      "Epoch 119: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3949 - binary_accuracy: 0.8184 - val_loss: 0.3232 - val_binary_accuracy: 0.8588\n",
      "Epoch 120/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3953 - binary_accuracy: 0.8169\n",
      "Epoch 120: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3953 - binary_accuracy: 0.8169 - val_loss: 0.3027 - val_binary_accuracy: 0.8693\n",
      "Epoch 121/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3945 - binary_accuracy: 0.8195\n",
      "Epoch 121: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3949 - binary_accuracy: 0.8195 - val_loss: 0.3340 - val_binary_accuracy: 0.8572\n",
      "Epoch 122/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3989 - binary_accuracy: 0.8173\n",
      "Epoch 122: val_binary_accuracy improved from 0.87345 to 0.87565, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3989 - binary_accuracy: 0.8175 - val_loss: 0.2936 - val_binary_accuracy: 0.8757\n",
      "Epoch 123/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3943 - binary_accuracy: 0.8205\n",
      "Epoch 123: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3948 - binary_accuracy: 0.8204 - val_loss: 0.3004 - val_binary_accuracy: 0.8715\n",
      "Epoch 124/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3979 - binary_accuracy: 0.8166\n",
      "Epoch 124: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3985 - binary_accuracy: 0.8162 - val_loss: 0.3031 - val_binary_accuracy: 0.8688\n",
      "Epoch 125/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3939 - binary_accuracy: 0.8190\n",
      "Epoch 125: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3939 - binary_accuracy: 0.8190 - val_loss: 0.3027 - val_binary_accuracy: 0.8677\n",
      "Epoch 126/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3938 - binary_accuracy: 0.8177\n",
      "Epoch 126: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3938 - binary_accuracy: 0.8177 - val_loss: 0.2940 - val_binary_accuracy: 0.8704\n",
      "Epoch 127/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3949 - binary_accuracy: 0.8204\n",
      "Epoch 127: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3947 - binary_accuracy: 0.8207 - val_loss: 0.3169 - val_binary_accuracy: 0.8517\n",
      "Epoch 128/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3948 - binary_accuracy: 0.8184\n",
      "Epoch 128: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3951 - binary_accuracy: 0.8182 - val_loss: 0.2971 - val_binary_accuracy: 0.8660\n",
      "Epoch 129/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3956 - binary_accuracy: 0.8181\n",
      "Epoch 129: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3949 - binary_accuracy: 0.8185 - val_loss: 0.3034 - val_binary_accuracy: 0.8677\n",
      "Epoch 130/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3967 - binary_accuracy: 0.8191\n",
      "Epoch 130: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3967 - binary_accuracy: 0.8191 - val_loss: 0.3138 - val_binary_accuracy: 0.8644\n",
      "Epoch 131/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3962 - binary_accuracy: 0.8170\n",
      "Epoch 131: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3962 - binary_accuracy: 0.8170 - val_loss: 0.3008 - val_binary_accuracy: 0.8746\n",
      "Epoch 132/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3959 - binary_accuracy: 0.8204\n",
      "Epoch 132: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3959 - binary_accuracy: 0.8204 - val_loss: 0.3302 - val_binary_accuracy: 0.8475\n",
      "Epoch 133/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3958 - binary_accuracy: 0.8158\n",
      "Epoch 133: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3956 - binary_accuracy: 0.8159 - val_loss: 0.3321 - val_binary_accuracy: 0.8495\n",
      "Epoch 134/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3950 - binary_accuracy: 0.8202\n",
      "Epoch 134: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3948 - binary_accuracy: 0.8203 - val_loss: 0.2989 - val_binary_accuracy: 0.8652\n",
      "Epoch 135/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3961 - binary_accuracy: 0.8195\n",
      "Epoch 135: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3953 - binary_accuracy: 0.8201 - val_loss: 0.3014 - val_binary_accuracy: 0.8712\n",
      "Epoch 136/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3921 - binary_accuracy: 0.8188\n",
      "Epoch 136: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3924 - binary_accuracy: 0.8184 - val_loss: 0.2947 - val_binary_accuracy: 0.8726\n",
      "Epoch 137/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3932 - binary_accuracy: 0.8196\n",
      "Epoch 137: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3933 - binary_accuracy: 0.8196 - val_loss: 0.3017 - val_binary_accuracy: 0.8652\n",
      "Epoch 138/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3975 - binary_accuracy: 0.8157\n",
      "Epoch 138: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3975 - binary_accuracy: 0.8156 - val_loss: 0.3131 - val_binary_accuracy: 0.8528\n",
      "Epoch 139/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3930 - binary_accuracy: 0.8216\n",
      "Epoch 139: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3924 - binary_accuracy: 0.8219 - val_loss: 0.3045 - val_binary_accuracy: 0.8641\n",
      "Epoch 140/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3922 - binary_accuracy: 0.8222\n",
      "Epoch 140: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3925 - binary_accuracy: 0.8220 - val_loss: 0.2960 - val_binary_accuracy: 0.8677\n",
      "Epoch 141/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3905 - binary_accuracy: 0.8182\n",
      "Epoch 141: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3905 - binary_accuracy: 0.8182 - val_loss: 0.2925 - val_binary_accuracy: 0.8751\n",
      "Epoch 142/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3921 - binary_accuracy: 0.8192\n",
      "Epoch 142: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3919 - binary_accuracy: 0.8192 - val_loss: 0.2965 - val_binary_accuracy: 0.8693\n",
      "Epoch 143/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3916 - binary_accuracy: 0.8209\n",
      "Epoch 143: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3912 - binary_accuracy: 0.8212 - val_loss: 0.2989 - val_binary_accuracy: 0.8693\n",
      "Epoch 144/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3930 - binary_accuracy: 0.8186\n",
      "Epoch 144: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3925 - binary_accuracy: 0.8191 - val_loss: 0.2959 - val_binary_accuracy: 0.8690\n",
      "Epoch 145/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3873 - binary_accuracy: 0.8214\n",
      "Epoch 145: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3873 - binary_accuracy: 0.8214 - val_loss: 0.2980 - val_binary_accuracy: 0.8688\n",
      "Epoch 146/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3939 - binary_accuracy: 0.8181\n",
      "Epoch 146: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3939 - binary_accuracy: 0.8181 - val_loss: 0.2981 - val_binary_accuracy: 0.8710\n",
      "Epoch 147/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3857 - binary_accuracy: 0.8209\n",
      "Epoch 147: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3859 - binary_accuracy: 0.8207 - val_loss: 0.2945 - val_binary_accuracy: 0.8751\n",
      "Epoch 148/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3865 - binary_accuracy: 0.8227\n",
      "Epoch 148: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3865 - binary_accuracy: 0.8227 - val_loss: 0.2945 - val_binary_accuracy: 0.8721\n",
      "Epoch 149/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3883 - binary_accuracy: 0.8232\n",
      "Epoch 149: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3885 - binary_accuracy: 0.8229 - val_loss: 0.3225 - val_binary_accuracy: 0.8572\n",
      "Epoch 150/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3905 - binary_accuracy: 0.8211\n",
      "Epoch 150: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3894 - binary_accuracy: 0.8218 - val_loss: 0.3132 - val_binary_accuracy: 0.8679\n",
      "Epoch 151/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3867 - binary_accuracy: 0.8242\n",
      "Epoch 151: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3867 - binary_accuracy: 0.8242 - val_loss: 0.2969 - val_binary_accuracy: 0.8723\n",
      "Epoch 152/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3902 - binary_accuracy: 0.8208\n",
      "Epoch 152: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3902 - binary_accuracy: 0.8208 - val_loss: 0.2947 - val_binary_accuracy: 0.8677\n",
      "Epoch 153/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3841 - binary_accuracy: 0.8253\n",
      "Epoch 153: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3841 - binary_accuracy: 0.8253 - val_loss: 0.2947 - val_binary_accuracy: 0.8630\n",
      "Epoch 154/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3900 - binary_accuracy: 0.8182\n",
      "Epoch 154: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3899 - binary_accuracy: 0.8183 - val_loss: 0.3106 - val_binary_accuracy: 0.8668\n",
      "Epoch 155/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3877 - binary_accuracy: 0.8198\n",
      "Epoch 155: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3869 - binary_accuracy: 0.8204 - val_loss: 0.2928 - val_binary_accuracy: 0.8748\n",
      "Epoch 156/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3872 - binary_accuracy: 0.8229\n",
      "Epoch 156: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3874 - binary_accuracy: 0.8230 - val_loss: 0.2913 - val_binary_accuracy: 0.8740\n",
      "Epoch 157/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3930 - binary_accuracy: 0.8219\n",
      "Epoch 157: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3927 - binary_accuracy: 0.8217 - val_loss: 0.3079 - val_binary_accuracy: 0.8707\n",
      "Epoch 158/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3860 - binary_accuracy: 0.8224\n",
      "Epoch 158: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3860 - binary_accuracy: 0.8224 - val_loss: 0.3005 - val_binary_accuracy: 0.8710\n",
      "Epoch 159/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3881 - binary_accuracy: 0.8213\n",
      "Epoch 159: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 0.3891 - binary_accuracy: 0.8210 - val_loss: 0.2900 - val_binary_accuracy: 0.8710\n",
      "Epoch 160/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3853 - binary_accuracy: 0.8222\n",
      "Epoch 160: val_binary_accuracy did not improve from 0.87565\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3857 - binary_accuracy: 0.8222 - val_loss: 0.2903 - val_binary_accuracy: 0.8757\n",
      "Epoch 161/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3864 - binary_accuracy: 0.8207\n",
      "Epoch 161: val_binary_accuracy improved from 0.87565 to 0.87758, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3863 - binary_accuracy: 0.8207 - val_loss: 0.2914 - val_binary_accuracy: 0.8776\n",
      "Epoch 162/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3814 - binary_accuracy: 0.8245\n",
      "Epoch 162: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3815 - binary_accuracy: 0.8242 - val_loss: 0.2977 - val_binary_accuracy: 0.8660\n",
      "Epoch 163/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3836 - binary_accuracy: 0.8218\n",
      "Epoch 163: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3834 - binary_accuracy: 0.8219 - val_loss: 0.2931 - val_binary_accuracy: 0.8712\n",
      "Epoch 164/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3843 - binary_accuracy: 0.8246\n",
      "Epoch 164: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3838 - binary_accuracy: 0.8250 - val_loss: 0.2989 - val_binary_accuracy: 0.8679\n",
      "Epoch 165/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3874 - binary_accuracy: 0.8240\n",
      "Epoch 165: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3877 - binary_accuracy: 0.8239 - val_loss: 0.2952 - val_binary_accuracy: 0.8693\n",
      "Epoch 166/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3841 - binary_accuracy: 0.8261\n",
      "Epoch 166: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3840 - binary_accuracy: 0.8259 - val_loss: 0.2915 - val_binary_accuracy: 0.8707\n",
      "Epoch 167/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3875 - binary_accuracy: 0.8233\n",
      "Epoch 167: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3869 - binary_accuracy: 0.8239 - val_loss: 0.2912 - val_binary_accuracy: 0.8734\n",
      "Epoch 168/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3817 - binary_accuracy: 0.8252\n",
      "Epoch 168: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3812 - binary_accuracy: 0.8252 - val_loss: 0.2849 - val_binary_accuracy: 0.8746\n",
      "Epoch 169/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3820 - binary_accuracy: 0.8218\n",
      "Epoch 169: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.3820 - binary_accuracy: 0.8218 - val_loss: 0.2866 - val_binary_accuracy: 0.8759\n",
      "Epoch 170/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3850 - binary_accuracy: 0.8206\n",
      "Epoch 170: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 5s 95ms/step - loss: 0.3850 - binary_accuracy: 0.8206 - val_loss: 0.2907 - val_binary_accuracy: 0.8776\n",
      "Epoch 171/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3822 - binary_accuracy: 0.8252\n",
      "Epoch 171: val_binary_accuracy did not improve from 0.87758\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3825 - binary_accuracy: 0.8250 - val_loss: 0.2907 - val_binary_accuracy: 0.8715\n",
      "Epoch 172/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3830 - binary_accuracy: 0.8258\n",
      "Epoch 172: val_binary_accuracy improved from 0.87758 to 0.87786, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 57ms/step - loss: 0.3830 - binary_accuracy: 0.8258 - val_loss: 0.2910 - val_binary_accuracy: 0.8779\n",
      "Epoch 173/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3818 - binary_accuracy: 0.8242\n",
      "Epoch 173: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 58ms/step - loss: 0.3818 - binary_accuracy: 0.8242 - val_loss: 0.3010 - val_binary_accuracy: 0.8577\n",
      "Epoch 174/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3831 - binary_accuracy: 0.8248\n",
      "Epoch 174: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3831 - binary_accuracy: 0.8248 - val_loss: 0.2944 - val_binary_accuracy: 0.8734\n",
      "Epoch 175/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3756 - binary_accuracy: 0.8286\n",
      "Epoch 175: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3756 - binary_accuracy: 0.8286 - val_loss: 0.3002 - val_binary_accuracy: 0.8740\n",
      "Epoch 176/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3801 - binary_accuracy: 0.8266\n",
      "Epoch 176: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.3801 - binary_accuracy: 0.8266 - val_loss: 0.2853 - val_binary_accuracy: 0.8768\n",
      "Epoch 177/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3873 - binary_accuracy: 0.8221\n",
      "Epoch 177: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.3873 - binary_accuracy: 0.8221 - val_loss: 0.2927 - val_binary_accuracy: 0.8707\n",
      "Epoch 178/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3796 - binary_accuracy: 0.8269\n",
      "Epoch 178: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3797 - binary_accuracy: 0.8270 - val_loss: 0.2900 - val_binary_accuracy: 0.8748\n",
      "Epoch 179/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3793 - binary_accuracy: 0.8270\n",
      "Epoch 179: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.3793 - binary_accuracy: 0.8270 - val_loss: 0.2960 - val_binary_accuracy: 0.8779\n",
      "Epoch 180/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3778 - binary_accuracy: 0.8266\n",
      "Epoch 180: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3778 - binary_accuracy: 0.8266 - val_loss: 0.2947 - val_binary_accuracy: 0.8746\n",
      "Epoch 181/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3812 - binary_accuracy: 0.8248\n",
      "Epoch 181: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3818 - binary_accuracy: 0.8245 - val_loss: 0.2869 - val_binary_accuracy: 0.8746\n",
      "Epoch 182/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3771 - binary_accuracy: 0.8266\n",
      "Epoch 182: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3774 - binary_accuracy: 0.8266 - val_loss: 0.2911 - val_binary_accuracy: 0.8751\n",
      "Epoch 183/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3821 - binary_accuracy: 0.8258\n",
      "Epoch 183: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3823 - binary_accuracy: 0.8254 - val_loss: 0.2863 - val_binary_accuracy: 0.8773\n",
      "Epoch 184/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3822 - binary_accuracy: 0.8251\n",
      "Epoch 184: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3825 - binary_accuracy: 0.8249 - val_loss: 0.2879 - val_binary_accuracy: 0.8743\n",
      "Epoch 185/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3799 - binary_accuracy: 0.8252\n",
      "Epoch 185: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3799 - binary_accuracy: 0.8252 - val_loss: 0.3017 - val_binary_accuracy: 0.8632\n",
      "Epoch 186/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3766 - binary_accuracy: 0.8243\n",
      "Epoch 186: val_binary_accuracy did not improve from 0.87786\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3766 - binary_accuracy: 0.8243 - val_loss: 0.2901 - val_binary_accuracy: 0.8715\n",
      "Epoch 187/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3817 - binary_accuracy: 0.8241\n",
      "Epoch 187: val_binary_accuracy improved from 0.87786 to 0.87951, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 53ms/step - loss: 0.3813 - binary_accuracy: 0.8241 - val_loss: 0.2867 - val_binary_accuracy: 0.8795\n",
      "Epoch 188/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3780 - binary_accuracy: 0.8261\n",
      "Epoch 188: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3780 - binary_accuracy: 0.8261 - val_loss: 0.2863 - val_binary_accuracy: 0.8779\n",
      "Epoch 189/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3830 - binary_accuracy: 0.8242\n",
      "Epoch 189: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3830 - binary_accuracy: 0.8241 - val_loss: 0.2871 - val_binary_accuracy: 0.8734\n",
      "Epoch 190/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3796 - binary_accuracy: 0.8259\n",
      "Epoch 190: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3801 - binary_accuracy: 0.8256 - val_loss: 0.2855 - val_binary_accuracy: 0.8754\n",
      "Epoch 191/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3803 - binary_accuracy: 0.8272\n",
      "Epoch 191: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3803 - binary_accuracy: 0.8272 - val_loss: 0.2928 - val_binary_accuracy: 0.8721\n",
      "Epoch 192/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3763 - binary_accuracy: 0.8300\n",
      "Epoch 192: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3764 - binary_accuracy: 0.8300 - val_loss: 0.2887 - val_binary_accuracy: 0.8743\n",
      "Epoch 193/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3768 - binary_accuracy: 0.8278\n",
      "Epoch 193: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3768 - binary_accuracy: 0.8278 - val_loss: 0.2836 - val_binary_accuracy: 0.8779\n",
      "Epoch 194/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3718 - binary_accuracy: 0.8295\n",
      "Epoch 194: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3715 - binary_accuracy: 0.8295 - val_loss: 0.2853 - val_binary_accuracy: 0.8682\n",
      "Epoch 195/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3805 - binary_accuracy: 0.8238\n",
      "Epoch 195: val_binary_accuracy did not improve from 0.87951\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3805 - binary_accuracy: 0.8238 - val_loss: 0.2808 - val_binary_accuracy: 0.8762\n",
      "Epoch 196/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3816 - binary_accuracy: 0.8240\n",
      "Epoch 196: val_binary_accuracy improved from 0.87951 to 0.87979, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3819 - binary_accuracy: 0.8236 - val_loss: 0.2821 - val_binary_accuracy: 0.8798\n",
      "Epoch 197/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3791 - binary_accuracy: 0.8290\n",
      "Epoch 197: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3791 - binary_accuracy: 0.8290 - val_loss: 0.2813 - val_binary_accuracy: 0.8798\n",
      "Epoch 198/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3772 - binary_accuracy: 0.8274\n",
      "Epoch 198: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3772 - binary_accuracy: 0.8274 - val_loss: 0.2882 - val_binary_accuracy: 0.8715\n",
      "Epoch 199/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3717 - binary_accuracy: 0.8300\n",
      "Epoch 199: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3719 - binary_accuracy: 0.8297 - val_loss: 0.2938 - val_binary_accuracy: 0.8759\n",
      "Epoch 200/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3729 - binary_accuracy: 0.8291\n",
      "Epoch 200: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 53ms/step - loss: 0.3729 - binary_accuracy: 0.8291 - val_loss: 0.2866 - val_binary_accuracy: 0.8748\n",
      "Epoch 201/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3783 - binary_accuracy: 0.8252\n",
      "Epoch 201: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3783 - binary_accuracy: 0.8253 - val_loss: 0.2872 - val_binary_accuracy: 0.8770\n",
      "Epoch 202/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3709 - binary_accuracy: 0.8283\n",
      "Epoch 202: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 58ms/step - loss: 0.3709 - binary_accuracy: 0.8284 - val_loss: 0.2852 - val_binary_accuracy: 0.8779\n",
      "Epoch 203/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3722 - binary_accuracy: 0.8302\n",
      "Epoch 203: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3722 - binary_accuracy: 0.8302 - val_loss: 0.2964 - val_binary_accuracy: 0.8754\n",
      "Epoch 204/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3746 - binary_accuracy: 0.8279\n",
      "Epoch 204: val_binary_accuracy did not improve from 0.87979\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3746 - binary_accuracy: 0.8279 - val_loss: 0.2940 - val_binary_accuracy: 0.8765\n",
      "Epoch 205/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3731 - binary_accuracy: 0.8283\n",
      "Epoch 205: val_binary_accuracy improved from 0.87979 to 0.88172, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3731 - binary_accuracy: 0.8283 - val_loss: 0.2820 - val_binary_accuracy: 0.8817\n",
      "Epoch 206/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3732 - binary_accuracy: 0.8275\n",
      "Epoch 206: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3732 - binary_accuracy: 0.8275 - val_loss: 0.2824 - val_binary_accuracy: 0.8748\n",
      "Epoch 207/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3751 - binary_accuracy: 0.8276\n",
      "Epoch 207: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3755 - binary_accuracy: 0.8272 - val_loss: 0.2863 - val_binary_accuracy: 0.8768\n",
      "Epoch 208/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3756 - binary_accuracy: 0.8274\n",
      "Epoch 208: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3754 - binary_accuracy: 0.8275 - val_loss: 0.2912 - val_binary_accuracy: 0.8707\n",
      "Epoch 209/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3751 - binary_accuracy: 0.8304\n",
      "Epoch 209: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3751 - binary_accuracy: 0.8304 - val_loss: 0.2881 - val_binary_accuracy: 0.8757\n",
      "Epoch 210/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3733 - binary_accuracy: 0.8297\n",
      "Epoch 210: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.3733 - binary_accuracy: 0.8297 - val_loss: 0.2824 - val_binary_accuracy: 0.8806\n",
      "Epoch 211/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3745 - binary_accuracy: 0.8260\n",
      "Epoch 211: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3747 - binary_accuracy: 0.8259 - val_loss: 0.2955 - val_binary_accuracy: 0.8701\n",
      "Epoch 212/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3751 - binary_accuracy: 0.8269\n",
      "Epoch 212: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3751 - binary_accuracy: 0.8269 - val_loss: 0.2822 - val_binary_accuracy: 0.8773\n",
      "Epoch 213/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3788 - binary_accuracy: 0.8248\n",
      "Epoch 213: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 58ms/step - loss: 0.3787 - binary_accuracy: 0.8250 - val_loss: 0.2857 - val_binary_accuracy: 0.8770\n",
      "Epoch 214/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3776 - binary_accuracy: 0.8265\n",
      "Epoch 214: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 57ms/step - loss: 0.3779 - binary_accuracy: 0.8266 - val_loss: 0.2793 - val_binary_accuracy: 0.8809\n",
      "Epoch 215/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3757 - binary_accuracy: 0.8266\n",
      "Epoch 215: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3763 - binary_accuracy: 0.8264 - val_loss: 0.2834 - val_binary_accuracy: 0.8768\n",
      "Epoch 216/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3701 - binary_accuracy: 0.8298\n",
      "Epoch 216: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3698 - binary_accuracy: 0.8300 - val_loss: 0.2837 - val_binary_accuracy: 0.8787\n",
      "Epoch 217/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3699 - binary_accuracy: 0.8285\n",
      "Epoch 217: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3699 - binary_accuracy: 0.8285 - val_loss: 0.2846 - val_binary_accuracy: 0.8787\n",
      "Epoch 218/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3686 - binary_accuracy: 0.8308\n",
      "Epoch 218: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3687 - binary_accuracy: 0.8309 - val_loss: 0.2909 - val_binary_accuracy: 0.8759\n",
      "Epoch 219/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3736 - binary_accuracy: 0.8281\n",
      "Epoch 219: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3736 - binary_accuracy: 0.8281 - val_loss: 0.2835 - val_binary_accuracy: 0.8779\n",
      "Epoch 220/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3750 - binary_accuracy: 0.8260\n",
      "Epoch 220: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3748 - binary_accuracy: 0.8262 - val_loss: 0.2788 - val_binary_accuracy: 0.8814\n",
      "Epoch 221/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3749 - binary_accuracy: 0.8289\n",
      "Epoch 221: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3748 - binary_accuracy: 0.8290 - val_loss: 0.2824 - val_binary_accuracy: 0.8715\n",
      "Epoch 222/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3739 - binary_accuracy: 0.8228\n",
      "Epoch 222: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3738 - binary_accuracy: 0.8231 - val_loss: 0.2843 - val_binary_accuracy: 0.8757\n",
      "Epoch 223/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3733 - binary_accuracy: 0.8262\n",
      "Epoch 223: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3736 - binary_accuracy: 0.8257 - val_loss: 0.2800 - val_binary_accuracy: 0.8812\n",
      "Epoch 224/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3717 - binary_accuracy: 0.8276\n",
      "Epoch 224: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3720 - binary_accuracy: 0.8275 - val_loss: 0.2807 - val_binary_accuracy: 0.8748\n",
      "Epoch 225/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3711 - binary_accuracy: 0.8297\n",
      "Epoch 225: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3708 - binary_accuracy: 0.8302 - val_loss: 0.2802 - val_binary_accuracy: 0.8812\n",
      "Epoch 226/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3753 - binary_accuracy: 0.8266\n",
      "Epoch 226: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3750 - binary_accuracy: 0.8267 - val_loss: 0.2849 - val_binary_accuracy: 0.8801\n",
      "Epoch 227/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3692 - binary_accuracy: 0.8305\n",
      "Epoch 227: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3692 - binary_accuracy: 0.8305 - val_loss: 0.2772 - val_binary_accuracy: 0.8812\n",
      "Epoch 228/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3744 - binary_accuracy: 0.8287\n",
      "Epoch 228: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3744 - binary_accuracy: 0.8287 - val_loss: 0.2891 - val_binary_accuracy: 0.8734\n",
      "Epoch 229/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3718 - binary_accuracy: 0.8300\n",
      "Epoch 229: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3722 - binary_accuracy: 0.8299 - val_loss: 0.2816 - val_binary_accuracy: 0.8754\n",
      "Epoch 230/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3717 - binary_accuracy: 0.8321\n",
      "Epoch 230: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3718 - binary_accuracy: 0.8320 - val_loss: 0.2760 - val_binary_accuracy: 0.8798\n",
      "Epoch 231/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3654 - binary_accuracy: 0.8314\n",
      "Epoch 231: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3654 - binary_accuracy: 0.8314 - val_loss: 0.2911 - val_binary_accuracy: 0.8726\n",
      "Epoch 232/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3668 - binary_accuracy: 0.8323\n",
      "Epoch 232: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3668 - binary_accuracy: 0.8323 - val_loss: 0.2892 - val_binary_accuracy: 0.8707\n",
      "Epoch 233/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3732 - binary_accuracy: 0.8270\n",
      "Epoch 233: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3729 - binary_accuracy: 0.8273 - val_loss: 0.2817 - val_binary_accuracy: 0.8792\n",
      "Epoch 234/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3742 - binary_accuracy: 0.8271\n",
      "Epoch 234: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3742 - binary_accuracy: 0.8271 - val_loss: 0.2840 - val_binary_accuracy: 0.8729\n",
      "Epoch 235/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3730 - binary_accuracy: 0.8286\n",
      "Epoch 235: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3725 - binary_accuracy: 0.8288 - val_loss: 0.2814 - val_binary_accuracy: 0.8770\n",
      "Epoch 236/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3698 - binary_accuracy: 0.8298\n",
      "Epoch 236: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3697 - binary_accuracy: 0.8295 - val_loss: 0.2833 - val_binary_accuracy: 0.8721\n",
      "Epoch 237/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3730 - binary_accuracy: 0.8281\n",
      "Epoch 237: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3727 - binary_accuracy: 0.8283 - val_loss: 0.2820 - val_binary_accuracy: 0.8779\n",
      "Epoch 238/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3691 - binary_accuracy: 0.8315\n",
      "Epoch 238: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3689 - binary_accuracy: 0.8316 - val_loss: 0.2763 - val_binary_accuracy: 0.8817\n",
      "Epoch 239/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3697 - binary_accuracy: 0.8284\n",
      "Epoch 239: val_binary_accuracy did not improve from 0.88172\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3694 - binary_accuracy: 0.8282 - val_loss: 0.2779 - val_binary_accuracy: 0.8814\n",
      "Epoch 240/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3681 - binary_accuracy: 0.8305\n",
      "Epoch 240: val_binary_accuracy improved from 0.88172 to 0.88530, saving model to model.hdf5\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3684 - binary_accuracy: 0.8303 - val_loss: 0.2788 - val_binary_accuracy: 0.8853\n",
      "Epoch 241/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3683 - binary_accuracy: 0.8290\n",
      "Epoch 241: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3683 - binary_accuracy: 0.8290 - val_loss: 0.2780 - val_binary_accuracy: 0.8784\n",
      "Epoch 242/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3679 - binary_accuracy: 0.8321\n",
      "Epoch 242: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3679 - binary_accuracy: 0.8321 - val_loss: 0.2788 - val_binary_accuracy: 0.8770\n",
      "Epoch 243/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3641 - binary_accuracy: 0.8344\n",
      "Epoch 243: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3655 - binary_accuracy: 0.8341 - val_loss: 0.2817 - val_binary_accuracy: 0.8806\n",
      "Epoch 244/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3683 - binary_accuracy: 0.8279\n",
      "Epoch 244: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3683 - binary_accuracy: 0.8279 - val_loss: 0.2889 - val_binary_accuracy: 0.8726\n",
      "Epoch 245/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3658 - binary_accuracy: 0.8310\n",
      "Epoch 245: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 52ms/step - loss: 0.3658 - binary_accuracy: 0.8310 - val_loss: 0.2827 - val_binary_accuracy: 0.8754\n",
      "Epoch 246/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3689 - binary_accuracy: 0.8323\n",
      "Epoch 246: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3689 - binary_accuracy: 0.8323 - val_loss: 0.2797 - val_binary_accuracy: 0.8806\n",
      "Epoch 247/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3688 - binary_accuracy: 0.8299\n",
      "Epoch 247: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3688 - binary_accuracy: 0.8299 - val_loss: 0.2770 - val_binary_accuracy: 0.8790\n",
      "Epoch 248/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3723 - binary_accuracy: 0.8247\n",
      "Epoch 248: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3723 - binary_accuracy: 0.8247 - val_loss: 0.2771 - val_binary_accuracy: 0.8817\n",
      "Epoch 249/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3613 - binary_accuracy: 0.8327\n",
      "Epoch 249: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3615 - binary_accuracy: 0.8327 - val_loss: 0.2806 - val_binary_accuracy: 0.8773\n",
      "Epoch 250/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3671 - binary_accuracy: 0.8304\n",
      "Epoch 250: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3670 - binary_accuracy: 0.8304 - val_loss: 0.2864 - val_binary_accuracy: 0.8699\n",
      "Epoch 251/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3653 - binary_accuracy: 0.8313\n",
      "Epoch 251: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3653 - binary_accuracy: 0.8313 - val_loss: 0.2877 - val_binary_accuracy: 0.8712\n",
      "Epoch 252/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3715 - binary_accuracy: 0.8281\n",
      "Epoch 252: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3715 - binary_accuracy: 0.8281 - val_loss: 0.2714 - val_binary_accuracy: 0.8823\n",
      "Epoch 253/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3722 - binary_accuracy: 0.8287\n",
      "Epoch 253: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3722 - binary_accuracy: 0.8287 - val_loss: 0.2793 - val_binary_accuracy: 0.8781\n",
      "Epoch 254/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3650 - binary_accuracy: 0.8296\n",
      "Epoch 254: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3648 - binary_accuracy: 0.8298 - val_loss: 0.2817 - val_binary_accuracy: 0.8746\n",
      "Epoch 255/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3636 - binary_accuracy: 0.8334\n",
      "Epoch 255: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3632 - binary_accuracy: 0.8335 - val_loss: 0.2752 - val_binary_accuracy: 0.8776\n",
      "Epoch 256/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3693 - binary_accuracy: 0.8280\n",
      "Epoch 256: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3693 - binary_accuracy: 0.8280 - val_loss: 0.2759 - val_binary_accuracy: 0.8784\n",
      "Epoch 257/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3634 - binary_accuracy: 0.8319\n",
      "Epoch 257: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3635 - binary_accuracy: 0.8322 - val_loss: 0.2797 - val_binary_accuracy: 0.8825\n",
      "Epoch 258/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3626 - binary_accuracy: 0.8311\n",
      "Epoch 258: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3626 - binary_accuracy: 0.8311 - val_loss: 0.2773 - val_binary_accuracy: 0.8814\n",
      "Epoch 259/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3612 - binary_accuracy: 0.8327\n",
      "Epoch 259: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3612 - binary_accuracy: 0.8327 - val_loss: 0.2729 - val_binary_accuracy: 0.8820\n",
      "Epoch 260/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3643 - binary_accuracy: 0.8331\n",
      "Epoch 260: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3650 - binary_accuracy: 0.8328 - val_loss: 0.2753 - val_binary_accuracy: 0.8825\n",
      "Epoch 261/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3641 - binary_accuracy: 0.8317\n",
      "Epoch 261: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3644 - binary_accuracy: 0.8314 - val_loss: 0.2740 - val_binary_accuracy: 0.8779\n",
      "Epoch 262/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3658 - binary_accuracy: 0.8316\n",
      "Epoch 262: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3655 - binary_accuracy: 0.8317 - val_loss: 0.2721 - val_binary_accuracy: 0.8806\n",
      "Epoch 263/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3692 - binary_accuracy: 0.8278\n",
      "Epoch 263: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.3694 - binary_accuracy: 0.8277 - val_loss: 0.2733 - val_binary_accuracy: 0.8850\n",
      "Epoch 264/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3641 - binary_accuracy: 0.8336\n",
      "Epoch 264: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3641 - binary_accuracy: 0.8336 - val_loss: 0.2782 - val_binary_accuracy: 0.8809\n",
      "Epoch 265/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3626 - binary_accuracy: 0.8330\n",
      "Epoch 265: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3628 - binary_accuracy: 0.8333 - val_loss: 0.2766 - val_binary_accuracy: 0.8795\n",
      "Epoch 266/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3635 - binary_accuracy: 0.8330\n",
      "Epoch 266: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3631 - binary_accuracy: 0.8334 - val_loss: 0.2755 - val_binary_accuracy: 0.8801\n",
      "Epoch 267/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3597 - binary_accuracy: 0.8349\n",
      "Epoch 267: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3599 - binary_accuracy: 0.8346 - val_loss: 0.2752 - val_binary_accuracy: 0.8809\n",
      "Epoch 268/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3647 - binary_accuracy: 0.8325\n",
      "Epoch 268: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3647 - binary_accuracy: 0.8325 - val_loss: 0.2797 - val_binary_accuracy: 0.8790\n",
      "Epoch 269/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3642 - binary_accuracy: 0.8304\n",
      "Epoch 269: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.3647 - binary_accuracy: 0.8303 - val_loss: 0.2751 - val_binary_accuracy: 0.8781\n",
      "Epoch 270/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3665 - binary_accuracy: 0.8331\n",
      "Epoch 270: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.3663 - binary_accuracy: 0.8332 - val_loss: 0.2916 - val_binary_accuracy: 0.8646\n",
      "Epoch 271/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3640 - binary_accuracy: 0.8333\n",
      "Epoch 271: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3640 - binary_accuracy: 0.8333 - val_loss: 0.2895 - val_binary_accuracy: 0.8685\n",
      "Epoch 272/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3626 - binary_accuracy: 0.8328\n",
      "Epoch 272: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3626 - binary_accuracy: 0.8328 - val_loss: 0.2748 - val_binary_accuracy: 0.8842\n",
      "Epoch 273/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3613 - binary_accuracy: 0.8328\n",
      "Epoch 273: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.3624 - binary_accuracy: 0.8323 - val_loss: 0.2757 - val_binary_accuracy: 0.8828\n",
      "Epoch 274/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3614 - binary_accuracy: 0.8303\n",
      "Epoch 274: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3620 - binary_accuracy: 0.8303 - val_loss: 0.2744 - val_binary_accuracy: 0.8817\n",
      "Epoch 275/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3616 - binary_accuracy: 0.8317\n",
      "Epoch 275: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 5s 84ms/step - loss: 0.3616 - binary_accuracy: 0.8317 - val_loss: 0.2785 - val_binary_accuracy: 0.8798\n",
      "Epoch 276/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3595 - binary_accuracy: 0.8344\n",
      "Epoch 276: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 5s 85ms/step - loss: 0.3595 - binary_accuracy: 0.8344 - val_loss: 0.2772 - val_binary_accuracy: 0.8801\n",
      "Epoch 277/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3654 - binary_accuracy: 0.8333\n",
      "Epoch 277: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.3653 - binary_accuracy: 0.8333 - val_loss: 0.2757 - val_binary_accuracy: 0.8795\n",
      "Epoch 278/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3649 - binary_accuracy: 0.8311\n",
      "Epoch 278: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3645 - binary_accuracy: 0.8315 - val_loss: 0.2686 - val_binary_accuracy: 0.8817\n",
      "Epoch 279/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3680 - binary_accuracy: 0.8315\n",
      "Epoch 279: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.3677 - binary_accuracy: 0.8320 - val_loss: 0.2731 - val_binary_accuracy: 0.8809\n",
      "Epoch 280/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3563 - binary_accuracy: 0.8381\n",
      "Epoch 280: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3562 - binary_accuracy: 0.8379 - val_loss: 0.2744 - val_binary_accuracy: 0.8784\n",
      "Epoch 281/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3631 - binary_accuracy: 0.8325\n",
      "Epoch 281: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3627 - binary_accuracy: 0.8325 - val_loss: 0.2756 - val_binary_accuracy: 0.8828\n",
      "Epoch 282/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3651 - binary_accuracy: 0.8300\n",
      "Epoch 282: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3651 - binary_accuracy: 0.8300 - val_loss: 0.2730 - val_binary_accuracy: 0.8795\n",
      "Epoch 283/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3622 - binary_accuracy: 0.8343\n",
      "Epoch 283: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3622 - binary_accuracy: 0.8343 - val_loss: 0.2768 - val_binary_accuracy: 0.8784\n",
      "Epoch 284/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3617 - binary_accuracy: 0.8330\n",
      "Epoch 284: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 5s 94ms/step - loss: 0.3617 - binary_accuracy: 0.8330 - val_loss: 0.2753 - val_binary_accuracy: 0.8809\n",
      "Epoch 285/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3581 - binary_accuracy: 0.8352\n",
      "Epoch 285: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 58ms/step - loss: 0.3581 - binary_accuracy: 0.8352 - val_loss: 0.2809 - val_binary_accuracy: 0.8751\n",
      "Epoch 286/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3621 - binary_accuracy: 0.8341\n",
      "Epoch 286: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3622 - binary_accuracy: 0.8341 - val_loss: 0.2779 - val_binary_accuracy: 0.8784\n",
      "Epoch 287/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3582 - binary_accuracy: 0.8354\n",
      "Epoch 287: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3584 - binary_accuracy: 0.8352 - val_loss: 0.2741 - val_binary_accuracy: 0.8779\n",
      "Epoch 288/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3615 - binary_accuracy: 0.8333\n",
      "Epoch 288: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3615 - binary_accuracy: 0.8333 - val_loss: 0.2838 - val_binary_accuracy: 0.8726\n",
      "Epoch 289/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3618 - binary_accuracy: 0.8320\n",
      "Epoch 289: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3617 - binary_accuracy: 0.8321 - val_loss: 0.2765 - val_binary_accuracy: 0.8806\n",
      "Epoch 290/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3672 - binary_accuracy: 0.8304\n",
      "Epoch 290: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 58ms/step - loss: 0.3672 - binary_accuracy: 0.8302 - val_loss: 0.2741 - val_binary_accuracy: 0.8773\n",
      "Epoch 291/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3611 - binary_accuracy: 0.8306\n",
      "Epoch 291: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3615 - binary_accuracy: 0.8306 - val_loss: 0.2820 - val_binary_accuracy: 0.8704\n",
      "Epoch 292/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3581 - binary_accuracy: 0.8363\n",
      "Epoch 292: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3589 - binary_accuracy: 0.8361 - val_loss: 0.2825 - val_binary_accuracy: 0.8757\n",
      "Epoch 293/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3658 - binary_accuracy: 0.8294\n",
      "Epoch 293: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3658 - binary_accuracy: 0.8294 - val_loss: 0.2772 - val_binary_accuracy: 0.8784\n",
      "Epoch 294/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3628 - binary_accuracy: 0.8340\n",
      "Epoch 294: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.3629 - binary_accuracy: 0.8339 - val_loss: 0.2766 - val_binary_accuracy: 0.8773\n",
      "Epoch 295/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3618 - binary_accuracy: 0.8335\n",
      "Epoch 295: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.3626 - binary_accuracy: 0.8331 - val_loss: 0.2701 - val_binary_accuracy: 0.8806\n",
      "Epoch 296/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3623 - binary_accuracy: 0.8309\n",
      "Epoch 296: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 52ms/step - loss: 0.3623 - binary_accuracy: 0.8310 - val_loss: 0.2761 - val_binary_accuracy: 0.8762\n",
      "Epoch 297/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3633 - binary_accuracy: 0.8328\n",
      "Epoch 297: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3632 - binary_accuracy: 0.8327 - val_loss: 0.2804 - val_binary_accuracy: 0.8770\n",
      "Epoch 298/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3552 - binary_accuracy: 0.8354\n",
      "Epoch 298: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.3555 - binary_accuracy: 0.8351 - val_loss: 0.2753 - val_binary_accuracy: 0.8828\n",
      "Epoch 299/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3556 - binary_accuracy: 0.8357\n",
      "Epoch 299: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3561 - binary_accuracy: 0.8354 - val_loss: 0.2743 - val_binary_accuracy: 0.8787\n",
      "Epoch 300/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3622 - binary_accuracy: 0.8321\n",
      "Epoch 300: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3627 - binary_accuracy: 0.8319 - val_loss: 0.2830 - val_binary_accuracy: 0.8751\n",
      "Epoch 301/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3595 - binary_accuracy: 0.8346\n",
      "Epoch 301: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3592 - binary_accuracy: 0.8348 - val_loss: 0.2730 - val_binary_accuracy: 0.8831\n",
      "Epoch 302/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3595 - binary_accuracy: 0.8337\n",
      "Epoch 302: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.3593 - binary_accuracy: 0.8339 - val_loss: 0.2807 - val_binary_accuracy: 0.8759\n",
      "Epoch 303/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3592 - binary_accuracy: 0.8335\n",
      "Epoch 303: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3592 - binary_accuracy: 0.8335 - val_loss: 0.2720 - val_binary_accuracy: 0.8773\n",
      "Epoch 304/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3548 - binary_accuracy: 0.8354\n",
      "Epoch 304: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3545 - binary_accuracy: 0.8358 - val_loss: 0.2791 - val_binary_accuracy: 0.8790\n",
      "Epoch 305/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3590 - binary_accuracy: 0.8348\n",
      "Epoch 305: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.3591 - binary_accuracy: 0.8347 - val_loss: 0.2718 - val_binary_accuracy: 0.8837\n",
      "Epoch 306/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3573 - binary_accuracy: 0.8348\n",
      "Epoch 306: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3573 - binary_accuracy: 0.8348 - val_loss: 0.2746 - val_binary_accuracy: 0.8850\n",
      "Epoch 307/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3574 - binary_accuracy: 0.8354\n",
      "Epoch 307: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3572 - binary_accuracy: 0.8355 - val_loss: 0.2754 - val_binary_accuracy: 0.8765\n",
      "Epoch 308/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3576 - binary_accuracy: 0.8345\n",
      "Epoch 308: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3576 - binary_accuracy: 0.8345 - val_loss: 0.2761 - val_binary_accuracy: 0.8751\n",
      "Epoch 309/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3550 - binary_accuracy: 0.8361\n",
      "Epoch 309: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3550 - binary_accuracy: 0.8361 - val_loss: 0.2671 - val_binary_accuracy: 0.8820\n",
      "Epoch 310/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3586 - binary_accuracy: 0.8335\n",
      "Epoch 310: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3586 - binary_accuracy: 0.8335 - val_loss: 0.2652 - val_binary_accuracy: 0.8853\n",
      "Epoch 311/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3546 - binary_accuracy: 0.8399\n",
      "Epoch 311: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3545 - binary_accuracy: 0.8399 - val_loss: 0.2695 - val_binary_accuracy: 0.8853\n",
      "Epoch 312/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3542 - binary_accuracy: 0.8367\n",
      "Epoch 312: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3551 - binary_accuracy: 0.8358 - val_loss: 0.2709 - val_binary_accuracy: 0.8845\n",
      "Epoch 313/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3595 - binary_accuracy: 0.8341\n",
      "Epoch 313: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3595 - binary_accuracy: 0.8341 - val_loss: 0.2686 - val_binary_accuracy: 0.8839\n",
      "Epoch 314/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3592 - binary_accuracy: 0.8356\n",
      "Epoch 314: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3591 - binary_accuracy: 0.8354 - val_loss: 0.2700 - val_binary_accuracy: 0.8792\n",
      "Epoch 315/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3594 - binary_accuracy: 0.8340\n",
      "Epoch 315: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3592 - binary_accuracy: 0.8339 - val_loss: 0.2745 - val_binary_accuracy: 0.8853\n",
      "Epoch 316/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3560 - binary_accuracy: 0.8375\n",
      "Epoch 316: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3560 - binary_accuracy: 0.8375 - val_loss: 0.2790 - val_binary_accuracy: 0.8734\n",
      "Epoch 317/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3572 - binary_accuracy: 0.8332\n",
      "Epoch 317: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 52ms/step - loss: 0.3572 - binary_accuracy: 0.8332 - val_loss: 0.2739 - val_binary_accuracy: 0.8837\n",
      "Epoch 318/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3554 - binary_accuracy: 0.8359\n",
      "Epoch 318: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3559 - binary_accuracy: 0.8354 - val_loss: 0.2738 - val_binary_accuracy: 0.8806\n",
      "Epoch 319/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3596 - binary_accuracy: 0.8344\n",
      "Epoch 319: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3597 - binary_accuracy: 0.8346 - val_loss: 0.2710 - val_binary_accuracy: 0.8817\n",
      "Epoch 320/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3603 - binary_accuracy: 0.8330\n",
      "Epoch 320: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 58ms/step - loss: 0.3603 - binary_accuracy: 0.8330 - val_loss: 0.2703 - val_binary_accuracy: 0.8837\n",
      "Epoch 321/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3540 - binary_accuracy: 0.8354\n",
      "Epoch 321: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3540 - binary_accuracy: 0.8354 - val_loss: 0.2714 - val_binary_accuracy: 0.8848\n",
      "Epoch 322/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3569 - binary_accuracy: 0.8332\n",
      "Epoch 322: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3570 - binary_accuracy: 0.8330 - val_loss: 0.2694 - val_binary_accuracy: 0.8787\n",
      "Epoch 323/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3536 - binary_accuracy: 0.8358\n",
      "Epoch 323: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3536 - binary_accuracy: 0.8358 - val_loss: 0.2814 - val_binary_accuracy: 0.8770\n",
      "Epoch 324/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3544 - binary_accuracy: 0.8398\n",
      "Epoch 324: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3550 - binary_accuracy: 0.8393 - val_loss: 0.2848 - val_binary_accuracy: 0.8715\n",
      "Epoch 325/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3571 - binary_accuracy: 0.8332\n",
      "Epoch 325: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 52ms/step - loss: 0.3571 - binary_accuracy: 0.8332 - val_loss: 0.2748 - val_binary_accuracy: 0.8795\n",
      "Epoch 326/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3555 - binary_accuracy: 0.8372\n",
      "Epoch 326: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3557 - binary_accuracy: 0.8372 - val_loss: 0.2706 - val_binary_accuracy: 0.8812\n",
      "Epoch 327/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3552 - binary_accuracy: 0.8373\n",
      "Epoch 327: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 47ms/step - loss: 0.3552 - binary_accuracy: 0.8373 - val_loss: 0.2789 - val_binary_accuracy: 0.8792\n",
      "Epoch 328/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3565 - binary_accuracy: 0.8388\n",
      "Epoch 328: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3565 - binary_accuracy: 0.8388 - val_loss: 0.2902 - val_binary_accuracy: 0.8723\n",
      "Epoch 329/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3574 - binary_accuracy: 0.8320\n",
      "Epoch 329: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3574 - binary_accuracy: 0.8320 - val_loss: 0.2775 - val_binary_accuracy: 0.8784\n",
      "Epoch 330/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3604 - binary_accuracy: 0.8359\n",
      "Epoch 330: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 0.3604 - binary_accuracy: 0.8359 - val_loss: 0.2691 - val_binary_accuracy: 0.8828\n",
      "Epoch 331/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3541 - binary_accuracy: 0.8358\n",
      "Epoch 331: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.3545 - binary_accuracy: 0.8361 - val_loss: 0.2726 - val_binary_accuracy: 0.8845\n",
      "Epoch 332/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3563 - binary_accuracy: 0.8343\n",
      "Epoch 332: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 59ms/step - loss: 0.3563 - binary_accuracy: 0.8345 - val_loss: 0.2740 - val_binary_accuracy: 0.8828\n",
      "Epoch 333/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3552 - binary_accuracy: 0.8365\n",
      "Epoch 333: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3548 - binary_accuracy: 0.8368 - val_loss: 0.2707 - val_binary_accuracy: 0.8831\n",
      "Epoch 334/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3560 - binary_accuracy: 0.8383\n",
      "Epoch 334: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3560 - binary_accuracy: 0.8383 - val_loss: 0.2713 - val_binary_accuracy: 0.8839\n",
      "Epoch 335/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3535 - binary_accuracy: 0.8381\n",
      "Epoch 335: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 57ms/step - loss: 0.3535 - binary_accuracy: 0.8381 - val_loss: 0.2723 - val_binary_accuracy: 0.8803\n",
      "Epoch 336/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3521 - binary_accuracy: 0.8388\n",
      "Epoch 336: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 50ms/step - loss: 0.3523 - binary_accuracy: 0.8387 - val_loss: 0.2733 - val_binary_accuracy: 0.8792\n",
      "Epoch 337/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3532 - binary_accuracy: 0.8358\n",
      "Epoch 337: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 49ms/step - loss: 0.3532 - binary_accuracy: 0.8358 - val_loss: 0.2741 - val_binary_accuracy: 0.8803\n",
      "Epoch 338/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3548 - binary_accuracy: 0.8374\n",
      "Epoch 338: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3550 - binary_accuracy: 0.8371 - val_loss: 0.2703 - val_binary_accuracy: 0.8825\n",
      "Epoch 339/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3544 - binary_accuracy: 0.8346\n",
      "Epoch 339: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 0.3548 - binary_accuracy: 0.8346 - val_loss: 0.2759 - val_binary_accuracy: 0.8825\n",
      "Epoch 340/1000\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3515 - binary_accuracy: 0.8352\n",
      "Epoch 340: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 54ms/step - loss: 0.3515 - binary_accuracy: 0.8352 - val_loss: 0.2687 - val_binary_accuracy: 0.8848\n",
      "Epoch 341/1000\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3583 - binary_accuracy: 0.8337\n",
      "Epoch 341: val_binary_accuracy did not improve from 0.88530\n",
      "57/57 [==============================] - 3s 51ms/step - loss: 0.3579 - binary_accuracy: 0.8337 - val_loss: 0.2766 - val_binary_accuracy: 0.8828\n",
      "Epoch 341: early stopping\n",
      "1005.1708336999873\n",
      "Best validation model: epoch 240  - val_binary_accuracy 0.8853046298027039\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))\n",
    "print (time.perf_counter() - start)\n",
    "best_idx = int(np.argmax(history.history['val_binary_accuracy']))\n",
    "best_value = np.max(history.history['val_binary_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_binary_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=pd.DataFrame(history.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.savefig(f\"./images/loss{n_neurons_per_hlayer}_{seed}_{n_epochs}_{results.val_binary_accuracy.values[-1:][0]}_{tasa_dropout}_{lr}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_binary_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.357937</td>\n",
       "      <td>0.833678</td>\n",
       "      <td>0.276621</td>\n",
       "      <td>0.882823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  binary_accuracy  val_loss  val_binary_accuracy\n",
       "340  0.357937         0.833678  0.276621             0.882823"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the training set: 0.8336779475212097\n",
      "Accuracy for the development test set: 0.8828232884407043\n",
      "  1/114 [..............................] - ETA: 2s - loss: 0.4974 - binary_accuracy: 0.8438"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 4ms/step - loss: 0.2977 - binary_accuracy: 0.8608\n",
      "Accuracy for the test set: 0.8607664704322815\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\")\n",
    "print(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\"\n",
    ")\n",
    "print(f\"Accuracy for the test set: {model.evaluate(X_test, y_test)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 4ms/step - loss: 0.2977 - binary_accuracy: 0.8608\n"
     ]
    }
   ],
   "source": [
    "v = open(\"./history/DeepFeedforward.txt\", \"a\")\n",
    "v.write(f\"Epoque: {n_epochs}\\n\")\n",
    "v.write(f\"Learning Rate: {lr}\\n\")\n",
    "v.write(f\"Batch Size: {batch_size}\\n\")\n",
    "v.write(f\"Dropout: {tasa_dropout}\\n\")\n",
    "v.write(f\"Neurons per layer: {n_neurons_per_hlayer}\\n\")\n",
    "v.write(f\"Activation: elu\\n\")\n",
    "v.write(f\"Optimizer: Adam\\n\")\n",
    "v.write(f\"seed = {seed}\\n\")\n",
    "v.write(\n",
    "    \"model.add(keras.layers.Dense(neurons, kernel_initializer=he_normal, use_bias=False)); model.add(keras.layers.BatchNormalization()); model.add(keras.layers.Activation(elu)); model.add(tf.keras.layers.Dropout(tasa_dropout))\"\n",
    ")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.write(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\\n\")\n",
    "v.write(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\\n\"\n",
    ")\n",
    "v.write(f\"Best validation model: epoch {best_idx+1} - val_binary_accuracy {best_value}\\n\")\n",
    "v.write(f\"Accuracy for the test set: {model.evaluate(X_test, y_test)[1]}\\n\")\n",
    "v.write(f\"Time: {time.perf_counter() - start}\\n\")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).round()\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
