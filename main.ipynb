{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETX = \"./data/prep/HotelReservationsPreparedCleanX.csv\"\n",
    "DATASETY = \"./data/prep/HotelReservationsY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(DATASETX)\n",
    "df_y = pd.read_csv(DATASETY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  \n",
       "0                    -0.6  \n",
       "1                    -1.0  \n",
       "2                    -1.0  \n",
       "3                    -1.0  \n",
       "4                    -1.0  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booking_status\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               0\n",
       "4               1"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  booking_status  \n",
       "0                    -0.6               1  \n",
       "1                    -1.0               1  \n",
       "2                    -1.0               1  \n",
       "3                    -1.0               0  \n",
       "4                    -1.0               1  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_x, df_y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.665914</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.677778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.674944</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.647407</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.268623</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.576667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.503386</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.575444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "16377          -0.5            -1.0             -1.000000          -0.764706   \n",
       "24639           0.0            -1.0             -0.428571          -0.764706   \n",
       "21974           0.0            -1.0             -0.428571          -1.000000   \n",
       "9205            0.0            -0.8             -1.000000          -0.647059   \n",
       "33303           0.0            -1.0             -0.428571          -0.411765   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "16377                0.0                        -1.0                 0.0   \n",
       "24639               -1.0                        -1.0                 0.0   \n",
       "21974               -1.0                         1.0                 1.0   \n",
       "9205                -1.0                        -1.0                 0.0   \n",
       "33303               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "16377  -0.665914       0.454545      0.133333                 -1.0   \n",
       "24639  -0.674944      -0.454545      0.466667                  0.0   \n",
       "21974  -0.268623      -0.090909      0.666667                  0.0   \n",
       "9205   -0.503386      -0.454545      0.733333                  0.0   \n",
       "33303  -0.399549       0.090909      0.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "16377            -1.0                          -1.0   \n",
       "24639            -1.0                          -1.0   \n",
       "21974            -1.0                          -1.0   \n",
       "9205             -1.0                          -1.0   \n",
       "33303            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "16377                                  -1.0           -0.677778   \n",
       "24639                                  -1.0           -0.647407   \n",
       "21974                                  -1.0           -0.576667   \n",
       "9205                                   -1.0           -0.643333   \n",
       "33303                                  -1.0           -0.575444   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "16377                    -1.0               1  \n",
       "24639                    -0.6               1  \n",
       "21974                    -0.6               0  \n",
       "9205                     -0.6               1  \n",
       "33303                    -0.6               1  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df, random_state=seed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, temp_set = train_test_split(df, train_size=TRAIN_SIZE, random_state=seed)\n",
    "\n",
    "validation_set, test_set = train_test_split(temp_set, train_size=VALIDATION_SIZE / (VALIDATION_SIZE + TEST_SIZE), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('booking_status', axis=1)\n",
    "y_train = train_set['booking_status']\n",
    "\n",
    "X_val = validation_set.drop('booking_status', axis=1)\n",
    "y_val = validation_set['booking_status']\n",
    "\n",
    "X_test = test_set.drop('booking_status', axis=1)\n",
    "y_test = test_set['booking_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = X_train.shape[1]\n",
    "OUTPUTS = 1\n",
    "NUM_TRAINING_EXAMPLES = int(round(X_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int (round (X_val.shape[0]/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs =1000\n",
    "lr = 0.1\n",
    "batch_size = 128\n",
    "tasa_dropout = 0.25\n",
    "error = 0.01\n",
    "n_neurons_per_hlayer = [250, 600, 250, 125, 50, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"DeepFeedforward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepFeedforward\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_77 (Dense)            (None, 250)               4000      \n",
      "                                                                 \n",
      " batch_normalization_66 (Ba  (None, 250)               1000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_66 (Activation)  (None, 250)               0         \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 600)               150000    \n",
      "                                                                 \n",
      " batch_normalization_67 (Ba  (None, 600)               2400      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_67 (Activation)  (None, 600)               0         \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 600)               0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 250)               150000    \n",
      "                                                                 \n",
      " batch_normalization_68 (Ba  (None, 250)               1000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_68 (Activation)  (None, 250)               0         \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 125)               31250     \n",
      "                                                                 \n",
      " batch_normalization_69 (Ba  (None, 125)               500       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_69 (Activation)  (None, 125)               0         \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 125)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 50)                6250      \n",
      "                                                                 \n",
      " batch_normalization_70 (Ba  (None, 50)                200       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_70 (Activation)  (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 10)                500       \n",
      "                                                                 \n",
      " batch_normalization_71 (Ba  (None, 10)                40        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_71 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 347151 (1.32 MB)\n",
      "Trainable params: 344581 (1.31 MB)\n",
      "Non-trainable params: 2570 (10.04 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.InputLayer(input_shape=(INPUTS,), batch_size=None))\n",
    "\n",
    "for neurons in n_neurons_per_hlayer:\n",
    "  model.add(keras.layers.Dense(neurons, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "  model.add(keras.layers.BatchNormalization())\n",
    "  model.add(keras.layers.Activation(\"elu\"))\n",
    "  model.add(tf.keras.layers.Dropout(tasa_dropout))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('model.hdf5', monitor='val_binary_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_binary_accuracy', factor=0.1, patience=45, min_lr=0.0001, verbose=1)\n",
    "early_stop = EarlyStopping('val_binary_accuracy', patience=101, verbose=1)\n",
    "\n",
    "callbacks = [model_checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "opt = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "#opt = SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"binary_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chema\\Desktop\\MASTER\\DeepLearning\\DL\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/227 [============================>.] - ETA: 0s - loss: 0.4520 - binary_accuracy: 0.7883"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chema\\Desktop\\MASTER\\DeepLearning\\DL\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.81638, saving model to model.hdf5\n",
      "227/227 [==============================] - 9s 19ms/step - loss: 0.4517 - binary_accuracy: 0.7884 - val_loss: 0.5742 - val_binary_accuracy: 0.8164\n",
      "Epoch 2/1000\n",
      "  5/227 [..............................] - ETA: 3s - loss: 0.4150 - binary_accuracy: 0.8078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chema\\Desktop\\MASTER\\DeepLearning\\DL\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - ETA: 0s - loss: 0.4167 - binary_accuracy: 0.8087\n",
      "Epoch 2: val_binary_accuracy improved from 0.81638 to 0.81693, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.4167 - binary_accuracy: 0.8087 - val_loss: 0.4028 - val_binary_accuracy: 0.8169\n",
      "Epoch 3/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.4101 - binary_accuracy: 0.8147\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.81693\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.4102 - binary_accuracy: 0.8147 - val_loss: 0.8062 - val_binary_accuracy: 0.7963\n",
      "Epoch 4/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.4037 - binary_accuracy: 0.8208\n",
      "Epoch 4: val_binary_accuracy improved from 0.81693 to 0.83264, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.4038 - binary_accuracy: 0.8208 - val_loss: 0.4389 - val_binary_accuracy: 0.8326\n",
      "Epoch 5/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3980 - binary_accuracy: 0.8227\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.83264\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3980 - binary_accuracy: 0.8227 - val_loss: 0.4416 - val_binary_accuracy: 0.8084\n",
      "Epoch 6/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3939 - binary_accuracy: 0.8243\n",
      "Epoch 6: val_binary_accuracy improved from 0.83264 to 0.83292, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3941 - binary_accuracy: 0.8241 - val_loss: 0.3691 - val_binary_accuracy: 0.8329\n",
      "Epoch 7/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3893 - binary_accuracy: 0.8255\n",
      "Epoch 7: val_binary_accuracy did not improve from 0.83292\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3891 - binary_accuracy: 0.8255 - val_loss: 0.5178 - val_binary_accuracy: 0.8150\n",
      "Epoch 8/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3829 - binary_accuracy: 0.8304\n",
      "Epoch 8: val_binary_accuracy did not improve from 0.83292\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3828 - binary_accuracy: 0.8304 - val_loss: 0.3887 - val_binary_accuracy: 0.8230\n",
      "Epoch 9/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3781 - binary_accuracy: 0.8318\n",
      "Epoch 9: val_binary_accuracy improved from 0.83292 to 0.83595, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3779 - binary_accuracy: 0.8318 - val_loss: 0.3747 - val_binary_accuracy: 0.8360\n",
      "Epoch 10/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3730 - binary_accuracy: 0.8339\n",
      "Epoch 10: val_binary_accuracy improved from 0.83595 to 0.83871, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3734 - binary_accuracy: 0.8339 - val_loss: 0.3650 - val_binary_accuracy: 0.8387\n",
      "Epoch 11/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3681 - binary_accuracy: 0.8382\n",
      "Epoch 11: val_binary_accuracy improved from 0.83871 to 0.84891, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3686 - binary_accuracy: 0.8379 - val_loss: 0.3480 - val_binary_accuracy: 0.8489\n",
      "Epoch 12/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3652 - binary_accuracy: 0.8375\n",
      "Epoch 12: val_binary_accuracy did not improve from 0.84891\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3652 - binary_accuracy: 0.8375 - val_loss: 0.3654 - val_binary_accuracy: 0.8442\n",
      "Epoch 13/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3724 - binary_accuracy: 0.8338\n",
      "Epoch 13: val_binary_accuracy did not improve from 0.84891\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3720 - binary_accuracy: 0.8341 - val_loss: 0.4178 - val_binary_accuracy: 0.8260\n",
      "Epoch 14/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3668 - binary_accuracy: 0.8372\n",
      "Epoch 14: val_binary_accuracy did not improve from 0.84891\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3668 - binary_accuracy: 0.8373 - val_loss: 0.4002 - val_binary_accuracy: 0.8437\n",
      "Epoch 15/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3590 - binary_accuracy: 0.8419\n",
      "Epoch 15: val_binary_accuracy did not improve from 0.84891\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3585 - binary_accuracy: 0.8422 - val_loss: 0.3587 - val_binary_accuracy: 0.8373\n",
      "Epoch 16/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3581 - binary_accuracy: 0.8424\n",
      "Epoch 16: val_binary_accuracy improved from 0.84891 to 0.85112, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3583 - binary_accuracy: 0.8424 - val_loss: 0.3308 - val_binary_accuracy: 0.8511\n",
      "Epoch 17/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3526 - binary_accuracy: 0.8433\n",
      "Epoch 17: val_binary_accuracy did not improve from 0.85112\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3527 - binary_accuracy: 0.8434 - val_loss: 0.3444 - val_binary_accuracy: 0.8462\n",
      "Epoch 18/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3540 - binary_accuracy: 0.8456\n",
      "Epoch 18: val_binary_accuracy did not improve from 0.85112\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3539 - binary_accuracy: 0.8456 - val_loss: 0.3792 - val_binary_accuracy: 0.8258\n",
      "Epoch 19/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3499 - binary_accuracy: 0.8448\n",
      "Epoch 19: val_binary_accuracy improved from 0.85112 to 0.85773, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3499 - binary_accuracy: 0.8448 - val_loss: 0.3327 - val_binary_accuracy: 0.8577\n",
      "Epoch 20/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3503 - binary_accuracy: 0.8441\n",
      "Epoch 20: val_binary_accuracy did not improve from 0.85773\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3503 - binary_accuracy: 0.8441 - val_loss: 0.3504 - val_binary_accuracy: 0.8478\n",
      "Epoch 21/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3474 - binary_accuracy: 0.8474\n",
      "Epoch 21: val_binary_accuracy did not improve from 0.85773\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3475 - binary_accuracy: 0.8473 - val_loss: 0.3346 - val_binary_accuracy: 0.8553\n",
      "Epoch 22/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3442 - binary_accuracy: 0.8486\n",
      "Epoch 22: val_binary_accuracy improved from 0.85773 to 0.85856, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3443 - binary_accuracy: 0.8486 - val_loss: 0.3295 - val_binary_accuracy: 0.8586\n",
      "Epoch 23/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3490 - binary_accuracy: 0.8470\n",
      "Epoch 23: val_binary_accuracy did not improve from 0.85856\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3490 - binary_accuracy: 0.8470 - val_loss: 0.3393 - val_binary_accuracy: 0.8566\n",
      "Epoch 24/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3457 - binary_accuracy: 0.8490\n",
      "Epoch 24: val_binary_accuracy did not improve from 0.85856\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3459 - binary_accuracy: 0.8488 - val_loss: 0.3232 - val_binary_accuracy: 0.8519\n",
      "Epoch 25/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3428 - binary_accuracy: 0.8516\n",
      "Epoch 25: val_binary_accuracy improved from 0.85856 to 0.86104, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3431 - binary_accuracy: 0.8513 - val_loss: 0.3205 - val_binary_accuracy: 0.8610\n",
      "Epoch 26/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3461 - binary_accuracy: 0.8476\n",
      "Epoch 26: val_binary_accuracy did not improve from 0.86104\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3456 - binary_accuracy: 0.8479 - val_loss: 0.3541 - val_binary_accuracy: 0.8448\n",
      "Epoch 27/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3421 - binary_accuracy: 0.8507\n",
      "Epoch 27: val_binary_accuracy improved from 0.86104 to 0.86738, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3420 - binary_accuracy: 0.8507 - val_loss: 0.3086 - val_binary_accuracy: 0.8674\n",
      "Epoch 28/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3382 - binary_accuracy: 0.8506\n",
      "Epoch 28: val_binary_accuracy did not improve from 0.86738\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3381 - binary_accuracy: 0.8508 - val_loss: 0.3441 - val_binary_accuracy: 0.8564\n",
      "Epoch 29/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3357 - binary_accuracy: 0.8537\n",
      "Epoch 29: val_binary_accuracy did not improve from 0.86738\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3359 - binary_accuracy: 0.8535 - val_loss: 0.3148 - val_binary_accuracy: 0.8514\n",
      "Epoch 30/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3367 - binary_accuracy: 0.8503\n",
      "Epoch 30: val_binary_accuracy did not improve from 0.86738\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3366 - binary_accuracy: 0.8502 - val_loss: 0.3106 - val_binary_accuracy: 0.8602\n",
      "Epoch 31/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3331 - binary_accuracy: 0.8522\n",
      "Epoch 31: val_binary_accuracy did not improve from 0.86738\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3332 - binary_accuracy: 0.8522 - val_loss: 0.3129 - val_binary_accuracy: 0.8577\n",
      "Epoch 32/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3324 - binary_accuracy: 0.8548\n",
      "Epoch 32: val_binary_accuracy did not improve from 0.86738\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3324 - binary_accuracy: 0.8548 - val_loss: 0.3183 - val_binary_accuracy: 0.8638\n",
      "Epoch 33/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3353 - binary_accuracy: 0.8538\n",
      "Epoch 33: val_binary_accuracy did not improve from 0.86738\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3354 - binary_accuracy: 0.8537 - val_loss: 0.3107 - val_binary_accuracy: 0.8555\n",
      "Epoch 34/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3329 - binary_accuracy: 0.8498\n",
      "Epoch 34: val_binary_accuracy improved from 0.86738 to 0.86766, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3329 - binary_accuracy: 0.8498 - val_loss: 0.3150 - val_binary_accuracy: 0.8677\n",
      "Epoch 35/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3292 - binary_accuracy: 0.8568\n",
      "Epoch 35: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3294 - binary_accuracy: 0.8567 - val_loss: 0.3098 - val_binary_accuracy: 0.8655\n",
      "Epoch 36/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3312 - binary_accuracy: 0.8553\n",
      "Epoch 36: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3316 - binary_accuracy: 0.8551 - val_loss: 0.3328 - val_binary_accuracy: 0.8638\n",
      "Epoch 37/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3292 - binary_accuracy: 0.8568\n",
      "Epoch 37: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3292 - binary_accuracy: 0.8568 - val_loss: 0.3251 - val_binary_accuracy: 0.8644\n",
      "Epoch 38/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3250 - binary_accuracy: 0.8568\n",
      "Epoch 38: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3250 - binary_accuracy: 0.8567 - val_loss: 0.3067 - val_binary_accuracy: 0.8671\n",
      "Epoch 39/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3275 - binary_accuracy: 0.8572\n",
      "Epoch 39: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3277 - binary_accuracy: 0.8570 - val_loss: 0.3427 - val_binary_accuracy: 0.8360\n",
      "Epoch 40/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3305 - binary_accuracy: 0.8535\n",
      "Epoch 40: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3305 - binary_accuracy: 0.8535 - val_loss: 0.3178 - val_binary_accuracy: 0.8644\n",
      "Epoch 41/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3220 - binary_accuracy: 0.8550\n",
      "Epoch 41: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3218 - binary_accuracy: 0.8549 - val_loss: 0.3019 - val_binary_accuracy: 0.8635\n",
      "Epoch 42/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3253 - binary_accuracy: 0.8552\n",
      "Epoch 42: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3251 - binary_accuracy: 0.8552 - val_loss: 0.3037 - val_binary_accuracy: 0.8641\n",
      "Epoch 43/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3190 - binary_accuracy: 0.8599\n",
      "Epoch 43: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3193 - binary_accuracy: 0.8596 - val_loss: 0.3122 - val_binary_accuracy: 0.8550\n",
      "Epoch 44/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3170 - binary_accuracy: 0.8612\n",
      "Epoch 44: val_binary_accuracy did not improve from 0.86766\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3172 - binary_accuracy: 0.8611 - val_loss: 0.3039 - val_binary_accuracy: 0.8630\n",
      "Epoch 45/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3204 - binary_accuracy: 0.8577\n",
      "Epoch 45: val_binary_accuracy improved from 0.86766 to 0.86959, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3200 - binary_accuracy: 0.8579 - val_loss: 0.2965 - val_binary_accuracy: 0.8696\n",
      "Epoch 46/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3230 - binary_accuracy: 0.8564\n",
      "Epoch 46: val_binary_accuracy did not improve from 0.86959\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3230 - binary_accuracy: 0.8564 - val_loss: 0.3136 - val_binary_accuracy: 0.8550\n",
      "Epoch 47/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3192 - binary_accuracy: 0.8600\n",
      "Epoch 47: val_binary_accuracy improved from 0.86959 to 0.87152, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3198 - binary_accuracy: 0.8598 - val_loss: 0.2970 - val_binary_accuracy: 0.8715\n",
      "Epoch 48/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3217 - binary_accuracy: 0.8591\n",
      "Epoch 48: val_binary_accuracy did not improve from 0.87152\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3217 - binary_accuracy: 0.8591 - val_loss: 0.2946 - val_binary_accuracy: 0.8660\n",
      "Epoch 49/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3237 - binary_accuracy: 0.8578\n",
      "Epoch 49: val_binary_accuracy did not improve from 0.87152\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3234 - binary_accuracy: 0.8579 - val_loss: 0.3036 - val_binary_accuracy: 0.8696\n",
      "Epoch 50/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3167 - binary_accuracy: 0.8595\n",
      "Epoch 50: val_binary_accuracy did not improve from 0.87152\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3167 - binary_accuracy: 0.8595 - val_loss: 0.2921 - val_binary_accuracy: 0.8668\n",
      "Epoch 51/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3198 - binary_accuracy: 0.8577\n",
      "Epoch 51: val_binary_accuracy did not improve from 0.87152\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3206 - binary_accuracy: 0.8574 - val_loss: 0.3191 - val_binary_accuracy: 0.8536\n",
      "Epoch 52/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3163 - binary_accuracy: 0.8616\n",
      "Epoch 52: val_binary_accuracy did not improve from 0.87152\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3166 - binary_accuracy: 0.8617 - val_loss: 0.3038 - val_binary_accuracy: 0.8682\n",
      "Epoch 53/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3153 - binary_accuracy: 0.8637\n",
      "Epoch 53: val_binary_accuracy did not improve from 0.87152\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3152 - binary_accuracy: 0.8636 - val_loss: 0.2984 - val_binary_accuracy: 0.8674\n",
      "Epoch 54/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3152 - binary_accuracy: 0.8625\n",
      "Epoch 54: val_binary_accuracy improved from 0.87152 to 0.87235, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3152 - binary_accuracy: 0.8624 - val_loss: 0.2984 - val_binary_accuracy: 0.8723\n",
      "Epoch 55/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3175 - binary_accuracy: 0.8615\n",
      "Epoch 55: val_binary_accuracy did not improve from 0.87235\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3176 - binary_accuracy: 0.8615 - val_loss: 0.2967 - val_binary_accuracy: 0.8699\n",
      "Epoch 56/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3156 - binary_accuracy: 0.8613\n",
      "Epoch 56: val_binary_accuracy did not improve from 0.87235\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3150 - binary_accuracy: 0.8617 - val_loss: 0.3019 - val_binary_accuracy: 0.8666\n",
      "Epoch 57/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3138 - binary_accuracy: 0.8622\n",
      "Epoch 57: val_binary_accuracy did not improve from 0.87235\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3142 - binary_accuracy: 0.8621 - val_loss: 0.3156 - val_binary_accuracy: 0.8721\n",
      "Epoch 58/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3136 - binary_accuracy: 0.8616\n",
      "Epoch 58: val_binary_accuracy did not improve from 0.87235\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3134 - binary_accuracy: 0.8616 - val_loss: 0.2885 - val_binary_accuracy: 0.8715\n",
      "Epoch 59/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3125 - binary_accuracy: 0.8607\n",
      "Epoch 59: val_binary_accuracy improved from 0.87235 to 0.87262, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3123 - binary_accuracy: 0.8607 - val_loss: 0.3074 - val_binary_accuracy: 0.8726\n",
      "Epoch 60/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3080 - binary_accuracy: 0.8645\n",
      "Epoch 60: val_binary_accuracy improved from 0.87262 to 0.87317, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3079 - binary_accuracy: 0.8645 - val_loss: 0.2908 - val_binary_accuracy: 0.8732\n",
      "Epoch 61/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3080 - binary_accuracy: 0.8649\n",
      "Epoch 61: val_binary_accuracy improved from 0.87317 to 0.87428, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3076 - binary_accuracy: 0.8649 - val_loss: 0.2952 - val_binary_accuracy: 0.8743\n",
      "Epoch 62/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3178 - binary_accuracy: 0.8596\n",
      "Epoch 62: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3178 - binary_accuracy: 0.8596 - val_loss: 0.2873 - val_binary_accuracy: 0.8743\n",
      "Epoch 63/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3055 - binary_accuracy: 0.8655\n",
      "Epoch 63: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3055 - binary_accuracy: 0.8655 - val_loss: 0.3240 - val_binary_accuracy: 0.8442\n",
      "Epoch 64/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3118 - binary_accuracy: 0.8634\n",
      "Epoch 64: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3123 - binary_accuracy: 0.8632 - val_loss: 0.2967 - val_binary_accuracy: 0.8732\n",
      "Epoch 65/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3078 - binary_accuracy: 0.8645\n",
      "Epoch 65: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3082 - binary_accuracy: 0.8644 - val_loss: 0.2993 - val_binary_accuracy: 0.8674\n",
      "Epoch 66/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3101 - binary_accuracy: 0.8627\n",
      "Epoch 66: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3101 - binary_accuracy: 0.8627 - val_loss: 0.2967 - val_binary_accuracy: 0.8715\n",
      "Epoch 67/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3097 - binary_accuracy: 0.8658\n",
      "Epoch 67: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3095 - binary_accuracy: 0.8660 - val_loss: 0.2932 - val_binary_accuracy: 0.8699\n",
      "Epoch 68/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3009 - binary_accuracy: 0.8685\n",
      "Epoch 68: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3016 - binary_accuracy: 0.8682 - val_loss: 0.2921 - val_binary_accuracy: 0.8715\n",
      "Epoch 69/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.3101 - binary_accuracy: 0.8630\n",
      "Epoch 69: val_binary_accuracy did not improve from 0.87428\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3101 - binary_accuracy: 0.8629 - val_loss: 0.2866 - val_binary_accuracy: 0.8737\n",
      "Epoch 70/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3064 - binary_accuracy: 0.8656\n",
      "Epoch 70: val_binary_accuracy improved from 0.87428 to 0.87841, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3063 - binary_accuracy: 0.8656 - val_loss: 0.2857 - val_binary_accuracy: 0.8784\n",
      "Epoch 71/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3058 - binary_accuracy: 0.8657\n",
      "Epoch 71: val_binary_accuracy did not improve from 0.87841\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3058 - binary_accuracy: 0.8657 - val_loss: 0.2991 - val_binary_accuracy: 0.8630\n",
      "Epoch 72/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3049 - binary_accuracy: 0.8654\n",
      "Epoch 72: val_binary_accuracy did not improve from 0.87841\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3049 - binary_accuracy: 0.8654 - val_loss: 0.2889 - val_binary_accuracy: 0.8746\n",
      "Epoch 73/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3090 - binary_accuracy: 0.8652\n",
      "Epoch 73: val_binary_accuracy did not improve from 0.87841\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3087 - binary_accuracy: 0.8655 - val_loss: 0.2898 - val_binary_accuracy: 0.8746\n",
      "Epoch 74/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3051 - binary_accuracy: 0.8676\n",
      "Epoch 74: val_binary_accuracy did not improve from 0.87841\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3049 - binary_accuracy: 0.8679 - val_loss: 0.2931 - val_binary_accuracy: 0.8707\n",
      "Epoch 75/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3014 - binary_accuracy: 0.8670\n",
      "Epoch 75: val_binary_accuracy did not improve from 0.87841\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3015 - binary_accuracy: 0.8669 - val_loss: 0.2969 - val_binary_accuracy: 0.8759\n",
      "Epoch 76/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3042 - binary_accuracy: 0.8705\n",
      "Epoch 76: val_binary_accuracy did not improve from 0.87841\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3040 - binary_accuracy: 0.8704 - val_loss: 0.2875 - val_binary_accuracy: 0.8743\n",
      "Epoch 77/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3026 - binary_accuracy: 0.8678\n",
      "Epoch 77: val_binary_accuracy improved from 0.87841 to 0.88034, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3027 - binary_accuracy: 0.8680 - val_loss: 0.3000 - val_binary_accuracy: 0.8803\n",
      "Epoch 78/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3095 - binary_accuracy: 0.8652\n",
      "Epoch 78: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3095 - binary_accuracy: 0.8652 - val_loss: 0.3022 - val_binary_accuracy: 0.8663\n",
      "Epoch 79/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2999 - binary_accuracy: 0.8682\n",
      "Epoch 79: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3000 - binary_accuracy: 0.8681 - val_loss: 0.2890 - val_binary_accuracy: 0.8734\n",
      "Epoch 80/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2994 - binary_accuracy: 0.8713\n",
      "Epoch 80: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2990 - binary_accuracy: 0.8715 - val_loss: 0.2899 - val_binary_accuracy: 0.8754\n",
      "Epoch 81/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.3015 - binary_accuracy: 0.8677\n",
      "Epoch 81: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3016 - binary_accuracy: 0.8675 - val_loss: 0.2914 - val_binary_accuracy: 0.8668\n",
      "Epoch 82/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3020 - binary_accuracy: 0.8675\n",
      "Epoch 82: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3020 - binary_accuracy: 0.8673 - val_loss: 0.2925 - val_binary_accuracy: 0.8632\n",
      "Epoch 83/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.3027 - binary_accuracy: 0.8668\n",
      "Epoch 83: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3029 - binary_accuracy: 0.8668 - val_loss: 0.2918 - val_binary_accuracy: 0.8787\n",
      "Epoch 84/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3026 - binary_accuracy: 0.8687\n",
      "Epoch 84: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.3026 - binary_accuracy: 0.8687 - val_loss: 0.2893 - val_binary_accuracy: 0.8715\n",
      "Epoch 85/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2988 - binary_accuracy: 0.8673\n",
      "Epoch 85: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2985 - binary_accuracy: 0.8673 - val_loss: 0.2882 - val_binary_accuracy: 0.8765\n",
      "Epoch 86/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.3005 - binary_accuracy: 0.8686\n",
      "Epoch 86: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.3005 - binary_accuracy: 0.8686 - val_loss: 0.3051 - val_binary_accuracy: 0.8580\n",
      "Epoch 87/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.3015 - binary_accuracy: 0.8692\n",
      "Epoch 87: val_binary_accuracy did not improve from 0.88034\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3010 - binary_accuracy: 0.8691 - val_loss: 0.3007 - val_binary_accuracy: 0.8693\n",
      "Epoch 88/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2971 - binary_accuracy: 0.8703\n",
      "Epoch 88: val_binary_accuracy improved from 0.88034 to 0.88089, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2971 - binary_accuracy: 0.8703 - val_loss: 0.2870 - val_binary_accuracy: 0.8809\n",
      "Epoch 89/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2968 - binary_accuracy: 0.8712\n",
      "Epoch 89: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2968 - binary_accuracy: 0.8712 - val_loss: 0.2843 - val_binary_accuracy: 0.8746\n",
      "Epoch 90/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2993 - binary_accuracy: 0.8677\n",
      "Epoch 90: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2993 - binary_accuracy: 0.8679 - val_loss: 0.2770 - val_binary_accuracy: 0.8770\n",
      "Epoch 91/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2965 - binary_accuracy: 0.8698\n",
      "Epoch 91: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2969 - binary_accuracy: 0.8696 - val_loss: 0.2911 - val_binary_accuracy: 0.8721\n",
      "Epoch 92/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2960 - binary_accuracy: 0.8687\n",
      "Epoch 92: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2960 - binary_accuracy: 0.8687 - val_loss: 0.2786 - val_binary_accuracy: 0.8787\n",
      "Epoch 93/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2976 - binary_accuracy: 0.8689\n",
      "Epoch 93: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2976 - binary_accuracy: 0.8690 - val_loss: 0.3002 - val_binary_accuracy: 0.8668\n",
      "Epoch 94/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2979 - binary_accuracy: 0.8684\n",
      "Epoch 94: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2982 - binary_accuracy: 0.8682 - val_loss: 0.2973 - val_binary_accuracy: 0.8644\n",
      "Epoch 95/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2948 - binary_accuracy: 0.8708\n",
      "Epoch 95: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2946 - binary_accuracy: 0.8708 - val_loss: 0.2817 - val_binary_accuracy: 0.8737\n",
      "Epoch 96/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2943 - binary_accuracy: 0.8717\n",
      "Epoch 96: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2947 - binary_accuracy: 0.8715 - val_loss: 0.2796 - val_binary_accuracy: 0.8773\n",
      "Epoch 97/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2946 - binary_accuracy: 0.8693\n",
      "Epoch 97: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2944 - binary_accuracy: 0.8693 - val_loss: 0.2838 - val_binary_accuracy: 0.8743\n",
      "Epoch 98/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2970 - binary_accuracy: 0.8699\n",
      "Epoch 98: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2969 - binary_accuracy: 0.8701 - val_loss: 0.2794 - val_binary_accuracy: 0.8781\n",
      "Epoch 99/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2998 - binary_accuracy: 0.8702\n",
      "Epoch 99: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.3003 - binary_accuracy: 0.8700 - val_loss: 0.2900 - val_binary_accuracy: 0.8776\n",
      "Epoch 100/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2968 - binary_accuracy: 0.8695\n",
      "Epoch 100: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2966 - binary_accuracy: 0.8697 - val_loss: 0.2876 - val_binary_accuracy: 0.8795\n",
      "Epoch 101/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2904 - binary_accuracy: 0.8757\n",
      "Epoch 101: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2904 - binary_accuracy: 0.8757 - val_loss: 0.2914 - val_binary_accuracy: 0.8781\n",
      "Epoch 102/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2978 - binary_accuracy: 0.8705\n",
      "Epoch 102: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2978 - binary_accuracy: 0.8705 - val_loss: 0.2806 - val_binary_accuracy: 0.8784\n",
      "Epoch 103/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2943 - binary_accuracy: 0.8729\n",
      "Epoch 103: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2942 - binary_accuracy: 0.8730 - val_loss: 0.2770 - val_binary_accuracy: 0.8790\n",
      "Epoch 104/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2904 - binary_accuracy: 0.8724\n",
      "Epoch 104: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2907 - binary_accuracy: 0.8723 - val_loss: 0.2838 - val_binary_accuracy: 0.8768\n",
      "Epoch 105/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2946 - binary_accuracy: 0.8721\n",
      "Epoch 105: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2943 - binary_accuracy: 0.8722 - val_loss: 0.2792 - val_binary_accuracy: 0.8726\n",
      "Epoch 106/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2934 - binary_accuracy: 0.8711\n",
      "Epoch 106: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2940 - binary_accuracy: 0.8710 - val_loss: 0.2857 - val_binary_accuracy: 0.8759\n",
      "Epoch 107/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2935 - binary_accuracy: 0.8723\n",
      "Epoch 107: val_binary_accuracy did not improve from 0.88089\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2942 - binary_accuracy: 0.8721 - val_loss: 0.2783 - val_binary_accuracy: 0.8803\n",
      "Epoch 108/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2975 - binary_accuracy: 0.8707\n",
      "Epoch 108: val_binary_accuracy improved from 0.88089 to 0.88200, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2978 - binary_accuracy: 0.8707 - val_loss: 0.2828 - val_binary_accuracy: 0.8820\n",
      "Epoch 109/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2898 - binary_accuracy: 0.8739\n",
      "Epoch 109: val_binary_accuracy did not improve from 0.88200\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2905 - binary_accuracy: 0.8737 - val_loss: 0.3045 - val_binary_accuracy: 0.8693\n",
      "Epoch 110/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2882 - binary_accuracy: 0.8737\n",
      "Epoch 110: val_binary_accuracy improved from 0.88200 to 0.88393, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2882 - binary_accuracy: 0.8737 - val_loss: 0.2738 - val_binary_accuracy: 0.8839\n",
      "Epoch 111/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2914 - binary_accuracy: 0.8698\n",
      "Epoch 111: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2914 - binary_accuracy: 0.8698 - val_loss: 0.2729 - val_binary_accuracy: 0.8795\n",
      "Epoch 112/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2879 - binary_accuracy: 0.8743\n",
      "Epoch 112: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2880 - binary_accuracy: 0.8742 - val_loss: 0.3018 - val_binary_accuracy: 0.8575\n",
      "Epoch 113/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2901 - binary_accuracy: 0.8743\n",
      "Epoch 113: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2901 - binary_accuracy: 0.8743 - val_loss: 0.2842 - val_binary_accuracy: 0.8839\n",
      "Epoch 114/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2928 - binary_accuracy: 0.8728\n",
      "Epoch 114: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2929 - binary_accuracy: 0.8728 - val_loss: 0.2822 - val_binary_accuracy: 0.8803\n",
      "Epoch 115/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2918 - binary_accuracy: 0.8717\n",
      "Epoch 115: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2919 - binary_accuracy: 0.8715 - val_loss: 0.2800 - val_binary_accuracy: 0.8795\n",
      "Epoch 116/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2884 - binary_accuracy: 0.8757\n",
      "Epoch 116: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2880 - binary_accuracy: 0.8761 - val_loss: 0.2789 - val_binary_accuracy: 0.8770\n",
      "Epoch 117/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2871 - binary_accuracy: 0.8758\n",
      "Epoch 117: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2871 - binary_accuracy: 0.8758 - val_loss: 0.2928 - val_binary_accuracy: 0.8712\n",
      "Epoch 118/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2897 - binary_accuracy: 0.8731\n",
      "Epoch 118: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2898 - binary_accuracy: 0.8729 - val_loss: 0.2852 - val_binary_accuracy: 0.8726\n",
      "Epoch 119/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2833 - binary_accuracy: 0.8757\n",
      "Epoch 119: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2833 - binary_accuracy: 0.8756 - val_loss: 0.2771 - val_binary_accuracy: 0.8765\n",
      "Epoch 120/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2887 - binary_accuracy: 0.8743\n",
      "Epoch 120: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2887 - binary_accuracy: 0.8743 - val_loss: 0.2799 - val_binary_accuracy: 0.8699\n",
      "Epoch 121/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2859 - binary_accuracy: 0.8759\n",
      "Epoch 121: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2860 - binary_accuracy: 0.8759 - val_loss: 0.2815 - val_binary_accuracy: 0.8743\n",
      "Epoch 122/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2890 - binary_accuracy: 0.8730\n",
      "Epoch 122: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2889 - binary_accuracy: 0.8731 - val_loss: 0.2761 - val_binary_accuracy: 0.8729\n",
      "Epoch 123/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2875 - binary_accuracy: 0.8748\n",
      "Epoch 123: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2878 - binary_accuracy: 0.8747 - val_loss: 0.2757 - val_binary_accuracy: 0.8795\n",
      "Epoch 124/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2889 - binary_accuracy: 0.8761\n",
      "Epoch 124: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2884 - binary_accuracy: 0.8762 - val_loss: 0.2752 - val_binary_accuracy: 0.8779\n",
      "Epoch 125/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2881 - binary_accuracy: 0.8736\n",
      "Epoch 125: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2882 - binary_accuracy: 0.8735 - val_loss: 0.2817 - val_binary_accuracy: 0.8801\n",
      "Epoch 126/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2869 - binary_accuracy: 0.8761\n",
      "Epoch 126: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2869 - binary_accuracy: 0.8761 - val_loss: 0.2814 - val_binary_accuracy: 0.8828\n",
      "Epoch 127/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2840 - binary_accuracy: 0.8738\n",
      "Epoch 127: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2841 - binary_accuracy: 0.8737 - val_loss: 0.2804 - val_binary_accuracy: 0.8812\n",
      "Epoch 128/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2868 - binary_accuracy: 0.8758\n",
      "Epoch 128: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2869 - binary_accuracy: 0.8757 - val_loss: 0.2924 - val_binary_accuracy: 0.8737\n",
      "Epoch 129/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2865 - binary_accuracy: 0.8757\n",
      "Epoch 129: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2864 - binary_accuracy: 0.8755 - val_loss: 0.2948 - val_binary_accuracy: 0.8718\n",
      "Epoch 130/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2824 - binary_accuracy: 0.8761\n",
      "Epoch 130: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2824 - binary_accuracy: 0.8761 - val_loss: 0.2899 - val_binary_accuracy: 0.8666\n",
      "Epoch 131/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2826 - binary_accuracy: 0.8759\n",
      "Epoch 131: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2828 - binary_accuracy: 0.8757 - val_loss: 0.2748 - val_binary_accuracy: 0.8814\n",
      "Epoch 132/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2839 - binary_accuracy: 0.8744\n",
      "Epoch 132: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2840 - binary_accuracy: 0.8745 - val_loss: 0.2819 - val_binary_accuracy: 0.8729\n",
      "Epoch 133/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2937 - binary_accuracy: 0.8717\n",
      "Epoch 133: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2934 - binary_accuracy: 0.8719 - val_loss: 0.2885 - val_binary_accuracy: 0.8784\n",
      "Epoch 134/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2859 - binary_accuracy: 0.8747\n",
      "Epoch 134: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2862 - binary_accuracy: 0.8746 - val_loss: 0.2787 - val_binary_accuracy: 0.8726\n",
      "Epoch 135/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2814 - binary_accuracy: 0.8750\n",
      "Epoch 135: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2810 - binary_accuracy: 0.8753 - val_loss: 0.2846 - val_binary_accuracy: 0.8773\n",
      "Epoch 136/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2835 - binary_accuracy: 0.8751\n",
      "Epoch 136: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2835 - binary_accuracy: 0.8751 - val_loss: 0.3081 - val_binary_accuracy: 0.8632\n",
      "Epoch 137/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2823 - binary_accuracy: 0.8777\n",
      "Epoch 137: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2823 - binary_accuracy: 0.8777 - val_loss: 0.2791 - val_binary_accuracy: 0.8806\n",
      "Epoch 138/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2829 - binary_accuracy: 0.8766\n",
      "Epoch 138: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2829 - binary_accuracy: 0.8768 - val_loss: 0.2854 - val_binary_accuracy: 0.8795\n",
      "Epoch 139/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2826 - binary_accuracy: 0.8760\n",
      "Epoch 139: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2826 - binary_accuracy: 0.8760 - val_loss: 0.2785 - val_binary_accuracy: 0.8787\n",
      "Epoch 140/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2828 - binary_accuracy: 0.8770\n",
      "Epoch 140: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2822 - binary_accuracy: 0.8772 - val_loss: 0.2768 - val_binary_accuracy: 0.8809\n",
      "Epoch 141/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2834 - binary_accuracy: 0.8771\n",
      "Epoch 141: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2834 - binary_accuracy: 0.8771 - val_loss: 0.2890 - val_binary_accuracy: 0.8746\n",
      "Epoch 142/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2848 - binary_accuracy: 0.8763\n",
      "Epoch 142: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2843 - binary_accuracy: 0.8766 - val_loss: 0.2850 - val_binary_accuracy: 0.8743\n",
      "Epoch 143/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2792 - binary_accuracy: 0.8787\n",
      "Epoch 143: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2791 - binary_accuracy: 0.8785 - val_loss: 0.2871 - val_binary_accuracy: 0.8817\n",
      "Epoch 144/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2819 - binary_accuracy: 0.8762\n",
      "Epoch 144: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2819 - binary_accuracy: 0.8762 - val_loss: 0.2806 - val_binary_accuracy: 0.8757\n",
      "Epoch 145/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2841 - binary_accuracy: 0.8758\n",
      "Epoch 145: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2841 - binary_accuracy: 0.8758 - val_loss: 0.2835 - val_binary_accuracy: 0.8790\n",
      "Epoch 146/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2786 - binary_accuracy: 0.8787\n",
      "Epoch 146: val_binary_accuracy did not improve from 0.88393\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2789 - binary_accuracy: 0.8784 - val_loss: 0.2769 - val_binary_accuracy: 0.8787\n",
      "Epoch 147/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2824 - binary_accuracy: 0.8750\n",
      "Epoch 147: val_binary_accuracy improved from 0.88393 to 0.88586, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2824 - binary_accuracy: 0.8750 - val_loss: 0.2877 - val_binary_accuracy: 0.8859\n",
      "Epoch 148/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2828 - binary_accuracy: 0.8776\n",
      "Epoch 148: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2827 - binary_accuracy: 0.8778 - val_loss: 0.2742 - val_binary_accuracy: 0.8790\n",
      "Epoch 149/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2828 - binary_accuracy: 0.8781\n",
      "Epoch 149: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2827 - binary_accuracy: 0.8780 - val_loss: 0.2778 - val_binary_accuracy: 0.8759\n",
      "Epoch 150/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2855 - binary_accuracy: 0.8760\n",
      "Epoch 150: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2854 - binary_accuracy: 0.8760 - val_loss: 0.2772 - val_binary_accuracy: 0.8820\n",
      "Epoch 151/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2773 - binary_accuracy: 0.8800\n",
      "Epoch 151: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2773 - binary_accuracy: 0.8799 - val_loss: 0.2783 - val_binary_accuracy: 0.8770\n",
      "Epoch 152/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2810 - binary_accuracy: 0.8757\n",
      "Epoch 152: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2810 - binary_accuracy: 0.8757 - val_loss: 0.3011 - val_binary_accuracy: 0.8726\n",
      "Epoch 153/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2825 - binary_accuracy: 0.8761\n",
      "Epoch 153: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2826 - binary_accuracy: 0.8762 - val_loss: 0.2871 - val_binary_accuracy: 0.8718\n",
      "Epoch 154/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2784 - binary_accuracy: 0.8791\n",
      "Epoch 154: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2785 - binary_accuracy: 0.8791 - val_loss: 0.2747 - val_binary_accuracy: 0.8814\n",
      "Epoch 155/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2791 - binary_accuracy: 0.8771\n",
      "Epoch 155: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2793 - binary_accuracy: 0.8770 - val_loss: 0.2806 - val_binary_accuracy: 0.8776\n",
      "Epoch 156/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2770 - binary_accuracy: 0.8790\n",
      "Epoch 156: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2770 - binary_accuracy: 0.8791 - val_loss: 0.2851 - val_binary_accuracy: 0.8757\n",
      "Epoch 157/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2798 - binary_accuracy: 0.8774\n",
      "Epoch 157: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2798 - binary_accuracy: 0.8774 - val_loss: 0.2845 - val_binary_accuracy: 0.8677\n",
      "Epoch 158/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2784 - binary_accuracy: 0.8782\n",
      "Epoch 158: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2787 - binary_accuracy: 0.8781 - val_loss: 0.2763 - val_binary_accuracy: 0.8790\n",
      "Epoch 159/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2761 - binary_accuracy: 0.8795\n",
      "Epoch 159: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2765 - binary_accuracy: 0.8793 - val_loss: 0.2875 - val_binary_accuracy: 0.8712\n",
      "Epoch 160/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2802 - binary_accuracy: 0.8763\n",
      "Epoch 160: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2804 - binary_accuracy: 0.8761 - val_loss: 0.2734 - val_binary_accuracy: 0.8806\n",
      "Epoch 161/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2775 - binary_accuracy: 0.8781\n",
      "Epoch 161: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2776 - binary_accuracy: 0.8779 - val_loss: 0.2746 - val_binary_accuracy: 0.8817\n",
      "Epoch 162/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2785 - binary_accuracy: 0.8766\n",
      "Epoch 162: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2787 - binary_accuracy: 0.8766 - val_loss: 0.2886 - val_binary_accuracy: 0.8737\n",
      "Epoch 163/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2788 - binary_accuracy: 0.8776\n",
      "Epoch 163: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2785 - binary_accuracy: 0.8777 - val_loss: 0.2751 - val_binary_accuracy: 0.8790\n",
      "Epoch 164/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2729 - binary_accuracy: 0.8787\n",
      "Epoch 164: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2732 - binary_accuracy: 0.8787 - val_loss: 0.2810 - val_binary_accuracy: 0.8828\n",
      "Epoch 165/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2771 - binary_accuracy: 0.8803\n",
      "Epoch 165: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2770 - binary_accuracy: 0.8804 - val_loss: 0.2791 - val_binary_accuracy: 0.8779\n",
      "Epoch 166/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2787 - binary_accuracy: 0.8769\n",
      "Epoch 166: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2787 - binary_accuracy: 0.8769 - val_loss: 0.2781 - val_binary_accuracy: 0.8803\n",
      "Epoch 167/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2774 - binary_accuracy: 0.8785\n",
      "Epoch 167: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2773 - binary_accuracy: 0.8784 - val_loss: 0.2765 - val_binary_accuracy: 0.8825\n",
      "Epoch 168/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2731 - binary_accuracy: 0.8809\n",
      "Epoch 168: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2732 - binary_accuracy: 0.8807 - val_loss: 0.2763 - val_binary_accuracy: 0.8798\n",
      "Epoch 169/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2782 - binary_accuracy: 0.8781\n",
      "Epoch 169: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2785 - binary_accuracy: 0.8780 - val_loss: 0.2732 - val_binary_accuracy: 0.8812\n",
      "Epoch 170/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2762 - binary_accuracy: 0.8785\n",
      "Epoch 170: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2762 - binary_accuracy: 0.8785 - val_loss: 0.2729 - val_binary_accuracy: 0.8817\n",
      "Epoch 171/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2785 - binary_accuracy: 0.8774\n",
      "Epoch 171: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2782 - binary_accuracy: 0.8776 - val_loss: 0.2751 - val_binary_accuracy: 0.8792\n",
      "Epoch 172/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2761 - binary_accuracy: 0.8789\n",
      "Epoch 172: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2762 - binary_accuracy: 0.8786 - val_loss: 0.2780 - val_binary_accuracy: 0.8784\n",
      "Epoch 173/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2733 - binary_accuracy: 0.8788\n",
      "Epoch 173: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2733 - binary_accuracy: 0.8788 - val_loss: 0.2777 - val_binary_accuracy: 0.8792\n",
      "Epoch 174/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2735 - binary_accuracy: 0.8800\n",
      "Epoch 174: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2737 - binary_accuracy: 0.8799 - val_loss: 0.2690 - val_binary_accuracy: 0.8809\n",
      "Epoch 175/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2741 - binary_accuracy: 0.8805\n",
      "Epoch 175: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2736 - binary_accuracy: 0.8806 - val_loss: 0.2807 - val_binary_accuracy: 0.8853\n",
      "Epoch 176/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2768 - binary_accuracy: 0.8790\n",
      "Epoch 176: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2767 - binary_accuracy: 0.8790 - val_loss: 0.2768 - val_binary_accuracy: 0.8817\n",
      "Epoch 177/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2788 - binary_accuracy: 0.8769\n",
      "Epoch 177: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2788 - binary_accuracy: 0.8769 - val_loss: 0.2754 - val_binary_accuracy: 0.8765\n",
      "Epoch 178/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2723 - binary_accuracy: 0.8800\n",
      "Epoch 178: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2723 - binary_accuracy: 0.8799 - val_loss: 0.2840 - val_binary_accuracy: 0.8757\n",
      "Epoch 179/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2746 - binary_accuracy: 0.8777\n",
      "Epoch 179: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2745 - binary_accuracy: 0.8778 - val_loss: 0.2800 - val_binary_accuracy: 0.8663\n",
      "Epoch 180/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2800 - binary_accuracy: 0.8772\n",
      "Epoch 180: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2800 - binary_accuracy: 0.8773 - val_loss: 0.2761 - val_binary_accuracy: 0.8820\n",
      "Epoch 181/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2758 - binary_accuracy: 0.8771\n",
      "Epoch 181: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2757 - binary_accuracy: 0.8771 - val_loss: 0.2784 - val_binary_accuracy: 0.8721\n",
      "Epoch 182/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2759 - binary_accuracy: 0.8787\n",
      "Epoch 182: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2759 - binary_accuracy: 0.8788 - val_loss: 0.2773 - val_binary_accuracy: 0.8787\n",
      "Epoch 183/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2762 - binary_accuracy: 0.8793\n",
      "Epoch 183: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2760 - binary_accuracy: 0.8794 - val_loss: 0.2768 - val_binary_accuracy: 0.8814\n",
      "Epoch 184/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2741 - binary_accuracy: 0.8802\n",
      "Epoch 184: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2740 - binary_accuracy: 0.8803 - val_loss: 0.2867 - val_binary_accuracy: 0.8806\n",
      "Epoch 185/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2730 - binary_accuracy: 0.8805\n",
      "Epoch 185: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2734 - binary_accuracy: 0.8802 - val_loss: 0.2756 - val_binary_accuracy: 0.8776\n",
      "Epoch 186/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2717 - binary_accuracy: 0.8811\n",
      "Epoch 186: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2717 - binary_accuracy: 0.8811 - val_loss: 0.2734 - val_binary_accuracy: 0.8790\n",
      "Epoch 187/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2762 - binary_accuracy: 0.8794\n",
      "Epoch 187: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2762 - binary_accuracy: 0.8794 - val_loss: 0.2879 - val_binary_accuracy: 0.8721\n",
      "Epoch 188/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2740 - binary_accuracy: 0.8779\n",
      "Epoch 188: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2739 - binary_accuracy: 0.8778 - val_loss: 0.2746 - val_binary_accuracy: 0.8845\n",
      "Epoch 189/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2775 - binary_accuracy: 0.8789\n",
      "Epoch 189: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2772 - binary_accuracy: 0.8788 - val_loss: 0.2767 - val_binary_accuracy: 0.8765\n",
      "Epoch 190/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2720 - binary_accuracy: 0.8841\n",
      "Epoch 190: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2720 - binary_accuracy: 0.8841 - val_loss: 0.2795 - val_binary_accuracy: 0.8770\n",
      "Epoch 191/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2709 - binary_accuracy: 0.8826\n",
      "Epoch 191: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2717 - binary_accuracy: 0.8824 - val_loss: 0.2842 - val_binary_accuracy: 0.8726\n",
      "Epoch 192/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2726 - binary_accuracy: 0.8806\n",
      "Epoch 192: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2725 - binary_accuracy: 0.8806 - val_loss: 0.2684 - val_binary_accuracy: 0.8806\n",
      "Epoch 193/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2688 - binary_accuracy: 0.8839\n",
      "Epoch 193: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2684 - binary_accuracy: 0.8840 - val_loss: 0.2812 - val_binary_accuracy: 0.8834\n",
      "Epoch 194/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2727 - binary_accuracy: 0.8802\n",
      "Epoch 194: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2730 - binary_accuracy: 0.8802 - val_loss: 0.2868 - val_binary_accuracy: 0.8792\n",
      "Epoch 195/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2717 - binary_accuracy: 0.8823\n",
      "Epoch 195: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2717 - binary_accuracy: 0.8823 - val_loss: 0.2757 - val_binary_accuracy: 0.8842\n",
      "Epoch 196/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2698 - binary_accuracy: 0.8821\n",
      "Epoch 196: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2698 - binary_accuracy: 0.8821 - val_loss: 0.2799 - val_binary_accuracy: 0.8848\n",
      "Epoch 197/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2712 - binary_accuracy: 0.8824\n",
      "Epoch 197: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2714 - binary_accuracy: 0.8823 - val_loss: 0.3089 - val_binary_accuracy: 0.8558\n",
      "Epoch 198/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2725 - binary_accuracy: 0.8803\n",
      "Epoch 198: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2726 - binary_accuracy: 0.8802 - val_loss: 0.2705 - val_binary_accuracy: 0.8806\n",
      "Epoch 199/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2705 - binary_accuracy: 0.8791\n",
      "Epoch 199: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2705 - binary_accuracy: 0.8791 - val_loss: 0.2850 - val_binary_accuracy: 0.8776\n",
      "Epoch 200/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2708 - binary_accuracy: 0.8813\n",
      "Epoch 200: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2708 - binary_accuracy: 0.8813 - val_loss: 0.2817 - val_binary_accuracy: 0.8806\n",
      "Epoch 201/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2701 - binary_accuracy: 0.8834\n",
      "Epoch 201: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2702 - binary_accuracy: 0.8832 - val_loss: 0.2759 - val_binary_accuracy: 0.8859\n",
      "Epoch 202/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2677 - binary_accuracy: 0.8830\n",
      "Epoch 202: val_binary_accuracy did not improve from 0.88586\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2671 - binary_accuracy: 0.8833 - val_loss: 0.2917 - val_binary_accuracy: 0.8781\n",
      "Epoch 203/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2734 - binary_accuracy: 0.8806\n",
      "Epoch 203: val_binary_accuracy improved from 0.88586 to 0.88613, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2732 - binary_accuracy: 0.8809 - val_loss: 0.2768 - val_binary_accuracy: 0.8861\n",
      "Epoch 204/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2718 - binary_accuracy: 0.8831\n",
      "Epoch 204: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2720 - binary_accuracy: 0.8828 - val_loss: 0.2792 - val_binary_accuracy: 0.8787\n",
      "Epoch 205/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2712 - binary_accuracy: 0.8823\n",
      "Epoch 205: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2715 - binary_accuracy: 0.8822 - val_loss: 0.2766 - val_binary_accuracy: 0.8795\n",
      "Epoch 206/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2698 - binary_accuracy: 0.8849\n",
      "Epoch 206: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2699 - binary_accuracy: 0.8848 - val_loss: 0.2785 - val_binary_accuracy: 0.8779\n",
      "Epoch 207/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2676 - binary_accuracy: 0.8840\n",
      "Epoch 207: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2675 - binary_accuracy: 0.8841 - val_loss: 0.2780 - val_binary_accuracy: 0.8828\n",
      "Epoch 208/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2682 - binary_accuracy: 0.8825\n",
      "Epoch 208: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2682 - binary_accuracy: 0.8824 - val_loss: 0.2822 - val_binary_accuracy: 0.8768\n",
      "Epoch 209/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2671 - binary_accuracy: 0.8835\n",
      "Epoch 209: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2671 - binary_accuracy: 0.8835 - val_loss: 0.2832 - val_binary_accuracy: 0.8759\n",
      "Epoch 210/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2710 - binary_accuracy: 0.8813\n",
      "Epoch 210: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2710 - binary_accuracy: 0.8813 - val_loss: 0.2750 - val_binary_accuracy: 0.8792\n",
      "Epoch 211/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2711 - binary_accuracy: 0.8835\n",
      "Epoch 211: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2714 - binary_accuracy: 0.8832 - val_loss: 0.2761 - val_binary_accuracy: 0.8768\n",
      "Epoch 212/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2676 - binary_accuracy: 0.8811\n",
      "Epoch 212: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2678 - binary_accuracy: 0.8810 - val_loss: 0.2835 - val_binary_accuracy: 0.8743\n",
      "Epoch 213/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2751 - binary_accuracy: 0.8801\n",
      "Epoch 213: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2751 - binary_accuracy: 0.8801 - val_loss: 0.2761 - val_binary_accuracy: 0.8850\n",
      "Epoch 214/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2719 - binary_accuracy: 0.8784\n",
      "Epoch 214: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2722 - binary_accuracy: 0.8784 - val_loss: 0.2760 - val_binary_accuracy: 0.8839\n",
      "Epoch 215/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2718 - binary_accuracy: 0.8825\n",
      "Epoch 215: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2719 - binary_accuracy: 0.8824 - val_loss: 0.2749 - val_binary_accuracy: 0.8850\n",
      "Epoch 216/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2679 - binary_accuracy: 0.8826\n",
      "Epoch 216: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2679 - binary_accuracy: 0.8824 - val_loss: 0.2767 - val_binary_accuracy: 0.8823\n",
      "Epoch 217/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2674 - binary_accuracy: 0.8826\n",
      "Epoch 217: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2675 - binary_accuracy: 0.8824 - val_loss: 0.2761 - val_binary_accuracy: 0.8814\n",
      "Epoch 218/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2651 - binary_accuracy: 0.8842\n",
      "Epoch 218: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2656 - binary_accuracy: 0.8841 - val_loss: 0.2816 - val_binary_accuracy: 0.8792\n",
      "Epoch 219/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2685 - binary_accuracy: 0.8823\n",
      "Epoch 219: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2687 - binary_accuracy: 0.8823 - val_loss: 0.2726 - val_binary_accuracy: 0.8839\n",
      "Epoch 220/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2705 - binary_accuracy: 0.8831\n",
      "Epoch 220: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2708 - binary_accuracy: 0.8828 - val_loss: 0.2848 - val_binary_accuracy: 0.8776\n",
      "Epoch 221/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2694 - binary_accuracy: 0.8833\n",
      "Epoch 221: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2691 - binary_accuracy: 0.8834 - val_loss: 0.2719 - val_binary_accuracy: 0.8842\n",
      "Epoch 222/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2710 - binary_accuracy: 0.8812\n",
      "Epoch 222: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2710 - binary_accuracy: 0.8812 - val_loss: 0.2793 - val_binary_accuracy: 0.8795\n",
      "Epoch 223/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2662 - binary_accuracy: 0.8827\n",
      "Epoch 223: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2663 - binary_accuracy: 0.8827 - val_loss: 0.2809 - val_binary_accuracy: 0.8812\n",
      "Epoch 224/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2697 - binary_accuracy: 0.8824\n",
      "Epoch 224: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2699 - binary_accuracy: 0.8822 - val_loss: 0.2800 - val_binary_accuracy: 0.8834\n",
      "Epoch 225/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2686 - binary_accuracy: 0.8816\n",
      "Epoch 225: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2686 - binary_accuracy: 0.8816 - val_loss: 0.2802 - val_binary_accuracy: 0.8754\n",
      "Epoch 226/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2654 - binary_accuracy: 0.8834\n",
      "Epoch 226: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2653 - binary_accuracy: 0.8835 - val_loss: 0.2727 - val_binary_accuracy: 0.8825\n",
      "Epoch 227/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2657 - binary_accuracy: 0.8848\n",
      "Epoch 227: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2657 - binary_accuracy: 0.8848 - val_loss: 0.2835 - val_binary_accuracy: 0.8809\n",
      "Epoch 228/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2680 - binary_accuracy: 0.8826\n",
      "Epoch 228: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2681 - binary_accuracy: 0.8827 - val_loss: 0.2753 - val_binary_accuracy: 0.8845\n",
      "Epoch 229/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2688 - binary_accuracy: 0.8806\n",
      "Epoch 229: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2693 - binary_accuracy: 0.8804 - val_loss: 0.2720 - val_binary_accuracy: 0.8759\n",
      "Epoch 230/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2658 - binary_accuracy: 0.8842\n",
      "Epoch 230: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2658 - binary_accuracy: 0.8842 - val_loss: 0.2770 - val_binary_accuracy: 0.8834\n",
      "Epoch 231/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2708 - binary_accuracy: 0.8828\n",
      "Epoch 231: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2700 - binary_accuracy: 0.8831 - val_loss: 0.2812 - val_binary_accuracy: 0.8770\n",
      "Epoch 232/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2735 - binary_accuracy: 0.8812\n",
      "Epoch 232: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2736 - binary_accuracy: 0.8813 - val_loss: 0.2719 - val_binary_accuracy: 0.8845\n",
      "Epoch 233/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2652 - binary_accuracy: 0.8835\n",
      "Epoch 233: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2654 - binary_accuracy: 0.8835 - val_loss: 0.2744 - val_binary_accuracy: 0.8850\n",
      "Epoch 234/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2688 - binary_accuracy: 0.8815\n",
      "Epoch 234: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2688 - binary_accuracy: 0.8815 - val_loss: 0.2954 - val_binary_accuracy: 0.8734\n",
      "Epoch 235/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2672 - binary_accuracy: 0.8835\n",
      "Epoch 235: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2672 - binary_accuracy: 0.8835 - val_loss: 0.2831 - val_binary_accuracy: 0.8781\n",
      "Epoch 236/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2671 - binary_accuracy: 0.8842\n",
      "Epoch 236: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2671 - binary_accuracy: 0.8842 - val_loss: 0.2861 - val_binary_accuracy: 0.8773\n",
      "Epoch 237/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2715 - binary_accuracy: 0.8811\n",
      "Epoch 237: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2714 - binary_accuracy: 0.8812 - val_loss: 0.2777 - val_binary_accuracy: 0.8812\n",
      "Epoch 238/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2654 - binary_accuracy: 0.8823\n",
      "Epoch 238: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2655 - binary_accuracy: 0.8822 - val_loss: 0.2751 - val_binary_accuracy: 0.8792\n",
      "Epoch 239/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2674 - binary_accuracy: 0.8823\n",
      "Epoch 239: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2674 - binary_accuracy: 0.8822 - val_loss: 0.2936 - val_binary_accuracy: 0.8759\n",
      "Epoch 240/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2648 - binary_accuracy: 0.8836\n",
      "Epoch 240: val_binary_accuracy did not improve from 0.88613\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2645 - binary_accuracy: 0.8837 - val_loss: 0.2823 - val_binary_accuracy: 0.8795\n",
      "Epoch 241/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2692 - binary_accuracy: 0.8826\n",
      "Epoch 241: val_binary_accuracy improved from 0.88613 to 0.88723, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2692 - binary_accuracy: 0.8826 - val_loss: 0.2701 - val_binary_accuracy: 0.8872\n",
      "Epoch 242/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2661 - binary_accuracy: 0.8831\n",
      "Epoch 242: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2663 - binary_accuracy: 0.8829 - val_loss: 0.2722 - val_binary_accuracy: 0.8859\n",
      "Epoch 243/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2648 - binary_accuracy: 0.8843\n",
      "Epoch 243: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2650 - binary_accuracy: 0.8842 - val_loss: 0.2728 - val_binary_accuracy: 0.8850\n",
      "Epoch 244/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2621 - binary_accuracy: 0.8851\n",
      "Epoch 244: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2620 - binary_accuracy: 0.8851 - val_loss: 0.2859 - val_binary_accuracy: 0.8792\n",
      "Epoch 245/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2663 - binary_accuracy: 0.8824\n",
      "Epoch 245: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2663 - binary_accuracy: 0.8824 - val_loss: 0.2875 - val_binary_accuracy: 0.8872\n",
      "Epoch 246/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2657 - binary_accuracy: 0.8840\n",
      "Epoch 246: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2653 - binary_accuracy: 0.8842 - val_loss: 0.2737 - val_binary_accuracy: 0.8812\n",
      "Epoch 247/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2658 - binary_accuracy: 0.8851\n",
      "Epoch 247: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2659 - binary_accuracy: 0.8853 - val_loss: 0.2773 - val_binary_accuracy: 0.8828\n",
      "Epoch 248/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2642 - binary_accuracy: 0.8844\n",
      "Epoch 248: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2642 - binary_accuracy: 0.8844 - val_loss: 0.2773 - val_binary_accuracy: 0.8790\n",
      "Epoch 249/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2649 - binary_accuracy: 0.8841\n",
      "Epoch 249: val_binary_accuracy did not improve from 0.88723\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2650 - binary_accuracy: 0.8841 - val_loss: 0.2783 - val_binary_accuracy: 0.8784\n",
      "Epoch 250/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2661 - binary_accuracy: 0.8808\n",
      "Epoch 250: val_binary_accuracy improved from 0.88723 to 0.88779, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2663 - binary_accuracy: 0.8807 - val_loss: 0.2730 - val_binary_accuracy: 0.8878\n",
      "Epoch 251/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2635 - binary_accuracy: 0.8831\n",
      "Epoch 251: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2640 - binary_accuracy: 0.8829 - val_loss: 0.2802 - val_binary_accuracy: 0.8806\n",
      "Epoch 252/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2632 - binary_accuracy: 0.8845\n",
      "Epoch 252: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2636 - binary_accuracy: 0.8842 - val_loss: 0.2811 - val_binary_accuracy: 0.8754\n",
      "Epoch 253/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2663 - binary_accuracy: 0.8841\n",
      "Epoch 253: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2661 - binary_accuracy: 0.8843 - val_loss: 0.2862 - val_binary_accuracy: 0.8828\n",
      "Epoch 254/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2682 - binary_accuracy: 0.8826\n",
      "Epoch 254: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2683 - binary_accuracy: 0.8826 - val_loss: 0.2757 - val_binary_accuracy: 0.8848\n",
      "Epoch 255/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2695 - binary_accuracy: 0.8816\n",
      "Epoch 255: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2693 - binary_accuracy: 0.8815 - val_loss: 0.2654 - val_binary_accuracy: 0.8867\n",
      "Epoch 256/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2653 - binary_accuracy: 0.8852\n",
      "Epoch 256: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2653 - binary_accuracy: 0.8852 - val_loss: 0.2767 - val_binary_accuracy: 0.8817\n",
      "Epoch 257/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2645 - binary_accuracy: 0.8868\n",
      "Epoch 257: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2645 - binary_accuracy: 0.8868 - val_loss: 0.2813 - val_binary_accuracy: 0.8814\n",
      "Epoch 258/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2639 - binary_accuracy: 0.8834\n",
      "Epoch 258: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2640 - binary_accuracy: 0.8834 - val_loss: 0.2779 - val_binary_accuracy: 0.8842\n",
      "Epoch 259/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2670 - binary_accuracy: 0.8809\n",
      "Epoch 259: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2666 - binary_accuracy: 0.8811 - val_loss: 0.3007 - val_binary_accuracy: 0.8784\n",
      "Epoch 260/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2676 - binary_accuracy: 0.8835\n",
      "Epoch 260: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2680 - binary_accuracy: 0.8835 - val_loss: 0.2807 - val_binary_accuracy: 0.8853\n",
      "Epoch 261/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2668 - binary_accuracy: 0.8820\n",
      "Epoch 261: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2668 - binary_accuracy: 0.8821 - val_loss: 0.3017 - val_binary_accuracy: 0.8809\n",
      "Epoch 262/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2651 - binary_accuracy: 0.8848\n",
      "Epoch 262: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2648 - binary_accuracy: 0.8849 - val_loss: 0.2824 - val_binary_accuracy: 0.8746\n",
      "Epoch 263/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2667 - binary_accuracy: 0.8826\n",
      "Epoch 263: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2669 - binary_accuracy: 0.8826 - val_loss: 0.2800 - val_binary_accuracy: 0.8828\n",
      "Epoch 264/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2620 - binary_accuracy: 0.8855\n",
      "Epoch 264: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2626 - binary_accuracy: 0.8848 - val_loss: 0.2753 - val_binary_accuracy: 0.8803\n",
      "Epoch 265/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2643 - binary_accuracy: 0.8827\n",
      "Epoch 265: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2637 - binary_accuracy: 0.8830 - val_loss: 0.2779 - val_binary_accuracy: 0.8872\n",
      "Epoch 266/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2655 - binary_accuracy: 0.8830\n",
      "Epoch 266: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2653 - binary_accuracy: 0.8834 - val_loss: 0.2803 - val_binary_accuracy: 0.8864\n",
      "Epoch 267/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2622 - binary_accuracy: 0.8849\n",
      "Epoch 267: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2622 - binary_accuracy: 0.8849 - val_loss: 0.2732 - val_binary_accuracy: 0.8839\n",
      "Epoch 268/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2656 - binary_accuracy: 0.8835\n",
      "Epoch 268: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2655 - binary_accuracy: 0.8837 - val_loss: 0.2948 - val_binary_accuracy: 0.8787\n",
      "Epoch 269/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2621 - binary_accuracy: 0.8847\n",
      "Epoch 269: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2626 - binary_accuracy: 0.8846 - val_loss: 0.2711 - val_binary_accuracy: 0.8806\n",
      "Epoch 270/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2637 - binary_accuracy: 0.8854\n",
      "Epoch 270: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2637 - binary_accuracy: 0.8856 - val_loss: 0.2748 - val_binary_accuracy: 0.8784\n",
      "Epoch 271/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2594 - binary_accuracy: 0.8871\n",
      "Epoch 271: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2598 - binary_accuracy: 0.8869 - val_loss: 0.2747 - val_binary_accuracy: 0.8795\n",
      "Epoch 272/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2627 - binary_accuracy: 0.8846\n",
      "Epoch 272: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2633 - binary_accuracy: 0.8843 - val_loss: 0.2771 - val_binary_accuracy: 0.8856\n",
      "Epoch 273/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2613 - binary_accuracy: 0.8851\n",
      "Epoch 273: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2615 - binary_accuracy: 0.8851 - val_loss: 0.2757 - val_binary_accuracy: 0.8770\n",
      "Epoch 274/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2595 - binary_accuracy: 0.8877\n",
      "Epoch 274: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2595 - binary_accuracy: 0.8876 - val_loss: 0.2692 - val_binary_accuracy: 0.8803\n",
      "Epoch 275/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2639 - binary_accuracy: 0.8860\n",
      "Epoch 275: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2639 - binary_accuracy: 0.8860 - val_loss: 0.2752 - val_binary_accuracy: 0.8834\n",
      "Epoch 276/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2631 - binary_accuracy: 0.8840\n",
      "Epoch 276: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2631 - binary_accuracy: 0.8840 - val_loss: 0.2761 - val_binary_accuracy: 0.8870\n",
      "Epoch 277/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2613 - binary_accuracy: 0.8876\n",
      "Epoch 277: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2614 - binary_accuracy: 0.8875 - val_loss: 0.2722 - val_binary_accuracy: 0.8831\n",
      "Epoch 278/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2591 - binary_accuracy: 0.8846\n",
      "Epoch 278: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2599 - binary_accuracy: 0.8844 - val_loss: 0.3107 - val_binary_accuracy: 0.8583\n",
      "Epoch 279/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2607 - binary_accuracy: 0.8849\n",
      "Epoch 279: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2605 - binary_accuracy: 0.8850 - val_loss: 0.2871 - val_binary_accuracy: 0.8828\n",
      "Epoch 280/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2653 - binary_accuracy: 0.8839\n",
      "Epoch 280: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2646 - binary_accuracy: 0.8842 - val_loss: 0.2887 - val_binary_accuracy: 0.8834\n",
      "Epoch 281/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2623 - binary_accuracy: 0.8858\n",
      "Epoch 281: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2619 - binary_accuracy: 0.8859 - val_loss: 0.2800 - val_binary_accuracy: 0.8757\n",
      "Epoch 282/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2609 - binary_accuracy: 0.8868\n",
      "Epoch 282: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2608 - binary_accuracy: 0.8869 - val_loss: 0.2799 - val_binary_accuracy: 0.8834\n",
      "Epoch 283/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2633 - binary_accuracy: 0.8843\n",
      "Epoch 283: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2633 - binary_accuracy: 0.8842 - val_loss: 0.2778 - val_binary_accuracy: 0.8801\n",
      "Epoch 284/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2640 - binary_accuracy: 0.8848\n",
      "Epoch 284: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2642 - binary_accuracy: 0.8846 - val_loss: 0.2918 - val_binary_accuracy: 0.8660\n",
      "Epoch 285/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2622 - binary_accuracy: 0.8848\n",
      "Epoch 285: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2622 - binary_accuracy: 0.8848 - val_loss: 0.2784 - val_binary_accuracy: 0.8792\n",
      "Epoch 286/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2606 - binary_accuracy: 0.8867\n",
      "Epoch 286: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2606 - binary_accuracy: 0.8867 - val_loss: 0.2774 - val_binary_accuracy: 0.8817\n",
      "Epoch 287/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2632 - binary_accuracy: 0.8834\n",
      "Epoch 287: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2632 - binary_accuracy: 0.8832 - val_loss: 0.2794 - val_binary_accuracy: 0.8837\n",
      "Epoch 288/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2634 - binary_accuracy: 0.8849\n",
      "Epoch 288: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2631 - binary_accuracy: 0.8850 - val_loss: 0.2956 - val_binary_accuracy: 0.8682\n",
      "Epoch 289/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2600 - binary_accuracy: 0.8869\n",
      "Epoch 289: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2598 - binary_accuracy: 0.8870 - val_loss: 0.2764 - val_binary_accuracy: 0.8814\n",
      "Epoch 290/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2694 - binary_accuracy: 0.8815\n",
      "Epoch 290: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2694 - binary_accuracy: 0.8815 - val_loss: 0.2686 - val_binary_accuracy: 0.8825\n",
      "Epoch 291/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2571 - binary_accuracy: 0.8880\n",
      "Epoch 291: val_binary_accuracy did not improve from 0.88779\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2577 - binary_accuracy: 0.8879 - val_loss: 0.2784 - val_binary_accuracy: 0.8834\n",
      "Epoch 292/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2600 - binary_accuracy: 0.8861\n",
      "Epoch 292: val_binary_accuracy improved from 0.88779 to 0.88999, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2595 - binary_accuracy: 0.8865 - val_loss: 0.2793 - val_binary_accuracy: 0.8900\n",
      "Epoch 293/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2562 - binary_accuracy: 0.8886\n",
      "Epoch 293: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2566 - binary_accuracy: 0.8886 - val_loss: 0.2778 - val_binary_accuracy: 0.8806\n",
      "Epoch 294/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2613 - binary_accuracy: 0.8837\n",
      "Epoch 294: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2613 - binary_accuracy: 0.8837 - val_loss: 0.2845 - val_binary_accuracy: 0.8820\n",
      "Epoch 295/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2646 - binary_accuracy: 0.8859\n",
      "Epoch 295: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2648 - binary_accuracy: 0.8859 - val_loss: 0.2735 - val_binary_accuracy: 0.8828\n",
      "Epoch 296/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2587 - binary_accuracy: 0.8854\n",
      "Epoch 296: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2587 - binary_accuracy: 0.8854 - val_loss: 0.2763 - val_binary_accuracy: 0.8768\n",
      "Epoch 297/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2614 - binary_accuracy: 0.8849\n",
      "Epoch 297: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2614 - binary_accuracy: 0.8849 - val_loss: 0.2706 - val_binary_accuracy: 0.8878\n",
      "Epoch 298/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2601 - binary_accuracy: 0.8839\n",
      "Epoch 298: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2601 - binary_accuracy: 0.8839 - val_loss: 0.2835 - val_binary_accuracy: 0.8820\n",
      "Epoch 299/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2554 - binary_accuracy: 0.8883\n",
      "Epoch 299: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2558 - binary_accuracy: 0.8880 - val_loss: 0.2799 - val_binary_accuracy: 0.8845\n",
      "Epoch 300/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2607 - binary_accuracy: 0.8863\n",
      "Epoch 300: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2609 - binary_accuracy: 0.8861 - val_loss: 0.2700 - val_binary_accuracy: 0.8823\n",
      "Epoch 301/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2614 - binary_accuracy: 0.8861\n",
      "Epoch 301: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2616 - binary_accuracy: 0.8861 - val_loss: 0.2790 - val_binary_accuracy: 0.8820\n",
      "Epoch 302/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2571 - binary_accuracy: 0.8863\n",
      "Epoch 302: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2571 - binary_accuracy: 0.8863 - val_loss: 0.2769 - val_binary_accuracy: 0.8853\n",
      "Epoch 303/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2614 - binary_accuracy: 0.8861\n",
      "Epoch 303: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2618 - binary_accuracy: 0.8859 - val_loss: 0.2707 - val_binary_accuracy: 0.8792\n",
      "Epoch 304/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2588 - binary_accuracy: 0.8874\n",
      "Epoch 304: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2586 - binary_accuracy: 0.8874 - val_loss: 0.3097 - val_binary_accuracy: 0.8848\n",
      "Epoch 305/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2614 - binary_accuracy: 0.8874\n",
      "Epoch 305: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2618 - binary_accuracy: 0.8871 - val_loss: 0.2881 - val_binary_accuracy: 0.8621\n",
      "Epoch 306/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2606 - binary_accuracy: 0.8862\n",
      "Epoch 306: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2605 - binary_accuracy: 0.8863 - val_loss: 0.2717 - val_binary_accuracy: 0.8828\n",
      "Epoch 307/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2534 - binary_accuracy: 0.8879\n",
      "Epoch 307: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2534 - binary_accuracy: 0.8879 - val_loss: 0.2795 - val_binary_accuracy: 0.8889\n",
      "Epoch 308/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2598 - binary_accuracy: 0.8851\n",
      "Epoch 308: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2595 - binary_accuracy: 0.8853 - val_loss: 0.2818 - val_binary_accuracy: 0.8759\n",
      "Epoch 309/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2561 - binary_accuracy: 0.8878\n",
      "Epoch 309: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2568 - binary_accuracy: 0.8873 - val_loss: 0.2767 - val_binary_accuracy: 0.8823\n",
      "Epoch 310/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2595 - binary_accuracy: 0.8880\n",
      "Epoch 310: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2596 - binary_accuracy: 0.8879 - val_loss: 0.2889 - val_binary_accuracy: 0.8779\n",
      "Epoch 311/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2599 - binary_accuracy: 0.8841\n",
      "Epoch 311: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2603 - binary_accuracy: 0.8838 - val_loss: 0.2818 - val_binary_accuracy: 0.8795\n",
      "Epoch 312/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2552 - binary_accuracy: 0.8872\n",
      "Epoch 312: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 3s 15ms/step - loss: 0.2554 - binary_accuracy: 0.8872 - val_loss: 0.2798 - val_binary_accuracy: 0.8801\n",
      "Epoch 313/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2562 - binary_accuracy: 0.8871\n",
      "Epoch 313: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2563 - binary_accuracy: 0.8869 - val_loss: 0.2699 - val_binary_accuracy: 0.8856\n",
      "Epoch 314/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2585 - binary_accuracy: 0.8853\n",
      "Epoch 314: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2584 - binary_accuracy: 0.8854 - val_loss: 0.2838 - val_binary_accuracy: 0.8839\n",
      "Epoch 315/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2597 - binary_accuracy: 0.8864\n",
      "Epoch 315: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 15ms/step - loss: 0.2597 - binary_accuracy: 0.8864 - val_loss: 0.2726 - val_binary_accuracy: 0.8795\n",
      "Epoch 316/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2592 - binary_accuracy: 0.8861\n",
      "Epoch 316: val_binary_accuracy did not improve from 0.88999\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2594 - binary_accuracy: 0.8859 - val_loss: 0.2679 - val_binary_accuracy: 0.8856\n",
      "Epoch 317/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2595 - binary_accuracy: 0.8867\n",
      "Epoch 317: val_binary_accuracy improved from 0.88999 to 0.89082, saving model to model.hdf5\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2599 - binary_accuracy: 0.8866 - val_loss: 0.2718 - val_binary_accuracy: 0.8908\n",
      "Epoch 318/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2615 - binary_accuracy: 0.8852\n",
      "Epoch 318: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2620 - binary_accuracy: 0.8849 - val_loss: 0.2811 - val_binary_accuracy: 0.8883\n",
      "Epoch 319/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2603 - binary_accuracy: 0.8861\n",
      "Epoch 319: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2603 - binary_accuracy: 0.8861 - val_loss: 0.2779 - val_binary_accuracy: 0.8803\n",
      "Epoch 320/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2533 - binary_accuracy: 0.8903\n",
      "Epoch 320: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2533 - binary_accuracy: 0.8903 - val_loss: 0.2807 - val_binary_accuracy: 0.8828\n",
      "Epoch 321/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2571 - binary_accuracy: 0.8883\n",
      "Epoch 321: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2574 - binary_accuracy: 0.8881 - val_loss: 0.2732 - val_binary_accuracy: 0.8850\n",
      "Epoch 322/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2569 - binary_accuracy: 0.8857\n",
      "Epoch 322: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2569 - binary_accuracy: 0.8857 - val_loss: 0.2827 - val_binary_accuracy: 0.8792\n",
      "Epoch 323/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2621 - binary_accuracy: 0.8868\n",
      "Epoch 323: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2618 - binary_accuracy: 0.8868 - val_loss: 0.2850 - val_binary_accuracy: 0.8817\n",
      "Epoch 324/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2611 - binary_accuracy: 0.8864\n",
      "Epoch 324: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2615 - binary_accuracy: 0.8862 - val_loss: 0.2791 - val_binary_accuracy: 0.8845\n",
      "Epoch 325/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2540 - binary_accuracy: 0.8875\n",
      "Epoch 325: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2538 - binary_accuracy: 0.8876 - val_loss: 0.2723 - val_binary_accuracy: 0.8881\n",
      "Epoch 326/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2572 - binary_accuracy: 0.8905\n",
      "Epoch 326: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2571 - binary_accuracy: 0.8904 - val_loss: 0.3009 - val_binary_accuracy: 0.8688\n",
      "Epoch 327/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2605 - binary_accuracy: 0.8852\n",
      "Epoch 327: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2604 - binary_accuracy: 0.8853 - val_loss: 0.2750 - val_binary_accuracy: 0.8801\n",
      "Epoch 328/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2558 - binary_accuracy: 0.8894\n",
      "Epoch 328: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2560 - binary_accuracy: 0.8893 - val_loss: 0.2771 - val_binary_accuracy: 0.8825\n",
      "Epoch 329/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2554 - binary_accuracy: 0.8881\n",
      "Epoch 329: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2551 - binary_accuracy: 0.8882 - val_loss: 0.2938 - val_binary_accuracy: 0.8812\n",
      "Epoch 330/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2513 - binary_accuracy: 0.8918\n",
      "Epoch 330: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2513 - binary_accuracy: 0.8919 - val_loss: 0.2851 - val_binary_accuracy: 0.8784\n",
      "Epoch 331/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2568 - binary_accuracy: 0.8864\n",
      "Epoch 331: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2570 - binary_accuracy: 0.8862 - val_loss: 0.2821 - val_binary_accuracy: 0.8834\n",
      "Epoch 332/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2566 - binary_accuracy: 0.8874\n",
      "Epoch 332: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2566 - binary_accuracy: 0.8873 - val_loss: 0.2875 - val_binary_accuracy: 0.8795\n",
      "Epoch 333/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2548 - binary_accuracy: 0.8872\n",
      "Epoch 333: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2548 - binary_accuracy: 0.8872 - val_loss: 0.2802 - val_binary_accuracy: 0.8859\n",
      "Epoch 334/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2560 - binary_accuracy: 0.8874\n",
      "Epoch 334: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2561 - binary_accuracy: 0.8874 - val_loss: 0.2769 - val_binary_accuracy: 0.8861\n",
      "Epoch 335/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2584 - binary_accuracy: 0.8874\n",
      "Epoch 335: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2587 - binary_accuracy: 0.8871 - val_loss: 0.2732 - val_binary_accuracy: 0.8853\n",
      "Epoch 336/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2593 - binary_accuracy: 0.8848\n",
      "Epoch 336: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2600 - binary_accuracy: 0.8845 - val_loss: 0.2796 - val_binary_accuracy: 0.8845\n",
      "Epoch 337/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2586 - binary_accuracy: 0.8849\n",
      "Epoch 337: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2586 - binary_accuracy: 0.8848 - val_loss: 0.2739 - val_binary_accuracy: 0.8859\n",
      "Epoch 338/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2554 - binary_accuracy: 0.8869\n",
      "Epoch 338: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2554 - binary_accuracy: 0.8869 - val_loss: 0.2841 - val_binary_accuracy: 0.8817\n",
      "Epoch 339/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2553 - binary_accuracy: 0.8872\n",
      "Epoch 339: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2550 - binary_accuracy: 0.8873 - val_loss: 0.2930 - val_binary_accuracy: 0.8842\n",
      "Epoch 340/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2592 - binary_accuracy: 0.8860\n",
      "Epoch 340: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2595 - binary_accuracy: 0.8860 - val_loss: 0.2837 - val_binary_accuracy: 0.8814\n",
      "Epoch 341/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2560 - binary_accuracy: 0.8864\n",
      "Epoch 341: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2557 - binary_accuracy: 0.8868 - val_loss: 0.2754 - val_binary_accuracy: 0.8812\n",
      "Epoch 342/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2578 - binary_accuracy: 0.8865\n",
      "Epoch 342: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2573 - binary_accuracy: 0.8866 - val_loss: 0.2897 - val_binary_accuracy: 0.8837\n",
      "Epoch 343/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2602 - binary_accuracy: 0.8881\n",
      "Epoch 343: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2598 - binary_accuracy: 0.8882 - val_loss: 0.2837 - val_binary_accuracy: 0.8806\n",
      "Epoch 344/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2569 - binary_accuracy: 0.8876\n",
      "Epoch 344: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2569 - binary_accuracy: 0.8876 - val_loss: 0.2783 - val_binary_accuracy: 0.8842\n",
      "Epoch 345/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2547 - binary_accuracy: 0.8887\n",
      "Epoch 345: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2547 - binary_accuracy: 0.8887 - val_loss: 0.2844 - val_binary_accuracy: 0.8828\n",
      "Epoch 346/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2564 - binary_accuracy: 0.8872\n",
      "Epoch 346: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2564 - binary_accuracy: 0.8872 - val_loss: 0.2836 - val_binary_accuracy: 0.8825\n",
      "Epoch 347/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2555 - binary_accuracy: 0.8865\n",
      "Epoch 347: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2561 - binary_accuracy: 0.8865 - val_loss: 0.2895 - val_binary_accuracy: 0.8677\n",
      "Epoch 348/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2515 - binary_accuracy: 0.8891\n",
      "Epoch 348: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2514 - binary_accuracy: 0.8892 - val_loss: 0.2733 - val_binary_accuracy: 0.8837\n",
      "Epoch 349/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2604 - binary_accuracy: 0.8872\n",
      "Epoch 349: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2602 - binary_accuracy: 0.8874 - val_loss: 0.2814 - val_binary_accuracy: 0.8740\n",
      "Epoch 350/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2566 - binary_accuracy: 0.8882\n",
      "Epoch 350: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2564 - binary_accuracy: 0.8883 - val_loss: 0.2868 - val_binary_accuracy: 0.8801\n",
      "Epoch 351/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2584 - binary_accuracy: 0.8872\n",
      "Epoch 351: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2581 - binary_accuracy: 0.8874 - val_loss: 0.2834 - val_binary_accuracy: 0.8743\n",
      "Epoch 352/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2577 - binary_accuracy: 0.8865\n",
      "Epoch 352: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2576 - binary_accuracy: 0.8866 - val_loss: 0.2768 - val_binary_accuracy: 0.8806\n",
      "Epoch 353/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2561 - binary_accuracy: 0.8890\n",
      "Epoch 353: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2561 - binary_accuracy: 0.8890 - val_loss: 0.2727 - val_binary_accuracy: 0.8878\n",
      "Epoch 354/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2521 - binary_accuracy: 0.8900\n",
      "Epoch 354: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2522 - binary_accuracy: 0.8902 - val_loss: 0.2739 - val_binary_accuracy: 0.8809\n",
      "Epoch 355/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2588 - binary_accuracy: 0.8885\n",
      "Epoch 355: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2588 - binary_accuracy: 0.8885 - val_loss: 0.2753 - val_binary_accuracy: 0.8834\n",
      "Epoch 356/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2627 - binary_accuracy: 0.8851\n",
      "Epoch 356: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2627 - binary_accuracy: 0.8851 - val_loss: 0.2758 - val_binary_accuracy: 0.8872\n",
      "Epoch 357/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2530 - binary_accuracy: 0.8894\n",
      "Epoch 357: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2530 - binary_accuracy: 0.8894 - val_loss: 0.2713 - val_binary_accuracy: 0.8837\n",
      "Epoch 358/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2556 - binary_accuracy: 0.8896\n",
      "Epoch 358: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2560 - binary_accuracy: 0.8895 - val_loss: 0.2737 - val_binary_accuracy: 0.8872\n",
      "Epoch 359/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2559 - binary_accuracy: 0.8866\n",
      "Epoch 359: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2559 - binary_accuracy: 0.8866 - val_loss: 0.2783 - val_binary_accuracy: 0.8820\n",
      "Epoch 360/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2578 - binary_accuracy: 0.8870\n",
      "Epoch 360: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2578 - binary_accuracy: 0.8869 - val_loss: 0.2903 - val_binary_accuracy: 0.8792\n",
      "Epoch 361/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2560 - binary_accuracy: 0.8874\n",
      "Epoch 361: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2560 - binary_accuracy: 0.8874 - val_loss: 0.2771 - val_binary_accuracy: 0.8817\n",
      "Epoch 362/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2543 - binary_accuracy: 0.8891\n",
      "Epoch 362: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2543 - binary_accuracy: 0.8891 - val_loss: 0.2732 - val_binary_accuracy: 0.8842\n",
      "Epoch 363/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2601 - binary_accuracy: 0.8870\n",
      "Epoch 363: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2601 - binary_accuracy: 0.8870 - val_loss: 0.2695 - val_binary_accuracy: 0.8900\n",
      "Epoch 364/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2575 - binary_accuracy: 0.8873\n",
      "Epoch 364: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2577 - binary_accuracy: 0.8872 - val_loss: 0.2747 - val_binary_accuracy: 0.8856\n",
      "Epoch 365/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2601 - binary_accuracy: 0.8876\n",
      "Epoch 365: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2595 - binary_accuracy: 0.8879 - val_loss: 0.2749 - val_binary_accuracy: 0.8845\n",
      "Epoch 366/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2533 - binary_accuracy: 0.8874\n",
      "Epoch 366: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2533 - binary_accuracy: 0.8874 - val_loss: 0.2867 - val_binary_accuracy: 0.8740\n",
      "Epoch 367/1000\n",
      "223/227 [============================>.] - ETA: 0s - loss: 0.2547 - binary_accuracy: 0.8881\n",
      "Epoch 367: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2542 - binary_accuracy: 0.8884 - val_loss: 0.3019 - val_binary_accuracy: 0.8790\n",
      "Epoch 368/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2550 - binary_accuracy: 0.8897\n",
      "Epoch 368: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2552 - binary_accuracy: 0.8895 - val_loss: 0.2923 - val_binary_accuracy: 0.8693\n",
      "Epoch 369/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2555 - binary_accuracy: 0.8879\n",
      "Epoch 369: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2564 - binary_accuracy: 0.8878 - val_loss: 0.2784 - val_binary_accuracy: 0.8825\n",
      "Epoch 370/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2553 - binary_accuracy: 0.8880\n",
      "Epoch 370: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2557 - binary_accuracy: 0.8879 - val_loss: 0.2750 - val_binary_accuracy: 0.8790\n",
      "Epoch 371/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2510 - binary_accuracy: 0.8898\n",
      "Epoch 371: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2514 - binary_accuracy: 0.8897 - val_loss: 0.2706 - val_binary_accuracy: 0.8867\n",
      "Epoch 372/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2551 - binary_accuracy: 0.8894\n",
      "Epoch 372: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2553 - binary_accuracy: 0.8893 - val_loss: 0.2891 - val_binary_accuracy: 0.8859\n",
      "Epoch 373/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2505 - binary_accuracy: 0.8912\n",
      "Epoch 373: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2510 - binary_accuracy: 0.8912 - val_loss: 0.2697 - val_binary_accuracy: 0.8878\n",
      "Epoch 374/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2527 - binary_accuracy: 0.8889\n",
      "Epoch 374: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2529 - binary_accuracy: 0.8889 - val_loss: 0.2818 - val_binary_accuracy: 0.8867\n",
      "Epoch 375/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2522 - binary_accuracy: 0.8894\n",
      "Epoch 375: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2522 - binary_accuracy: 0.8894 - val_loss: 0.2939 - val_binary_accuracy: 0.8859\n",
      "Epoch 376/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2500 - binary_accuracy: 0.8908\n",
      "Epoch 376: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2500 - binary_accuracy: 0.8908 - val_loss: 0.2756 - val_binary_accuracy: 0.8801\n",
      "Epoch 377/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2506 - binary_accuracy: 0.8891\n",
      "Epoch 377: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2506 - binary_accuracy: 0.8891 - val_loss: 0.2769 - val_binary_accuracy: 0.8842\n",
      "Epoch 378/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2494 - binary_accuracy: 0.8914\n",
      "Epoch 378: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2494 - binary_accuracy: 0.8914 - val_loss: 0.2886 - val_binary_accuracy: 0.8842\n",
      "Epoch 379/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2481 - binary_accuracy: 0.8910\n",
      "Epoch 379: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2480 - binary_accuracy: 0.8910 - val_loss: 0.2800 - val_binary_accuracy: 0.8845\n",
      "Epoch 380/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2581 - binary_accuracy: 0.8871\n",
      "Epoch 380: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2580 - binary_accuracy: 0.8871 - val_loss: 0.2947 - val_binary_accuracy: 0.8845\n",
      "Epoch 381/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2525 - binary_accuracy: 0.8871\n",
      "Epoch 381: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2525 - binary_accuracy: 0.8870 - val_loss: 0.2796 - val_binary_accuracy: 0.8867\n",
      "Epoch 382/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2503 - binary_accuracy: 0.8898\n",
      "Epoch 382: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2506 - binary_accuracy: 0.8894 - val_loss: 0.2723 - val_binary_accuracy: 0.8834\n",
      "Epoch 383/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2531 - binary_accuracy: 0.8885\n",
      "Epoch 383: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2526 - binary_accuracy: 0.8886 - val_loss: 0.3151 - val_binary_accuracy: 0.8718\n",
      "Epoch 384/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2563 - binary_accuracy: 0.8886\n",
      "Epoch 384: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2563 - binary_accuracy: 0.8886 - val_loss: 0.2775 - val_binary_accuracy: 0.8779\n",
      "Epoch 385/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2532 - binary_accuracy: 0.8893\n",
      "Epoch 385: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2537 - binary_accuracy: 0.8892 - val_loss: 0.2716 - val_binary_accuracy: 0.8781\n",
      "Epoch 386/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2558 - binary_accuracy: 0.8878\n",
      "Epoch 386: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2556 - binary_accuracy: 0.8878 - val_loss: 0.2732 - val_binary_accuracy: 0.8814\n",
      "Epoch 387/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2528 - binary_accuracy: 0.8862\n",
      "Epoch 387: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2528 - binary_accuracy: 0.8862 - val_loss: 0.2976 - val_binary_accuracy: 0.8814\n",
      "Epoch 388/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2558 - binary_accuracy: 0.8859\n",
      "Epoch 388: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2559 - binary_accuracy: 0.8858 - val_loss: 0.2789 - val_binary_accuracy: 0.8781\n",
      "Epoch 389/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2561 - binary_accuracy: 0.8882\n",
      "Epoch 389: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2563 - binary_accuracy: 0.8880 - val_loss: 0.2700 - val_binary_accuracy: 0.8812\n",
      "Epoch 390/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2519 - binary_accuracy: 0.8905\n",
      "Epoch 390: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2519 - binary_accuracy: 0.8905 - val_loss: 0.2787 - val_binary_accuracy: 0.8825\n",
      "Epoch 391/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2555 - binary_accuracy: 0.8848\n",
      "Epoch 391: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2554 - binary_accuracy: 0.8849 - val_loss: 0.2770 - val_binary_accuracy: 0.8861\n",
      "Epoch 392/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2556 - binary_accuracy: 0.8876\n",
      "Epoch 392: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2553 - binary_accuracy: 0.8880 - val_loss: 0.2805 - val_binary_accuracy: 0.8883\n",
      "Epoch 393/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2550 - binary_accuracy: 0.8863\n",
      "Epoch 393: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2550 - binary_accuracy: 0.8863 - val_loss: 0.2767 - val_binary_accuracy: 0.8773\n",
      "Epoch 394/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2511 - binary_accuracy: 0.8914\n",
      "Epoch 394: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2514 - binary_accuracy: 0.8911 - val_loss: 0.2742 - val_binary_accuracy: 0.8883\n",
      "Epoch 395/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2499 - binary_accuracy: 0.8914\n",
      "Epoch 395: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2501 - binary_accuracy: 0.8913 - val_loss: 0.2762 - val_binary_accuracy: 0.8834\n",
      "Epoch 396/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2465 - binary_accuracy: 0.8895\n",
      "Epoch 396: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2464 - binary_accuracy: 0.8895 - val_loss: 0.2952 - val_binary_accuracy: 0.8809\n",
      "Epoch 397/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2534 - binary_accuracy: 0.8911\n",
      "Epoch 397: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2534 - binary_accuracy: 0.8910 - val_loss: 0.2699 - val_binary_accuracy: 0.8870\n",
      "Epoch 398/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2549 - binary_accuracy: 0.8888\n",
      "Epoch 398: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2545 - binary_accuracy: 0.8890 - val_loss: 0.2907 - val_binary_accuracy: 0.8848\n",
      "Epoch 399/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2521 - binary_accuracy: 0.8910\n",
      "Epoch 399: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2521 - binary_accuracy: 0.8910 - val_loss: 0.2713 - val_binary_accuracy: 0.8881\n",
      "Epoch 400/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2562 - binary_accuracy: 0.8879\n",
      "Epoch 400: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2562 - binary_accuracy: 0.8878 - val_loss: 0.2733 - val_binary_accuracy: 0.8787\n",
      "Epoch 401/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2531 - binary_accuracy: 0.8892\n",
      "Epoch 401: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2531 - binary_accuracy: 0.8892 - val_loss: 0.2820 - val_binary_accuracy: 0.8770\n",
      "Epoch 402/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2563 - binary_accuracy: 0.8894\n",
      "Epoch 402: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2565 - binary_accuracy: 0.8893 - val_loss: 0.2736 - val_binary_accuracy: 0.8806\n",
      "Epoch 403/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2524 - binary_accuracy: 0.8899\n",
      "Epoch 403: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2521 - binary_accuracy: 0.8899 - val_loss: 0.2787 - val_binary_accuracy: 0.8757\n",
      "Epoch 404/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2566 - binary_accuracy: 0.8880\n",
      "Epoch 404: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2564 - binary_accuracy: 0.8880 - val_loss: 0.2694 - val_binary_accuracy: 0.8861\n",
      "Epoch 405/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2531 - binary_accuracy: 0.8883\n",
      "Epoch 405: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2535 - binary_accuracy: 0.8882 - val_loss: 0.2856 - val_binary_accuracy: 0.8701\n",
      "Epoch 406/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2539 - binary_accuracy: 0.8886\n",
      "Epoch 406: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2540 - binary_accuracy: 0.8886 - val_loss: 0.2912 - val_binary_accuracy: 0.8743\n",
      "Epoch 407/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2544 - binary_accuracy: 0.8882\n",
      "Epoch 407: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2544 - binary_accuracy: 0.8883 - val_loss: 0.2778 - val_binary_accuracy: 0.8825\n",
      "Epoch 408/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2529 - binary_accuracy: 0.8888\n",
      "Epoch 408: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2528 - binary_accuracy: 0.8887 - val_loss: 0.2701 - val_binary_accuracy: 0.8867\n",
      "Epoch 409/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2558 - binary_accuracy: 0.8862\n",
      "Epoch 409: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2558 - binary_accuracy: 0.8861 - val_loss: 0.2712 - val_binary_accuracy: 0.8859\n",
      "Epoch 410/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2557 - binary_accuracy: 0.8880\n",
      "Epoch 410: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2550 - binary_accuracy: 0.8884 - val_loss: 0.2759 - val_binary_accuracy: 0.8870\n",
      "Epoch 411/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2514 - binary_accuracy: 0.8885\n",
      "Epoch 411: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2512 - binary_accuracy: 0.8887 - val_loss: 0.2735 - val_binary_accuracy: 0.8878\n",
      "Epoch 412/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2541 - binary_accuracy: 0.8889\n",
      "Epoch 412: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2541 - binary_accuracy: 0.8889 - val_loss: 0.2678 - val_binary_accuracy: 0.8814\n",
      "Epoch 413/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2495 - binary_accuracy: 0.8898\n",
      "Epoch 413: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2494 - binary_accuracy: 0.8898 - val_loss: 0.2787 - val_binary_accuracy: 0.8757\n",
      "Epoch 414/1000\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.2495 - binary_accuracy: 0.8905\n",
      "Epoch 414: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2506 - binary_accuracy: 0.8901 - val_loss: 0.2760 - val_binary_accuracy: 0.8803\n",
      "Epoch 415/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2526 - binary_accuracy: 0.8894\n",
      "Epoch 415: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2528 - binary_accuracy: 0.8894 - val_loss: 0.2755 - val_binary_accuracy: 0.8754\n",
      "Epoch 416/1000\n",
      "227/227 [==============================] - ETA: 0s - loss: 0.2481 - binary_accuracy: 0.8901\n",
      "Epoch 416: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 17ms/step - loss: 0.2481 - binary_accuracy: 0.8901 - val_loss: 0.2698 - val_binary_accuracy: 0.8837\n",
      "Epoch 417/1000\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.2476 - binary_accuracy: 0.8903\n",
      "Epoch 417: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2476 - binary_accuracy: 0.8903 - val_loss: 0.2824 - val_binary_accuracy: 0.8848\n",
      "Epoch 418/1000\n",
      "224/227 [============================>.] - ETA: 0s - loss: 0.2503 - binary_accuracy: 0.8905\n",
      "Epoch 418: val_binary_accuracy did not improve from 0.89082\n",
      "227/227 [==============================] - 4s 16ms/step - loss: 0.2502 - binary_accuracy: 0.8905 - val_loss: 0.2770 - val_binary_accuracy: 0.8875\n",
      "Epoch 418: early stopping\n",
      "1500.5944935999942\n",
      "Best validation model: epoch 317  - val_binary_accuracy 0.8908188343048096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start = time.perf_counter()\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))\n",
    "print (time.perf_counter() - start)\n",
    "best_idx = int(np.argmax(history.history['val_binary_accuracy']))\n",
    "best_value = np.max(history.history['val_binary_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_binary_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=pd.DataFrame(history.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.savefig(f\"./images/loss{n_neurons_per_hlayer}_{seed}_{n_epochs}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the training set: 0.8904742002487183\n",
      "Accuracy for the development test set: 0.8908188343048096\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\")\n",
    "print(\n",
    "    f\"Accuracy for the development test set: {best_value}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = open(\"./history/DeepFeedforward.txt\", \"a\")\n",
    "v.write(f\"Epoque: {n_epochs}\\n\")\n",
    "v.write(f\"Learning Rate: {lr}\\n\")\n",
    "v.write(f\"Batch Size: {batch_size}\\n\")\n",
    "v.write(f\"Dropout: {tasa_dropout}\\n\")\n",
    "v.write(f\"Neurons per layer: {n_neurons_per_hlayer}\\n\")\n",
    "v.write(f\"Activation: elu\\n\")\n",
    "v.write(f\"Optimizer: Adam\\n\")\n",
    "v.write(f\"seed = {seed}\\n\")\n",
    "v.write(\n",
    "    \"model.add(keras.layers.Dense(neurons, kernel_initializer=he_normal, use_bias=False)); model.add(keras.layers.BatchNormalization()); model.add(keras.layers.Activation(elu)); model.add(tf.keras.layers.Dropout(tasa_dropout))\"\n",
    ")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.write(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\\n\")\n",
    "v.write(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\\n\"\n",
    ")\n",
    "v.write(f\"Time: {time.perf_counter() - start}\\n\")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).round()\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
