{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETX = \"./data/prep/HotelReservationsPreparedCleanX.csv\"\n",
    "DATASETY = \"./data/prep/HotelReservationsY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(DATASETX)\n",
    "df_y = pd.read_csv(DATASETY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  \n",
       "0                    -0.6  \n",
       "1                    -1.0  \n",
       "2                    -1.0  \n",
       "3                    -1.0  \n",
       "4                    -1.0  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booking_status\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               0\n",
       "4               1"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  booking_status  \n",
       "0                    -0.6               1  \n",
       "1                    -1.0               1  \n",
       "2                    -1.0               1  \n",
       "3                    -1.0               0  \n",
       "4                    -1.0               1  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_x, df_y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.665914</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.677778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.674944</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.647407</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.268623</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.576667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.503386</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.575444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "16377          -0.5            -1.0             -1.000000          -0.764706   \n",
       "24639           0.0            -1.0             -0.428571          -0.764706   \n",
       "21974           0.0            -1.0             -0.428571          -1.000000   \n",
       "9205            0.0            -0.8             -1.000000          -0.647059   \n",
       "33303           0.0            -1.0             -0.428571          -0.411765   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "16377                0.0                        -1.0                 0.0   \n",
       "24639               -1.0                        -1.0                 0.0   \n",
       "21974               -1.0                         1.0                 1.0   \n",
       "9205                -1.0                        -1.0                 0.0   \n",
       "33303               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "16377  -0.665914       0.454545      0.133333                 -1.0   \n",
       "24639  -0.674944      -0.454545      0.466667                  0.0   \n",
       "21974  -0.268623      -0.090909      0.666667                  0.0   \n",
       "9205   -0.503386      -0.454545      0.733333                  0.0   \n",
       "33303  -0.399549       0.090909      0.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "16377            -1.0                          -1.0   \n",
       "24639            -1.0                          -1.0   \n",
       "21974            -1.0                          -1.0   \n",
       "9205             -1.0                          -1.0   \n",
       "33303            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "16377                                  -1.0           -0.677778   \n",
       "24639                                  -1.0           -0.647407   \n",
       "21974                                  -1.0           -0.576667   \n",
       "9205                                   -1.0           -0.643333   \n",
       "33303                                  -1.0           -0.575444   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "16377                    -1.0               1  \n",
       "24639                    -0.6               1  \n",
       "21974                    -0.6               0  \n",
       "9205                     -0.6               1  \n",
       "33303                    -0.6               1  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df, random_state=seed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, temp_set = train_test_split(df, train_size=TRAIN_SIZE, random_state=seed)\n",
    "\n",
    "validation_set, test_set = train_test_split(temp_set, train_size=VALIDATION_SIZE / (VALIDATION_SIZE + TEST_SIZE), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('booking_status', axis=1)\n",
    "y_train = train_set['booking_status']\n",
    "\n",
    "X_val = validation_set.drop('booking_status', axis=1)\n",
    "y_val = validation_set['booking_status']\n",
    "\n",
    "X_test = test_set.drop('booking_status', axis=1)\n",
    "y_test = test_set['booking_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.462754</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.663333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.602709</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.304259</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.663148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.688488</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.740778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31249</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.128668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.552963</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "25003          -0.5            -1.0             -1.000000          -0.764706   \n",
       "34921          -0.5            -0.6             -0.714286          -0.529412   \n",
       "13102           0.0            -1.0             -1.000000          -0.647059   \n",
       "4487            0.0            -1.0             -1.000000          -0.647059   \n",
       "31249           0.5            -1.0             -1.000000          -0.529412   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "25003               -1.0                        -1.0                 0.0   \n",
       "34921               -1.0                        -1.0                -1.0   \n",
       "13102               -1.0                        -1.0                 1.0   \n",
       "4487                -1.0                        -1.0                 0.0   \n",
       "31249               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "25003  -0.462754      -0.454545     -0.066667                  0.0   \n",
       "34921  -0.602709      -0.272727      0.000000                  0.0   \n",
       "13102  -0.399549      -0.090909     -0.533333                  0.0   \n",
       "4487   -0.688488       0.636364      0.600000                  0.0   \n",
       "31249  -0.128668       1.000000      0.800000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "25003            -1.0                          -1.0   \n",
       "34921            -1.0                          -1.0   \n",
       "13102            -1.0                          -1.0   \n",
       "4487             -1.0                          -1.0   \n",
       "31249            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "25003                                  -1.0           -0.663333   \n",
       "34921                                  -1.0           -0.304259   \n",
       "13102                                  -1.0           -0.663148   \n",
       "4487                                   -1.0           -0.740778   \n",
       "31249                                  -1.0           -0.552963   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "25003                    -1.0               1  \n",
       "34921                    -0.6               0  \n",
       "13102                    -1.0               0  \n",
       "4487                     -1.0               0  \n",
       "31249                    -0.6               0  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23403</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.673148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12427</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.959368</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557562</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.376667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.729120</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.496667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.092551</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.745185</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "23403           0.0            -1.0             -1.000000          -0.882353   \n",
       "12427          -0.5            -1.0             -0.714286          -1.000000   \n",
       "8230            0.5            -1.0             -1.000000          -0.764706   \n",
       "15344           0.0            -1.0             -1.000000          -0.647059   \n",
       "35059           0.0            -1.0             -1.000000          -0.529412   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "23403               -1.0                        -1.0                -1.0   \n",
       "12427                1.0                        -1.0                 0.0   \n",
       "8230                -1.0                        -1.0                 1.0   \n",
       "15344               -1.0                        -1.0                 0.0   \n",
       "35059               -1.0                        -1.0                 0.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "23403  -1.000000      -1.000000     -0.600000                  0.0   \n",
       "12427  -0.959368      -0.636364     -0.600000                  0.0   \n",
       "8230   -0.557562      -0.272727      0.066667                  0.0   \n",
       "15344  -0.729120       0.454545      0.733333                  0.0   \n",
       "35059  -0.092551       0.818182     -1.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "23403            -1.0                          -1.0   \n",
       "12427            -1.0                          -1.0   \n",
       "8230             -1.0                          -1.0   \n",
       "15344            -1.0                          -1.0   \n",
       "35059            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "23403                                  -1.0           -0.673148   \n",
       "12427                                  -1.0           -0.666667   \n",
       "8230                                   -1.0           -0.376667   \n",
       "15344                                  -1.0           -0.496667   \n",
       "35059                                  -1.0           -0.745185   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "23403                    -1.0               1  \n",
       "12427                    -0.6               1  \n",
       "8230                     -1.0               0  \n",
       "15344                    -1.0               0  \n",
       "35059                     0.2               1  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.855530</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33371</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.720090</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.610000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.562077</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.759333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.720090</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.820000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.760722</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "20955           0.0            -1.0             -0.714286          -0.764706   \n",
       "33371           0.0            -1.0             -0.428571          -0.882353   \n",
       "22008           0.0            -1.0             -0.714286          -0.647059   \n",
       "35320           0.0            -1.0             -0.714286          -0.882353   \n",
       "25986           0.0            -1.0             -0.714286          -0.882353   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "20955               -1.0                        -1.0                 0.0   \n",
       "33371               -1.0                        -1.0                 0.0   \n",
       "22008               -1.0                        -1.0                 0.0   \n",
       "35320                1.0                        -1.0                 0.0   \n",
       "25986               -1.0                        -1.0                 0.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "20955  -0.855530       0.636364      0.200000                  0.0   \n",
       "33371  -0.720090       0.090909      0.066667                  0.0   \n",
       "22008  -0.562077      -0.636364      0.533333                  0.0   \n",
       "35320  -0.720090      -0.818182     -0.066667                  0.0   \n",
       "25986  -0.760722       0.636364      0.866667                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "20955            -1.0                          -1.0   \n",
       "33371            -1.0                          -1.0   \n",
       "22008            -1.0                          -1.0   \n",
       "35320            -1.0                          -1.0   \n",
       "25986            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "20955                                  -1.0           -0.643333   \n",
       "33371                                  -1.0           -0.610000   \n",
       "22008                                  -1.0           -0.759333   \n",
       "35320                                  -1.0           -0.820000   \n",
       "25986                                  -1.0           -0.603333   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "20955                    -0.6               1  \n",
       "33371                    -0.6               1  \n",
       "22008                    -0.6               1  \n",
       "35320                    -1.0               1  \n",
       "25986                    -0.6               0  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (29016, 16)\n",
      "Test:  (3627, 16)\n",
      "Validation:  (3627, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: \", X_train.shape)\n",
    "print(\"Test: \", X_test.shape)\n",
    "print(\"Validation: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = X_train.shape[1]\n",
    "OUTPUTS = 1\n",
    "NUM_TRAINING_EXAMPLES = int(round(X_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int (round (X_val.shape[0]/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.01\n",
    "batch_size = 256\n",
    "tasa_dropout = 0.4\n",
    "n_neurons_per_hlayer = [1024, 512, 256, 124, 62, 31, 16, 8, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"DeepFeedforward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepFeedforward\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_103 (Dense)           (None, 1024)              16384     \n",
      "                                                                 \n",
      " batch_normalization_92 (Ba  (None, 1024)              4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_92 (Activation)  (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_92 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 512)               524288    \n",
      "                                                                 \n",
      " batch_normalization_93 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_93 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_93 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 256)               131072    \n",
      "                                                                 \n",
      " batch_normalization_94 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_94 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_94 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 124)               31744     \n",
      "                                                                 \n",
      " batch_normalization_95 (Ba  (None, 124)               496       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_95 (Activation)  (None, 124)               0         \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, 124)               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 62)                7688      \n",
      "                                                                 \n",
      " batch_normalization_96 (Ba  (None, 62)                248       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_96 (Activation)  (None, 62)                0         \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 62)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 31)                1922      \n",
      "                                                                 \n",
      " batch_normalization_97 (Ba  (None, 31)                124       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_97 (Activation)  (None, 31)                0         \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 31)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 16)                496       \n",
      "                                                                 \n",
      " batch_normalization_98 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_98 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 8)                 128       \n",
      "                                                                 \n",
      " batch_normalization_99 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_99 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 4)                 32        \n",
      "                                                                 \n",
      " batch_normalization_100 (B  (None, 4)                 16        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_100 (Activation  (None, 4)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_100 (Dropout)       (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 2)                 8         \n",
      "                                                                 \n",
      " batch_normalization_101 (B  (None, 2)                 8         \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_101 (Activation  (None, 2)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 721921 (2.75 MB)\n",
      "Trainable params: 717843 (2.74 MB)\n",
      "Non-trainable params: 4078 (15.93 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.InputLayer(input_shape=(INPUTS,), batch_size=None))\n",
    "\n",
    "for neurons in n_neurons_per_hlayer:\n",
    "  model.add(keras.layers.Dense(neurons, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "  model.add(keras.layers.BatchNormalization())\n",
    "  model.add(keras.layers.Activation(\"elu\"))\n",
    "  model.add(tf.keras.layers.Dropout(tasa_dropout))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint = ModelCheckpoint('model.hdf5', monitor='val_binary_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_binary_accuracy', factor=0.1, patience=45, min_lr=0.0001, verbose=1)\n",
    "early_stop = EarlyStopping('val_binary_accuracy', patience=101, verbose=1)\n",
    "callbacks = [model_checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=lr)\n",
    "#opt = SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"binary_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/114 [============================>.] - ETA: 0s - loss: 0.5675 - binary_accuracy: 0.7043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.80094, saving model to model.hdf5\n",
      "114/114 [==============================] - 10s 38ms/step - loss: 0.5673 - binary_accuracy: 0.7041 - val_loss: 0.7482 - val_binary_accuracy: 0.8009\n",
      "Epoch 2/1000\n",
      "  1/114 [..............................] - ETA: 3s - loss: 0.5101 - binary_accuracy: 0.7031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - ETA: 0s - loss: 0.5154 - binary_accuracy: 0.7466\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.80094\n",
      "114/114 [==============================] - 5s 41ms/step - loss: 0.5154 - binary_accuracy: 0.7466 - val_loss: 0.4532 - val_binary_accuracy: 0.7830\n",
      "Epoch 3/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4989 - binary_accuracy: 0.7628\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.80094\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.4990 - binary_accuracy: 0.7628 - val_loss: 0.4480 - val_binary_accuracy: 0.7623\n",
      "Epoch 4/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4920 - binary_accuracy: 0.7703\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.80094\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4920 - binary_accuracy: 0.7702 - val_loss: 0.5367 - val_binary_accuracy: 0.7916\n",
      "Epoch 5/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4852 - binary_accuracy: 0.7765\n",
      "Epoch 5: val_binary_accuracy improved from 0.80094 to 0.80618, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4855 - binary_accuracy: 0.7767 - val_loss: 0.4222 - val_binary_accuracy: 0.8062\n",
      "Epoch 6/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4805 - binary_accuracy: 0.7795\n",
      "Epoch 6: val_binary_accuracy improved from 0.80618 to 0.81445, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4801 - binary_accuracy: 0.7798 - val_loss: 0.4292 - val_binary_accuracy: 0.8144\n",
      "Epoch 7/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4802 - binary_accuracy: 0.7801\n",
      "Epoch 7: val_binary_accuracy improved from 0.81445 to 0.81583, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.4801 - binary_accuracy: 0.7802 - val_loss: 0.4139 - val_binary_accuracy: 0.8158\n",
      "Epoch 8/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4781 - binary_accuracy: 0.7795\n",
      "Epoch 8: val_binary_accuracy did not improve from 0.81583\n",
      "114/114 [==============================] - 3s 27ms/step - loss: 0.4775 - binary_accuracy: 0.7800 - val_loss: 0.4196 - val_binary_accuracy: 0.8136\n",
      "Epoch 9/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4717 - binary_accuracy: 0.7875\n",
      "Epoch 9: val_binary_accuracy did not improve from 0.81583\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4715 - binary_accuracy: 0.7875 - val_loss: 0.4857 - val_binary_accuracy: 0.8076\n",
      "Epoch 10/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4691 - binary_accuracy: 0.7833\n",
      "Epoch 10: val_binary_accuracy improved from 0.81583 to 0.82630, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.4692 - binary_accuracy: 0.7830 - val_loss: 0.3971 - val_binary_accuracy: 0.8263\n",
      "Epoch 11/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4707 - binary_accuracy: 0.7867\n",
      "Epoch 11: val_binary_accuracy did not improve from 0.82630\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.4708 - binary_accuracy: 0.7864 - val_loss: 0.3905 - val_binary_accuracy: 0.8167\n",
      "Epoch 12/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4647 - binary_accuracy: 0.7901\n",
      "Epoch 12: val_binary_accuracy did not improve from 0.82630\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4647 - binary_accuracy: 0.7901 - val_loss: 0.4669 - val_binary_accuracy: 0.7698\n",
      "Epoch 13/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4658 - binary_accuracy: 0.7889\n",
      "Epoch 13: val_binary_accuracy did not improve from 0.82630\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4658 - binary_accuracy: 0.7889 - val_loss: 0.3877 - val_binary_accuracy: 0.8246\n",
      "Epoch 14/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4608 - binary_accuracy: 0.7909\n",
      "Epoch 14: val_binary_accuracy did not improve from 0.82630\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4612 - binary_accuracy: 0.7908 - val_loss: 0.3999 - val_binary_accuracy: 0.8153\n",
      "Epoch 15/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4605 - binary_accuracy: 0.7899\n",
      "Epoch 15: val_binary_accuracy did not improve from 0.82630\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4604 - binary_accuracy: 0.7900 - val_loss: 0.4194 - val_binary_accuracy: 0.8213\n",
      "Epoch 16/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4554 - binary_accuracy: 0.7977\n",
      "Epoch 16: val_binary_accuracy improved from 0.82630 to 0.83375, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 0.4554 - binary_accuracy: 0.7977 - val_loss: 0.3800 - val_binary_accuracy: 0.8337\n",
      "Epoch 17/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4588 - binary_accuracy: 0.7938\n",
      "Epoch 17: val_binary_accuracy did not improve from 0.83375\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.4589 - binary_accuracy: 0.7937 - val_loss: 0.3863 - val_binary_accuracy: 0.8310\n",
      "Epoch 18/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4612 - binary_accuracy: 0.7935\n",
      "Epoch 18: val_binary_accuracy improved from 0.83375 to 0.83595, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4605 - binary_accuracy: 0.7935 - val_loss: 0.3707 - val_binary_accuracy: 0.8360\n",
      "Epoch 19/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4564 - binary_accuracy: 0.7962\n",
      "Epoch 19: val_binary_accuracy did not improve from 0.83595\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.4564 - binary_accuracy: 0.7959 - val_loss: 0.3814 - val_binary_accuracy: 0.8263\n",
      "Epoch 20/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4550 - binary_accuracy: 0.7943\n",
      "Epoch 20: val_binary_accuracy did not improve from 0.83595\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4554 - binary_accuracy: 0.7943 - val_loss: 0.3860 - val_binary_accuracy: 0.8194\n",
      "Epoch 21/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4539 - binary_accuracy: 0.7944\n",
      "Epoch 21: val_binary_accuracy did not improve from 0.83595\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4539 - binary_accuracy: 0.7942 - val_loss: 0.3791 - val_binary_accuracy: 0.8340\n",
      "Epoch 22/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4517 - binary_accuracy: 0.7967\n",
      "Epoch 22: val_binary_accuracy improved from 0.83595 to 0.84119, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.4517 - binary_accuracy: 0.7967 - val_loss: 0.3699 - val_binary_accuracy: 0.8412\n",
      "Epoch 23/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4465 - binary_accuracy: 0.8010\n",
      "Epoch 23: val_binary_accuracy did not improve from 0.84119\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4465 - binary_accuracy: 0.8009 - val_loss: 0.4414 - val_binary_accuracy: 0.7993\n",
      "Epoch 24/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4529 - binary_accuracy: 0.7963\n",
      "Epoch 24: val_binary_accuracy did not improve from 0.84119\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.4528 - binary_accuracy: 0.7963 - val_loss: 0.4027 - val_binary_accuracy: 0.8200\n",
      "Epoch 25/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4523 - binary_accuracy: 0.7960\n",
      "Epoch 25: val_binary_accuracy did not improve from 0.84119\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.4523 - binary_accuracy: 0.7961 - val_loss: 0.3629 - val_binary_accuracy: 0.8412\n",
      "Epoch 26/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4472 - binary_accuracy: 0.7987\n",
      "Epoch 26: val_binary_accuracy did not improve from 0.84119\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.4474 - binary_accuracy: 0.7983 - val_loss: 0.3856 - val_binary_accuracy: 0.8285\n",
      "Epoch 27/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4450 - binary_accuracy: 0.7984\n",
      "Epoch 27: val_binary_accuracy did not improve from 0.84119\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.4461 - binary_accuracy: 0.7982 - val_loss: 0.3709 - val_binary_accuracy: 0.8409\n",
      "Epoch 28/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4461 - binary_accuracy: 0.7984\n",
      "Epoch 28: val_binary_accuracy did not improve from 0.84119\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4461 - binary_accuracy: 0.7984 - val_loss: 0.3826 - val_binary_accuracy: 0.8351\n",
      "Epoch 29/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4440 - binary_accuracy: 0.8023\n",
      "Epoch 29: val_binary_accuracy improved from 0.84119 to 0.84836, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.4440 - binary_accuracy: 0.8023 - val_loss: 0.3614 - val_binary_accuracy: 0.8484\n",
      "Epoch 30/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4448 - binary_accuracy: 0.7985\n",
      "Epoch 30: val_binary_accuracy did not improve from 0.84836\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4446 - binary_accuracy: 0.7987 - val_loss: 0.3875 - val_binary_accuracy: 0.8208\n",
      "Epoch 31/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4499 - binary_accuracy: 0.7957\n",
      "Epoch 31: val_binary_accuracy did not improve from 0.84836\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4500 - binary_accuracy: 0.7957 - val_loss: 0.3727 - val_binary_accuracy: 0.8437\n",
      "Epoch 32/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4459 - binary_accuracy: 0.7987\n",
      "Epoch 32: val_binary_accuracy did not improve from 0.84836\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4462 - binary_accuracy: 0.7983 - val_loss: 0.3670 - val_binary_accuracy: 0.8373\n",
      "Epoch 33/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4419 - binary_accuracy: 0.8005\n",
      "Epoch 33: val_binary_accuracy did not improve from 0.84836\n",
      "114/114 [==============================] - 3s 28ms/step - loss: 0.4423 - binary_accuracy: 0.8005 - val_loss: 0.3658 - val_binary_accuracy: 0.8371\n",
      "Epoch 34/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4427 - binary_accuracy: 0.8015\n",
      "Epoch 34: val_binary_accuracy improved from 0.84836 to 0.84891, saving model to model.hdf5\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.4425 - binary_accuracy: 0.8016 - val_loss: 0.3555 - val_binary_accuracy: 0.8489\n",
      "Epoch 35/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4423 - binary_accuracy: 0.7998\n",
      "Epoch 35: val_binary_accuracy did not improve from 0.84891\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.4421 - binary_accuracy: 0.8000 - val_loss: 0.3532 - val_binary_accuracy: 0.8423\n",
      "Epoch 36/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4401 - binary_accuracy: 0.7984\n",
      "Epoch 36: val_binary_accuracy improved from 0.84891 to 0.85112, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 38ms/step - loss: 0.4404 - binary_accuracy: 0.7983 - val_loss: 0.3473 - val_binary_accuracy: 0.8511\n",
      "Epoch 37/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4438 - binary_accuracy: 0.7975\n",
      "Epoch 37: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 0.4438 - binary_accuracy: 0.7975 - val_loss: 0.3576 - val_binary_accuracy: 0.8406\n",
      "Epoch 38/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4388 - binary_accuracy: 0.8028\n",
      "Epoch 38: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4388 - binary_accuracy: 0.8028 - val_loss: 0.3749 - val_binary_accuracy: 0.8299\n",
      "Epoch 39/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4357 - binary_accuracy: 0.8012\n",
      "Epoch 39: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 4s 39ms/step - loss: 0.4356 - binary_accuracy: 0.8013 - val_loss: 0.3580 - val_binary_accuracy: 0.8417\n",
      "Epoch 40/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4420 - binary_accuracy: 0.7985\n",
      "Epoch 40: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 40ms/step - loss: 0.4420 - binary_accuracy: 0.7985 - val_loss: 0.3513 - val_binary_accuracy: 0.8451\n",
      "Epoch 41/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4353 - binary_accuracy: 0.8037\n",
      "Epoch 41: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 40ms/step - loss: 0.4352 - binary_accuracy: 0.8036 - val_loss: 0.3536 - val_binary_accuracy: 0.8467\n",
      "Epoch 42/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4353 - binary_accuracy: 0.8077\n",
      "Epoch 42: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 41ms/step - loss: 0.4353 - binary_accuracy: 0.8077 - val_loss: 0.3516 - val_binary_accuracy: 0.8434\n",
      "Epoch 43/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4330 - binary_accuracy: 0.8036\n",
      "Epoch 43: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 47ms/step - loss: 0.4330 - binary_accuracy: 0.8036 - val_loss: 0.3438 - val_binary_accuracy: 0.8500\n",
      "Epoch 44/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4340 - binary_accuracy: 0.8028\n",
      "Epoch 44: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.4341 - binary_accuracy: 0.8029 - val_loss: 0.3792 - val_binary_accuracy: 0.8211\n",
      "Epoch 45/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4346 - binary_accuracy: 0.8034\n",
      "Epoch 45: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4346 - binary_accuracy: 0.8034 - val_loss: 0.3483 - val_binary_accuracy: 0.8506\n",
      "Epoch 46/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4337 - binary_accuracy: 0.8037\n",
      "Epoch 46: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4337 - binary_accuracy: 0.8037 - val_loss: 0.3411 - val_binary_accuracy: 0.8506\n",
      "Epoch 47/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4318 - binary_accuracy: 0.8042\n",
      "Epoch 47: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 41ms/step - loss: 0.4321 - binary_accuracy: 0.8040 - val_loss: 0.3514 - val_binary_accuracy: 0.8492\n",
      "Epoch 48/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4316 - binary_accuracy: 0.8040\n",
      "Epoch 48: val_binary_accuracy did not improve from 0.85112\n",
      "114/114 [==============================] - 5s 43ms/step - loss: 0.4316 - binary_accuracy: 0.8040 - val_loss: 0.3529 - val_binary_accuracy: 0.8464\n",
      "Epoch 49/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4307 - binary_accuracy: 0.8064\n",
      "Epoch 49: val_binary_accuracy improved from 0.85112 to 0.85167, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 56ms/step - loss: 0.4306 - binary_accuracy: 0.8064 - val_loss: 0.3392 - val_binary_accuracy: 0.8517\n",
      "Epoch 50/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4322 - binary_accuracy: 0.8057\n",
      "Epoch 50: val_binary_accuracy did not improve from 0.85167\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.4322 - binary_accuracy: 0.8058 - val_loss: 0.3458 - val_binary_accuracy: 0.8478\n",
      "Epoch 51/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4285 - binary_accuracy: 0.8086\n",
      "Epoch 51: val_binary_accuracy improved from 0.85167 to 0.85498, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.4287 - binary_accuracy: 0.8085 - val_loss: 0.3336 - val_binary_accuracy: 0.8550\n",
      "Epoch 52/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4303 - binary_accuracy: 0.8061\n",
      "Epoch 52: val_binary_accuracy did not improve from 0.85498\n",
      "114/114 [==============================] - 7s 57ms/step - loss: 0.4303 - binary_accuracy: 0.8061 - val_loss: 0.3604 - val_binary_accuracy: 0.8398\n",
      "Epoch 53/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4283 - binary_accuracy: 0.8075\n",
      "Epoch 53: val_binary_accuracy improved from 0.85498 to 0.85663, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 0.4283 - binary_accuracy: 0.8075 - val_loss: 0.3375 - val_binary_accuracy: 0.8566\n",
      "Epoch 54/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4248 - binary_accuracy: 0.8088\n",
      "Epoch 54: val_binary_accuracy improved from 0.85663 to 0.85746, saving model to model.hdf5\n",
      "114/114 [==============================] - 5s 42ms/step - loss: 0.4248 - binary_accuracy: 0.8088 - val_loss: 0.3349 - val_binary_accuracy: 0.8575\n",
      "Epoch 55/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4275 - binary_accuracy: 0.8063\n",
      "Epoch 55: val_binary_accuracy did not improve from 0.85746\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.4275 - binary_accuracy: 0.8064 - val_loss: 0.3348 - val_binary_accuracy: 0.8544\n",
      "Epoch 56/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4272 - binary_accuracy: 0.8051\n",
      "Epoch 56: val_binary_accuracy did not improve from 0.85746\n",
      "114/114 [==============================] - 5s 48ms/step - loss: 0.4272 - binary_accuracy: 0.8051 - val_loss: 0.3417 - val_binary_accuracy: 0.8533\n",
      "Epoch 57/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4217 - binary_accuracy: 0.8101\n",
      "Epoch 57: val_binary_accuracy improved from 0.85746 to 0.85856, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 0.4218 - binary_accuracy: 0.8100 - val_loss: 0.3306 - val_binary_accuracy: 0.8586\n",
      "Epoch 58/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4269 - binary_accuracy: 0.8055\n",
      "Epoch 58: val_binary_accuracy improved from 0.85856 to 0.86104, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 56ms/step - loss: 0.4266 - binary_accuracy: 0.8057 - val_loss: 0.3343 - val_binary_accuracy: 0.8610\n",
      "Epoch 59/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4226 - binary_accuracy: 0.8075\n",
      "Epoch 59: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 5s 42ms/step - loss: 0.4226 - binary_accuracy: 0.8075 - val_loss: 0.3470 - val_binary_accuracy: 0.8566\n",
      "Epoch 60/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4247 - binary_accuracy: 0.8066\n",
      "Epoch 60: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 5s 42ms/step - loss: 0.4245 - binary_accuracy: 0.8067 - val_loss: 0.3424 - val_binary_accuracy: 0.8475\n",
      "Epoch 61/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4244 - binary_accuracy: 0.8066\n",
      "Epoch 61: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.4244 - binary_accuracy: 0.8066 - val_loss: 0.3309 - val_binary_accuracy: 0.8572\n",
      "Epoch 62/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4207 - binary_accuracy: 0.8070\n",
      "Epoch 62: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 4s 38ms/step - loss: 0.4207 - binary_accuracy: 0.8070 - val_loss: 0.3313 - val_binary_accuracy: 0.8569\n",
      "Epoch 63/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4225 - binary_accuracy: 0.8075\n",
      "Epoch 63: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.4225 - binary_accuracy: 0.8075 - val_loss: 0.3295 - val_binary_accuracy: 0.8588\n",
      "Epoch 64/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4150 - binary_accuracy: 0.8122\n",
      "Epoch 64: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.4150 - binary_accuracy: 0.8122 - val_loss: 0.3421 - val_binary_accuracy: 0.8511\n",
      "Epoch 65/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4208 - binary_accuracy: 0.8088\n",
      "Epoch 65: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 5s 41ms/step - loss: 0.4208 - binary_accuracy: 0.8087 - val_loss: 0.3345 - val_binary_accuracy: 0.8597\n",
      "Epoch 66/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4232 - binary_accuracy: 0.8077\n",
      "Epoch 66: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 5s 43ms/step - loss: 0.4232 - binary_accuracy: 0.8076 - val_loss: 0.3440 - val_binary_accuracy: 0.8525\n",
      "Epoch 67/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4219 - binary_accuracy: 0.8108\n",
      "Epoch 67: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.4221 - binary_accuracy: 0.8107 - val_loss: 0.3323 - val_binary_accuracy: 0.8558\n",
      "Epoch 68/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4152 - binary_accuracy: 0.8111\n",
      "Epoch 68: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4152 - binary_accuracy: 0.8111 - val_loss: 0.3280 - val_binary_accuracy: 0.8577\n",
      "Epoch 69/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4124 - binary_accuracy: 0.8127\n",
      "Epoch 69: val_binary_accuracy did not improve from 0.86104\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.4125 - binary_accuracy: 0.8127 - val_loss: 0.3357 - val_binary_accuracy: 0.8553\n",
      "Epoch 70/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4178 - binary_accuracy: 0.8089\n",
      "Epoch 70: val_binary_accuracy improved from 0.86104 to 0.86159, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 0.4178 - binary_accuracy: 0.8089 - val_loss: 0.3249 - val_binary_accuracy: 0.8616\n",
      "Epoch 71/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4168 - binary_accuracy: 0.8091\n",
      "Epoch 71: val_binary_accuracy did not improve from 0.86159\n",
      "114/114 [==============================] - 7s 58ms/step - loss: 0.4170 - binary_accuracy: 0.8089 - val_loss: 0.3263 - val_binary_accuracy: 0.8594\n",
      "Epoch 72/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4184 - binary_accuracy: 0.8118\n",
      "Epoch 72: val_binary_accuracy did not improve from 0.86159\n",
      "114/114 [==============================] - 6s 57ms/step - loss: 0.4184 - binary_accuracy: 0.8117 - val_loss: 0.3265 - val_binary_accuracy: 0.8599\n",
      "Epoch 73/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4188 - binary_accuracy: 0.8085\n",
      "Epoch 73: val_binary_accuracy did not improve from 0.86159\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.4188 - binary_accuracy: 0.8085 - val_loss: 0.3231 - val_binary_accuracy: 0.8561\n",
      "Epoch 74/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4146 - binary_accuracy: 0.8122\n",
      "Epoch 74: val_binary_accuracy improved from 0.86159 to 0.86270, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 0.4146 - binary_accuracy: 0.8122 - val_loss: 0.3200 - val_binary_accuracy: 0.8627\n",
      "Epoch 75/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4128 - binary_accuracy: 0.8115\n",
      "Epoch 75: val_binary_accuracy did not improve from 0.86270\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 0.4126 - binary_accuracy: 0.8116 - val_loss: 0.3408 - val_binary_accuracy: 0.8426\n",
      "Epoch 76/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4127 - binary_accuracy: 0.8125\n",
      "Epoch 76: val_binary_accuracy did not improve from 0.86270\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.4127 - binary_accuracy: 0.8125 - val_loss: 0.3369 - val_binary_accuracy: 0.8519\n",
      "Epoch 77/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4130 - binary_accuracy: 0.8112\n",
      "Epoch 77: val_binary_accuracy did not improve from 0.86270\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.4131 - binary_accuracy: 0.8112 - val_loss: 0.3407 - val_binary_accuracy: 0.8577\n",
      "Epoch 78/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4088 - binary_accuracy: 0.8140\n",
      "Epoch 78: val_binary_accuracy improved from 0.86270 to 0.86738, saving model to model.hdf5\n",
      "114/114 [==============================] - 7s 58ms/step - loss: 0.4090 - binary_accuracy: 0.8139 - val_loss: 0.3200 - val_binary_accuracy: 0.8674\n",
      "Epoch 79/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4130 - binary_accuracy: 0.8117\n",
      "Epoch 79: val_binary_accuracy did not improve from 0.86738\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.4132 - binary_accuracy: 0.8116 - val_loss: 0.3215 - val_binary_accuracy: 0.8638\n",
      "Epoch 80/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4157 - binary_accuracy: 0.8111\n",
      "Epoch 80: val_binary_accuracy did not improve from 0.86738\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 0.4155 - binary_accuracy: 0.8113 - val_loss: 0.3206 - val_binary_accuracy: 0.8627\n",
      "Epoch 81/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4115 - binary_accuracy: 0.8130\n",
      "Epoch 81: val_binary_accuracy did not improve from 0.86738\n",
      "114/114 [==============================] - 7s 64ms/step - loss: 0.4118 - binary_accuracy: 0.8129 - val_loss: 0.3148 - val_binary_accuracy: 0.8621\n",
      "Epoch 82/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4122 - binary_accuracy: 0.8136\n",
      "Epoch 82: val_binary_accuracy did not improve from 0.86738\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4122 - binary_accuracy: 0.8136 - val_loss: 0.3278 - val_binary_accuracy: 0.8597\n",
      "Epoch 83/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4152 - binary_accuracy: 0.8120\n",
      "Epoch 83: val_binary_accuracy improved from 0.86738 to 0.86793, saving model to model.hdf5\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4152 - binary_accuracy: 0.8120 - val_loss: 0.3190 - val_binary_accuracy: 0.8679\n",
      "Epoch 84/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4074 - binary_accuracy: 0.8142\n",
      "Epoch 84: val_binary_accuracy did not improve from 0.86793\n",
      "114/114 [==============================] - 5s 48ms/step - loss: 0.4073 - binary_accuracy: 0.8143 - val_loss: 0.3136 - val_binary_accuracy: 0.8619\n",
      "Epoch 85/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4078 - binary_accuracy: 0.8158\n",
      "Epoch 85: val_binary_accuracy did not improve from 0.86793\n",
      "114/114 [==============================] - 4s 38ms/step - loss: 0.4083 - binary_accuracy: 0.8156 - val_loss: 0.3180 - val_binary_accuracy: 0.8668\n",
      "Epoch 86/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4122 - binary_accuracy: 0.8111\n",
      "Epoch 86: val_binary_accuracy did not improve from 0.86793\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4123 - binary_accuracy: 0.8110 - val_loss: 0.3183 - val_binary_accuracy: 0.8666\n",
      "Epoch 87/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4117 - binary_accuracy: 0.8126\n",
      "Epoch 87: val_binary_accuracy did not improve from 0.86793\n",
      "114/114 [==============================] - 7s 60ms/step - loss: 0.4117 - binary_accuracy: 0.8126 - val_loss: 0.3303 - val_binary_accuracy: 0.8624\n",
      "Epoch 88/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4110 - binary_accuracy: 0.8114\n",
      "Epoch 88: val_binary_accuracy did not improve from 0.86793\n",
      "114/114 [==============================] - 7s 62ms/step - loss: 0.4108 - binary_accuracy: 0.8115 - val_loss: 0.3192 - val_binary_accuracy: 0.8558\n",
      "Epoch 89/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4093 - binary_accuracy: 0.8127\n",
      "Epoch 89: val_binary_accuracy improved from 0.86793 to 0.87042, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 57ms/step - loss: 0.4093 - binary_accuracy: 0.8127 - val_loss: 0.3075 - val_binary_accuracy: 0.8704\n",
      "Epoch 90/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4076 - binary_accuracy: 0.8135\n",
      "Epoch 90: val_binary_accuracy did not improve from 0.87042\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4076 - binary_accuracy: 0.8135 - val_loss: 0.3261 - val_binary_accuracy: 0.8544\n",
      "Epoch 91/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4076 - binary_accuracy: 0.8142\n",
      "Epoch 91: val_binary_accuracy did not improve from 0.87042\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4076 - binary_accuracy: 0.8142 - val_loss: 0.3124 - val_binary_accuracy: 0.8657\n",
      "Epoch 92/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4103 - binary_accuracy: 0.8149\n",
      "Epoch 92: val_binary_accuracy did not improve from 0.87042\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4105 - binary_accuracy: 0.8150 - val_loss: 0.3187 - val_binary_accuracy: 0.8627\n",
      "Epoch 93/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4085 - binary_accuracy: 0.8145\n",
      "Epoch 93: val_binary_accuracy did not improve from 0.87042\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.4089 - binary_accuracy: 0.8144 - val_loss: 0.3156 - val_binary_accuracy: 0.8649\n",
      "Epoch 94/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4051 - binary_accuracy: 0.8150\n",
      "Epoch 94: val_binary_accuracy did not improve from 0.87042\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.4051 - binary_accuracy: 0.8150 - val_loss: 0.3128 - val_binary_accuracy: 0.8663\n",
      "Epoch 95/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4068 - binary_accuracy: 0.8153\n",
      "Epoch 95: val_binary_accuracy did not improve from 0.87042\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.4068 - binary_accuracy: 0.8153 - val_loss: 0.3074 - val_binary_accuracy: 0.8699\n",
      "Epoch 96/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4073 - binary_accuracy: 0.8149\n",
      "Epoch 96: val_binary_accuracy improved from 0.87042 to 0.87152, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 0.4074 - binary_accuracy: 0.8148 - val_loss: 0.3063 - val_binary_accuracy: 0.8715\n",
      "Epoch 97/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4085 - binary_accuracy: 0.8142\n",
      "Epoch 97: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 43ms/step - loss: 0.4086 - binary_accuracy: 0.8142 - val_loss: 0.3122 - val_binary_accuracy: 0.8660\n",
      "Epoch 98/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4045 - binary_accuracy: 0.8166\n",
      "Epoch 98: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.4042 - binary_accuracy: 0.8168 - val_loss: 0.3123 - val_binary_accuracy: 0.8679\n",
      "Epoch 99/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4052 - binary_accuracy: 0.8147\n",
      "Epoch 99: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4052 - binary_accuracy: 0.8147 - val_loss: 0.3123 - val_binary_accuracy: 0.8627\n",
      "Epoch 100/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4039 - binary_accuracy: 0.8142\n",
      "Epoch 100: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 48ms/step - loss: 0.4039 - binary_accuracy: 0.8142 - val_loss: 0.3057 - val_binary_accuracy: 0.8688\n",
      "Epoch 101/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4059 - binary_accuracy: 0.8141\n",
      "Epoch 101: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 47ms/step - loss: 0.4059 - binary_accuracy: 0.8141 - val_loss: 0.3069 - val_binary_accuracy: 0.8666\n",
      "Epoch 102/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4027 - binary_accuracy: 0.8164\n",
      "Epoch 102: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 42ms/step - loss: 0.4027 - binary_accuracy: 0.8164 - val_loss: 0.3149 - val_binary_accuracy: 0.8671\n",
      "Epoch 103/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4059 - binary_accuracy: 0.8149\n",
      "Epoch 103: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4059 - binary_accuracy: 0.8149 - val_loss: 0.3134 - val_binary_accuracy: 0.8679\n",
      "Epoch 104/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4027 - binary_accuracy: 0.8189\n",
      "Epoch 104: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4027 - binary_accuracy: 0.8189 - val_loss: 0.3121 - val_binary_accuracy: 0.8668\n",
      "Epoch 105/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4071 - binary_accuracy: 0.8160\n",
      "Epoch 105: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4071 - binary_accuracy: 0.8160 - val_loss: 0.3232 - val_binary_accuracy: 0.8544\n",
      "Epoch 106/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4059 - binary_accuracy: 0.8133\n",
      "Epoch 106: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4060 - binary_accuracy: 0.8131 - val_loss: 0.3039 - val_binary_accuracy: 0.8682\n",
      "Epoch 107/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4047 - binary_accuracy: 0.8148\n",
      "Epoch 107: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4047 - binary_accuracy: 0.8149 - val_loss: 0.3087 - val_binary_accuracy: 0.8660\n",
      "Epoch 108/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4062 - binary_accuracy: 0.8147\n",
      "Epoch 108: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4063 - binary_accuracy: 0.8147 - val_loss: 0.3120 - val_binary_accuracy: 0.8655\n",
      "Epoch 109/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3974 - binary_accuracy: 0.8186\n",
      "Epoch 109: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.3977 - binary_accuracy: 0.8183 - val_loss: 0.3316 - val_binary_accuracy: 0.8541\n",
      "Epoch 110/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4033 - binary_accuracy: 0.8147\n",
      "Epoch 110: val_binary_accuracy did not improve from 0.87152\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.4037 - binary_accuracy: 0.8145 - val_loss: 0.3112 - val_binary_accuracy: 0.8635\n",
      "Epoch 111/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4004 - binary_accuracy: 0.8178\n",
      "Epoch 111: val_binary_accuracy improved from 0.87152 to 0.87400, saving model to model.hdf5\n",
      "114/114 [==============================] - 5s 41ms/step - loss: 0.4004 - binary_accuracy: 0.8178 - val_loss: 0.3061 - val_binary_accuracy: 0.8740\n",
      "Epoch 112/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3988 - binary_accuracy: 0.8208\n",
      "Epoch 112: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.3988 - binary_accuracy: 0.8208 - val_loss: 0.3128 - val_binary_accuracy: 0.8690\n",
      "Epoch 113/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3995 - binary_accuracy: 0.8165\n",
      "Epoch 113: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 5s 44ms/step - loss: 0.3994 - binary_accuracy: 0.8165 - val_loss: 0.3001 - val_binary_accuracy: 0.8715\n",
      "Epoch 114/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4022 - binary_accuracy: 0.8166\n",
      "Epoch 114: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4022 - binary_accuracy: 0.8166 - val_loss: 0.3094 - val_binary_accuracy: 0.8671\n",
      "Epoch 115/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3989 - binary_accuracy: 0.8204\n",
      "Epoch 115: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3989 - binary_accuracy: 0.8204 - val_loss: 0.3014 - val_binary_accuracy: 0.8682\n",
      "Epoch 116/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4022 - binary_accuracy: 0.8177\n",
      "Epoch 116: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.4017 - binary_accuracy: 0.8180 - val_loss: 0.3045 - val_binary_accuracy: 0.8688\n",
      "Epoch 117/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3989 - binary_accuracy: 0.8184\n",
      "Epoch 117: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3995 - binary_accuracy: 0.8181 - val_loss: 0.2958 - val_binary_accuracy: 0.8732\n",
      "Epoch 118/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.4008 - binary_accuracy: 0.8158\n",
      "Epoch 118: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.4007 - binary_accuracy: 0.8160 - val_loss: 0.3023 - val_binary_accuracy: 0.8696\n",
      "Epoch 119/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4008 - binary_accuracy: 0.8166\n",
      "Epoch 119: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.4011 - binary_accuracy: 0.8165 - val_loss: 0.2984 - val_binary_accuracy: 0.8668\n",
      "Epoch 120/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4020 - binary_accuracy: 0.8151\n",
      "Epoch 120: val_binary_accuracy did not improve from 0.87400\n",
      "114/114 [==============================] - 5s 41ms/step - loss: 0.4020 - binary_accuracy: 0.8151 - val_loss: 0.2990 - val_binary_accuracy: 0.8679\n",
      "Epoch 121/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3944 - binary_accuracy: 0.8216\n",
      "Epoch 121: val_binary_accuracy improved from 0.87400 to 0.87510, saving model to model.hdf5\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.3944 - binary_accuracy: 0.8216 - val_loss: 0.2988 - val_binary_accuracy: 0.8751\n",
      "Epoch 122/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4020 - binary_accuracy: 0.8143\n",
      "Epoch 122: val_binary_accuracy did not improve from 0.87510\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.4020 - binary_accuracy: 0.8143 - val_loss: 0.2998 - val_binary_accuracy: 0.8688\n",
      "Epoch 123/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4009 - binary_accuracy: 0.8186\n",
      "Epoch 123: val_binary_accuracy improved from 0.87510 to 0.87731, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.4009 - binary_accuracy: 0.8186 - val_loss: 0.3017 - val_binary_accuracy: 0.8773\n",
      "Epoch 124/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3994 - binary_accuracy: 0.8173\n",
      "Epoch 124: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3994 - binary_accuracy: 0.8173 - val_loss: 0.3073 - val_binary_accuracy: 0.8657\n",
      "Epoch 125/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3984 - binary_accuracy: 0.8157\n",
      "Epoch 125: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 48ms/step - loss: 0.3986 - binary_accuracy: 0.8157 - val_loss: 0.3045 - val_binary_accuracy: 0.8677\n",
      "Epoch 126/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3966 - binary_accuracy: 0.8195\n",
      "Epoch 126: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3965 - binary_accuracy: 0.8196 - val_loss: 0.2990 - val_binary_accuracy: 0.8693\n",
      "Epoch 127/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3948 - binary_accuracy: 0.8183\n",
      "Epoch 127: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.3948 - binary_accuracy: 0.8183 - val_loss: 0.3019 - val_binary_accuracy: 0.8715\n",
      "Epoch 128/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3955 - binary_accuracy: 0.8220\n",
      "Epoch 128: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3957 - binary_accuracy: 0.8219 - val_loss: 0.3030 - val_binary_accuracy: 0.8748\n",
      "Epoch 129/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3957 - binary_accuracy: 0.8198\n",
      "Epoch 129: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.3957 - binary_accuracy: 0.8198 - val_loss: 0.3111 - val_binary_accuracy: 0.8693\n",
      "Epoch 130/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3940 - binary_accuracy: 0.8196\n",
      "Epoch 130: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3942 - binary_accuracy: 0.8196 - val_loss: 0.3011 - val_binary_accuracy: 0.8677\n",
      "Epoch 131/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3972 - binary_accuracy: 0.8167\n",
      "Epoch 131: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3972 - binary_accuracy: 0.8167 - val_loss: 0.3017 - val_binary_accuracy: 0.8699\n",
      "Epoch 132/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3972 - binary_accuracy: 0.8203\n",
      "Epoch 132: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.3972 - binary_accuracy: 0.8203 - val_loss: 0.2971 - val_binary_accuracy: 0.8690\n",
      "Epoch 133/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3954 - binary_accuracy: 0.8171\n",
      "Epoch 133: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3954 - binary_accuracy: 0.8171 - val_loss: 0.2913 - val_binary_accuracy: 0.8726\n",
      "Epoch 134/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.4024 - binary_accuracy: 0.8159\n",
      "Epoch 134: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 0.4029 - binary_accuracy: 0.8157 - val_loss: 0.3036 - val_binary_accuracy: 0.8696\n",
      "Epoch 135/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3948 - binary_accuracy: 0.8213\n",
      "Epoch 135: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3948 - binary_accuracy: 0.8212 - val_loss: 0.3002 - val_binary_accuracy: 0.8674\n",
      "Epoch 136/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3921 - binary_accuracy: 0.8197\n",
      "Epoch 136: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.3918 - binary_accuracy: 0.8200 - val_loss: 0.3024 - val_binary_accuracy: 0.8721\n",
      "Epoch 137/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3935 - binary_accuracy: 0.8204\n",
      "Epoch 137: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3938 - binary_accuracy: 0.8202 - val_loss: 0.3095 - val_binary_accuracy: 0.8685\n",
      "Epoch 138/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3929 - binary_accuracy: 0.8215\n",
      "Epoch 138: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 0.3929 - binary_accuracy: 0.8214 - val_loss: 0.3026 - val_binary_accuracy: 0.8685\n",
      "Epoch 139/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3928 - binary_accuracy: 0.8195\n",
      "Epoch 139: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 0.3927 - binary_accuracy: 0.8196 - val_loss: 0.2955 - val_binary_accuracy: 0.8704\n",
      "Epoch 140/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3902 - binary_accuracy: 0.8202\n",
      "Epoch 140: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.3907 - binary_accuracy: 0.8200 - val_loss: 0.2960 - val_binary_accuracy: 0.8729\n",
      "Epoch 141/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3932 - binary_accuracy: 0.8184\n",
      "Epoch 141: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3932 - binary_accuracy: 0.8184 - val_loss: 0.2975 - val_binary_accuracy: 0.8696\n",
      "Epoch 142/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3921 - binary_accuracy: 0.8219\n",
      "Epoch 142: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 5s 40ms/step - loss: 0.3921 - binary_accuracy: 0.8221 - val_loss: 0.2956 - val_binary_accuracy: 0.8743\n",
      "Epoch 143/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3911 - binary_accuracy: 0.8218\n",
      "Epoch 143: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3910 - binary_accuracy: 0.8218 - val_loss: 0.3031 - val_binary_accuracy: 0.8668\n",
      "Epoch 144/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3967 - binary_accuracy: 0.8185\n",
      "Epoch 144: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 5s 48ms/step - loss: 0.3967 - binary_accuracy: 0.8187 - val_loss: 0.3004 - val_binary_accuracy: 0.8732\n",
      "Epoch 145/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3951 - binary_accuracy: 0.8197\n",
      "Epoch 145: val_binary_accuracy did not improve from 0.87731\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 0.3952 - binary_accuracy: 0.8195 - val_loss: 0.2911 - val_binary_accuracy: 0.8759\n",
      "Epoch 146/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3899 - binary_accuracy: 0.8239\n",
      "Epoch 146: val_binary_accuracy improved from 0.87731 to 0.87758, saving model to model.hdf5\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3900 - binary_accuracy: 0.8239 - val_loss: 0.2966 - val_binary_accuracy: 0.8776\n",
      "Epoch 147/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3890 - binary_accuracy: 0.8206\n",
      "Epoch 147: val_binary_accuracy did not improve from 0.87758\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3892 - binary_accuracy: 0.8205 - val_loss: 0.2932 - val_binary_accuracy: 0.8726\n",
      "Epoch 148/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3950 - binary_accuracy: 0.8171\n",
      "Epoch 148: val_binary_accuracy did not improve from 0.87758\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.3947 - binary_accuracy: 0.8173 - val_loss: 0.2967 - val_binary_accuracy: 0.8677\n",
      "Epoch 149/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3890 - binary_accuracy: 0.8206\n",
      "Epoch 149: val_binary_accuracy did not improve from 0.87758\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3890 - binary_accuracy: 0.8206 - val_loss: 0.2992 - val_binary_accuracy: 0.8624\n",
      "Epoch 150/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3906 - binary_accuracy: 0.8218\n",
      "Epoch 150: val_binary_accuracy did not improve from 0.87758\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3906 - binary_accuracy: 0.8219 - val_loss: 0.2921 - val_binary_accuracy: 0.8743\n",
      "Epoch 151/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3951 - binary_accuracy: 0.8202\n",
      "Epoch 151: val_binary_accuracy improved from 0.87758 to 0.87814, saving model to model.hdf5\n",
      "114/114 [==============================] - 5s 47ms/step - loss: 0.3955 - binary_accuracy: 0.8200 - val_loss: 0.2924 - val_binary_accuracy: 0.8781\n",
      "Epoch 152/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3914 - binary_accuracy: 0.8214\n",
      "Epoch 152: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3914 - binary_accuracy: 0.8214 - val_loss: 0.2920 - val_binary_accuracy: 0.8759\n",
      "Epoch 153/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3869 - binary_accuracy: 0.8228\n",
      "Epoch 153: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 48ms/step - loss: 0.3869 - binary_accuracy: 0.8228 - val_loss: 0.2946 - val_binary_accuracy: 0.8737\n",
      "Epoch 154/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3915 - binary_accuracy: 0.8210\n",
      "Epoch 154: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 0.3916 - binary_accuracy: 0.8211 - val_loss: 0.2944 - val_binary_accuracy: 0.8712\n",
      "Epoch 155/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3871 - binary_accuracy: 0.8228\n",
      "Epoch 155: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3871 - binary_accuracy: 0.8228 - val_loss: 0.2925 - val_binary_accuracy: 0.8699\n",
      "Epoch 156/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3851 - binary_accuracy: 0.8259\n",
      "Epoch 156: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3851 - binary_accuracy: 0.8260 - val_loss: 0.2964 - val_binary_accuracy: 0.8696\n",
      "Epoch 157/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3859 - binary_accuracy: 0.8253\n",
      "Epoch 157: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3859 - binary_accuracy: 0.8253 - val_loss: 0.2899 - val_binary_accuracy: 0.8781\n",
      "Epoch 158/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3864 - binary_accuracy: 0.8235\n",
      "Epoch 158: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3864 - binary_accuracy: 0.8235 - val_loss: 0.2948 - val_binary_accuracy: 0.8729\n",
      "Epoch 159/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3846 - binary_accuracy: 0.8239\n",
      "Epoch 159: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.3846 - binary_accuracy: 0.8239 - val_loss: 0.2937 - val_binary_accuracy: 0.8674\n",
      "Epoch 160/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3904 - binary_accuracy: 0.8212\n",
      "Epoch 160: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.3905 - binary_accuracy: 0.8211 - val_loss: 0.2968 - val_binary_accuracy: 0.8715\n",
      "Epoch 161/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3895 - binary_accuracy: 0.8208\n",
      "Epoch 161: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3896 - binary_accuracy: 0.8208 - val_loss: 0.2902 - val_binary_accuracy: 0.8776\n",
      "Epoch 162/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3861 - binary_accuracy: 0.8223\n",
      "Epoch 162: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3861 - binary_accuracy: 0.8223 - val_loss: 0.3008 - val_binary_accuracy: 0.8685\n",
      "Epoch 163/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3905 - binary_accuracy: 0.8225\n",
      "Epoch 163: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3905 - binary_accuracy: 0.8225 - val_loss: 0.2890 - val_binary_accuracy: 0.8773\n",
      "Epoch 164/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3874 - binary_accuracy: 0.8254\n",
      "Epoch 164: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3874 - binary_accuracy: 0.8255 - val_loss: 0.3006 - val_binary_accuracy: 0.8655\n",
      "Epoch 165/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3890 - binary_accuracy: 0.8232\n",
      "Epoch 165: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3889 - binary_accuracy: 0.8232 - val_loss: 0.2899 - val_binary_accuracy: 0.8765\n",
      "Epoch 166/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3867 - binary_accuracy: 0.8250\n",
      "Epoch 166: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3866 - binary_accuracy: 0.8251 - val_loss: 0.2928 - val_binary_accuracy: 0.8759\n",
      "Epoch 167/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3837 - binary_accuracy: 0.8255\n",
      "Epoch 167: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 0.3835 - binary_accuracy: 0.8255 - val_loss: 0.3060 - val_binary_accuracy: 0.8635\n",
      "Epoch 168/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3821 - binary_accuracy: 0.8245\n",
      "Epoch 168: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3822 - binary_accuracy: 0.8245 - val_loss: 0.2907 - val_binary_accuracy: 0.8754\n",
      "Epoch 169/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3805 - binary_accuracy: 0.8284\n",
      "Epoch 169: val_binary_accuracy did not improve from 0.87814\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 0.3805 - binary_accuracy: 0.8284 - val_loss: 0.2919 - val_binary_accuracy: 0.8770\n",
      "Epoch 170/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3837 - binary_accuracy: 0.8245\n",
      "Epoch 170: val_binary_accuracy improved from 0.87814 to 0.87841, saving model to model.hdf5\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.3838 - binary_accuracy: 0.8243 - val_loss: 0.2923 - val_binary_accuracy: 0.8784\n",
      "Epoch 171/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3855 - binary_accuracy: 0.8243\n",
      "Epoch 171: val_binary_accuracy did not improve from 0.87841\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3855 - binary_accuracy: 0.8242 - val_loss: 0.2842 - val_binary_accuracy: 0.8757\n",
      "Epoch 172/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3856 - binary_accuracy: 0.8255\n",
      "Epoch 172: val_binary_accuracy did not improve from 0.87841\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.3856 - binary_accuracy: 0.8255 - val_loss: 0.2879 - val_binary_accuracy: 0.8726\n",
      "Epoch 173/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3858 - binary_accuracy: 0.8242\n",
      "Epoch 173: val_binary_accuracy did not improve from 0.87841\n",
      "114/114 [==============================] - 5s 43ms/step - loss: 0.3857 - binary_accuracy: 0.8243 - val_loss: 0.2989 - val_binary_accuracy: 0.8660\n",
      "Epoch 174/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3848 - binary_accuracy: 0.8264\n",
      "Epoch 174: val_binary_accuracy did not improve from 0.87841\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3854 - binary_accuracy: 0.8264 - val_loss: 0.2892 - val_binary_accuracy: 0.8746\n",
      "Epoch 175/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3833 - binary_accuracy: 0.8245\n",
      "Epoch 175: val_binary_accuracy improved from 0.87841 to 0.87951, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3834 - binary_accuracy: 0.8244 - val_loss: 0.2853 - val_binary_accuracy: 0.8795\n",
      "Epoch 176/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3838 - binary_accuracy: 0.8243\n",
      "Epoch 176: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3842 - binary_accuracy: 0.8242 - val_loss: 0.2852 - val_binary_accuracy: 0.8765\n",
      "Epoch 177/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3814 - binary_accuracy: 0.8289\n",
      "Epoch 177: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3816 - binary_accuracy: 0.8289 - val_loss: 0.2901 - val_binary_accuracy: 0.8690\n",
      "Epoch 178/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3801 - binary_accuracy: 0.8289\n",
      "Epoch 178: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3801 - binary_accuracy: 0.8289 - val_loss: 0.2909 - val_binary_accuracy: 0.8773\n",
      "Epoch 179/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3817 - binary_accuracy: 0.8252\n",
      "Epoch 179: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3818 - binary_accuracy: 0.8251 - val_loss: 0.2870 - val_binary_accuracy: 0.8795\n",
      "Epoch 180/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3862 - binary_accuracy: 0.8234\n",
      "Epoch 180: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3860 - binary_accuracy: 0.8236 - val_loss: 0.2893 - val_binary_accuracy: 0.8787\n",
      "Epoch 181/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3866 - binary_accuracy: 0.8234\n",
      "Epoch 181: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3867 - binary_accuracy: 0.8234 - val_loss: 0.2874 - val_binary_accuracy: 0.8732\n",
      "Epoch 182/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3861 - binary_accuracy: 0.8232\n",
      "Epoch 182: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3861 - binary_accuracy: 0.8232 - val_loss: 0.2960 - val_binary_accuracy: 0.8663\n",
      "Epoch 183/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3831 - binary_accuracy: 0.8254\n",
      "Epoch 183: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3835 - binary_accuracy: 0.8251 - val_loss: 0.2839 - val_binary_accuracy: 0.8768\n",
      "Epoch 184/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3872 - binary_accuracy: 0.8217\n",
      "Epoch 184: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3874 - binary_accuracy: 0.8215 - val_loss: 0.2889 - val_binary_accuracy: 0.8781\n",
      "Epoch 185/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3829 - binary_accuracy: 0.8268\n",
      "Epoch 185: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3828 - binary_accuracy: 0.8270 - val_loss: 0.2933 - val_binary_accuracy: 0.8693\n",
      "Epoch 186/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3811 - binary_accuracy: 0.8269\n",
      "Epoch 186: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3809 - binary_accuracy: 0.8270 - val_loss: 0.2889 - val_binary_accuracy: 0.8732\n",
      "Epoch 187/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3803 - binary_accuracy: 0.8258\n",
      "Epoch 187: val_binary_accuracy did not improve from 0.87951\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3800 - binary_accuracy: 0.8259 - val_loss: 0.2904 - val_binary_accuracy: 0.8751\n",
      "Epoch 188/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3825 - binary_accuracy: 0.8267\n",
      "Epoch 188: val_binary_accuracy improved from 0.87951 to 0.87979, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3830 - binary_accuracy: 0.8264 - val_loss: 0.2857 - val_binary_accuracy: 0.8798\n",
      "Epoch 189/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3814 - binary_accuracy: 0.8285\n",
      "Epoch 189: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3817 - binary_accuracy: 0.8283 - val_loss: 0.2873 - val_binary_accuracy: 0.8751\n",
      "Epoch 190/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3810 - binary_accuracy: 0.8252\n",
      "Epoch 190: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3810 - binary_accuracy: 0.8252 - val_loss: 0.2856 - val_binary_accuracy: 0.8790\n",
      "Epoch 191/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3797 - binary_accuracy: 0.8258\n",
      "Epoch 191: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3798 - binary_accuracy: 0.8258 - val_loss: 0.2862 - val_binary_accuracy: 0.8762\n",
      "Epoch 192/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3809 - binary_accuracy: 0.8285\n",
      "Epoch 192: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3809 - binary_accuracy: 0.8285 - val_loss: 0.2898 - val_binary_accuracy: 0.8710\n",
      "Epoch 193/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3794 - binary_accuracy: 0.8276\n",
      "Epoch 193: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3794 - binary_accuracy: 0.8276 - val_loss: 0.2910 - val_binary_accuracy: 0.8740\n",
      "Epoch 194/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3825 - binary_accuracy: 0.8271\n",
      "Epoch 194: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3824 - binary_accuracy: 0.8272 - val_loss: 0.2964 - val_binary_accuracy: 0.8646\n",
      "Epoch 195/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3867 - binary_accuracy: 0.8255\n",
      "Epoch 195: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3867 - binary_accuracy: 0.8255 - val_loss: 0.3025 - val_binary_accuracy: 0.8682\n",
      "Epoch 196/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3810 - binary_accuracy: 0.8268\n",
      "Epoch 196: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3815 - binary_accuracy: 0.8264 - val_loss: 0.2907 - val_binary_accuracy: 0.8751\n",
      "Epoch 197/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3780 - binary_accuracy: 0.8264\n",
      "Epoch 197: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3780 - binary_accuracy: 0.8264 - val_loss: 0.2878 - val_binary_accuracy: 0.8770\n",
      "Epoch 198/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3819 - binary_accuracy: 0.8272\n",
      "Epoch 198: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3819 - binary_accuracy: 0.8272 - val_loss: 0.2818 - val_binary_accuracy: 0.8781\n",
      "Epoch 199/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3773 - binary_accuracy: 0.8286\n",
      "Epoch 199: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3773 - binary_accuracy: 0.8286 - val_loss: 0.2835 - val_binary_accuracy: 0.8798\n",
      "Epoch 200/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3800 - binary_accuracy: 0.8274\n",
      "Epoch 200: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3800 - binary_accuracy: 0.8273 - val_loss: 0.2879 - val_binary_accuracy: 0.8795\n",
      "Epoch 201/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3791 - binary_accuracy: 0.8259\n",
      "Epoch 201: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3791 - binary_accuracy: 0.8260 - val_loss: 0.2909 - val_binary_accuracy: 0.8773\n",
      "Epoch 202/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3804 - binary_accuracy: 0.8259\n",
      "Epoch 202: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3809 - binary_accuracy: 0.8259 - val_loss: 0.2872 - val_binary_accuracy: 0.8762\n",
      "Epoch 203/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3775 - binary_accuracy: 0.8266\n",
      "Epoch 203: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3777 - binary_accuracy: 0.8265 - val_loss: 0.2884 - val_binary_accuracy: 0.8732\n",
      "Epoch 204/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3760 - binary_accuracy: 0.8279\n",
      "Epoch 204: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3761 - binary_accuracy: 0.8277 - val_loss: 0.2817 - val_binary_accuracy: 0.8768\n",
      "Epoch 205/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3792 - binary_accuracy: 0.8285\n",
      "Epoch 205: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3793 - binary_accuracy: 0.8283 - val_loss: 0.2829 - val_binary_accuracy: 0.8748\n",
      "Epoch 206/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3803 - binary_accuracy: 0.8263\n",
      "Epoch 206: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3809 - binary_accuracy: 0.8262 - val_loss: 0.2819 - val_binary_accuracy: 0.8779\n",
      "Epoch 207/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3807 - binary_accuracy: 0.8238\n",
      "Epoch 207: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3807 - binary_accuracy: 0.8238 - val_loss: 0.2796 - val_binary_accuracy: 0.8792\n",
      "Epoch 208/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3783 - binary_accuracy: 0.8291\n",
      "Epoch 208: val_binary_accuracy did not improve from 0.87979\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3783 - binary_accuracy: 0.8291 - val_loss: 0.2810 - val_binary_accuracy: 0.8773\n",
      "Epoch 209/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3752 - binary_accuracy: 0.8309\n",
      "Epoch 209: val_binary_accuracy improved from 0.87979 to 0.88062, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3750 - binary_accuracy: 0.8311 - val_loss: 0.2767 - val_binary_accuracy: 0.8806\n",
      "Epoch 210/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3801 - binary_accuracy: 0.8274\n",
      "Epoch 210: val_binary_accuracy did not improve from 0.88062\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3806 - binary_accuracy: 0.8271 - val_loss: 0.2828 - val_binary_accuracy: 0.8765\n",
      "Epoch 211/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3788 - binary_accuracy: 0.8277\n",
      "Epoch 211: val_binary_accuracy did not improve from 0.88062\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3789 - binary_accuracy: 0.8278 - val_loss: 0.2835 - val_binary_accuracy: 0.8787\n",
      "Epoch 212/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3812 - binary_accuracy: 0.8274\n",
      "Epoch 212: val_binary_accuracy did not improve from 0.88062\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3809 - binary_accuracy: 0.8279 - val_loss: 0.2858 - val_binary_accuracy: 0.8776\n",
      "Epoch 213/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3780 - binary_accuracy: 0.8267\n",
      "Epoch 213: val_binary_accuracy improved from 0.88062 to 0.88365, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3779 - binary_accuracy: 0.8267 - val_loss: 0.2830 - val_binary_accuracy: 0.8837\n",
      "Epoch 214/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3714 - binary_accuracy: 0.8332\n",
      "Epoch 214: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3714 - binary_accuracy: 0.8332 - val_loss: 0.2869 - val_binary_accuracy: 0.8781\n",
      "Epoch 215/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3748 - binary_accuracy: 0.8290\n",
      "Epoch 215: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3749 - binary_accuracy: 0.8292 - val_loss: 0.2832 - val_binary_accuracy: 0.8773\n",
      "Epoch 216/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3799 - binary_accuracy: 0.8275\n",
      "Epoch 216: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3800 - binary_accuracy: 0.8274 - val_loss: 0.2810 - val_binary_accuracy: 0.8790\n",
      "Epoch 217/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3716 - binary_accuracy: 0.8329\n",
      "Epoch 217: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3718 - binary_accuracy: 0.8329 - val_loss: 0.2788 - val_binary_accuracy: 0.8806\n",
      "Epoch 218/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3752 - binary_accuracy: 0.8284\n",
      "Epoch 218: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3752 - binary_accuracy: 0.8282 - val_loss: 0.2801 - val_binary_accuracy: 0.8828\n",
      "Epoch 219/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3744 - binary_accuracy: 0.8309\n",
      "Epoch 219: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3744 - binary_accuracy: 0.8309 - val_loss: 0.2967 - val_binary_accuracy: 0.8693\n",
      "Epoch 220/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3812 - binary_accuracy: 0.8260\n",
      "Epoch 220: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3811 - binary_accuracy: 0.8260 - val_loss: 0.2884 - val_binary_accuracy: 0.8712\n",
      "Epoch 221/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3807 - binary_accuracy: 0.8261\n",
      "Epoch 221: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3808 - binary_accuracy: 0.8259 - val_loss: 0.2909 - val_binary_accuracy: 0.8751\n",
      "Epoch 222/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3773 - binary_accuracy: 0.8289\n",
      "Epoch 222: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3772 - binary_accuracy: 0.8289 - val_loss: 0.2879 - val_binary_accuracy: 0.8751\n",
      "Epoch 223/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3751 - binary_accuracy: 0.8310\n",
      "Epoch 223: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3756 - binary_accuracy: 0.8303 - val_loss: 0.2819 - val_binary_accuracy: 0.8779\n",
      "Epoch 224/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3722 - binary_accuracy: 0.8293\n",
      "Epoch 224: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3724 - binary_accuracy: 0.8293 - val_loss: 0.2918 - val_binary_accuracy: 0.8762\n",
      "Epoch 225/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3720 - binary_accuracy: 0.8314\n",
      "Epoch 225: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3717 - binary_accuracy: 0.8315 - val_loss: 0.2875 - val_binary_accuracy: 0.8682\n",
      "Epoch 226/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3781 - binary_accuracy: 0.8271\n",
      "Epoch 226: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3783 - binary_accuracy: 0.8271 - val_loss: 0.2824 - val_binary_accuracy: 0.8787\n",
      "Epoch 227/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3804 - binary_accuracy: 0.8251\n",
      "Epoch 227: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3804 - binary_accuracy: 0.8251 - val_loss: 0.2832 - val_binary_accuracy: 0.8770\n",
      "Epoch 228/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3717 - binary_accuracy: 0.8271\n",
      "Epoch 228: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3719 - binary_accuracy: 0.8269 - val_loss: 0.2779 - val_binary_accuracy: 0.8823\n",
      "Epoch 229/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3737 - binary_accuracy: 0.8303\n",
      "Epoch 229: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3739 - binary_accuracy: 0.8301 - val_loss: 0.2788 - val_binary_accuracy: 0.8781\n",
      "Epoch 230/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3732 - binary_accuracy: 0.8301\n",
      "Epoch 230: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3737 - binary_accuracy: 0.8299 - val_loss: 0.2807 - val_binary_accuracy: 0.8759\n",
      "Epoch 231/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3765 - binary_accuracy: 0.8281\n",
      "Epoch 231: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3764 - binary_accuracy: 0.8282 - val_loss: 0.2752 - val_binary_accuracy: 0.8823\n",
      "Epoch 232/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3724 - binary_accuracy: 0.8322\n",
      "Epoch 232: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3725 - binary_accuracy: 0.8320 - val_loss: 0.2803 - val_binary_accuracy: 0.8792\n",
      "Epoch 233/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3727 - binary_accuracy: 0.8314\n",
      "Epoch 233: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3725 - binary_accuracy: 0.8314 - val_loss: 0.2922 - val_binary_accuracy: 0.8746\n",
      "Epoch 234/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3770 - binary_accuracy: 0.8268\n",
      "Epoch 234: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3768 - binary_accuracy: 0.8266 - val_loss: 0.2767 - val_binary_accuracy: 0.8801\n",
      "Epoch 235/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3800 - binary_accuracy: 0.8266\n",
      "Epoch 235: val_binary_accuracy did not improve from 0.88365\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3800 - binary_accuracy: 0.8265 - val_loss: 0.2767 - val_binary_accuracy: 0.8812\n",
      "Epoch 236/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3704 - binary_accuracy: 0.8314\n",
      "Epoch 236: val_binary_accuracy improved from 0.88365 to 0.88530, saving model to model.hdf5\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3708 - binary_accuracy: 0.8311 - val_loss: 0.2787 - val_binary_accuracy: 0.8853\n",
      "Epoch 237/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3739 - binary_accuracy: 0.8269\n",
      "Epoch 237: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3738 - binary_accuracy: 0.8268 - val_loss: 0.2806 - val_binary_accuracy: 0.8814\n",
      "Epoch 238/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3704 - binary_accuracy: 0.8308\n",
      "Epoch 238: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3708 - binary_accuracy: 0.8305 - val_loss: 0.2858 - val_binary_accuracy: 0.8814\n",
      "Epoch 239/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3753 - binary_accuracy: 0.8259\n",
      "Epoch 239: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3753 - binary_accuracy: 0.8259 - val_loss: 0.2786 - val_binary_accuracy: 0.8842\n",
      "Epoch 240/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3715 - binary_accuracy: 0.8321\n",
      "Epoch 240: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3725 - binary_accuracy: 0.8315 - val_loss: 0.2794 - val_binary_accuracy: 0.8814\n",
      "Epoch 241/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3718 - binary_accuracy: 0.8294\n",
      "Epoch 241: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3716 - binary_accuracy: 0.8293 - val_loss: 0.2788 - val_binary_accuracy: 0.8812\n",
      "Epoch 242/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3711 - binary_accuracy: 0.8316\n",
      "Epoch 242: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3708 - binary_accuracy: 0.8318 - val_loss: 0.2780 - val_binary_accuracy: 0.8798\n",
      "Epoch 243/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3710 - binary_accuracy: 0.8292\n",
      "Epoch 243: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3710 - binary_accuracy: 0.8292 - val_loss: 0.2790 - val_binary_accuracy: 0.8795\n",
      "Epoch 244/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3650 - binary_accuracy: 0.8340\n",
      "Epoch 244: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3646 - binary_accuracy: 0.8342 - val_loss: 0.2801 - val_binary_accuracy: 0.8834\n",
      "Epoch 245/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3743 - binary_accuracy: 0.8292\n",
      "Epoch 245: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3743 - binary_accuracy: 0.8292 - val_loss: 0.2842 - val_binary_accuracy: 0.8781\n",
      "Epoch 246/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3680 - binary_accuracy: 0.8337\n",
      "Epoch 246: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3680 - binary_accuracy: 0.8337 - val_loss: 0.2770 - val_binary_accuracy: 0.8812\n",
      "Epoch 247/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3765 - binary_accuracy: 0.8322\n",
      "Epoch 247: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3765 - binary_accuracy: 0.8322 - val_loss: 0.2773 - val_binary_accuracy: 0.8817\n",
      "Epoch 248/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3701 - binary_accuracy: 0.8300\n",
      "Epoch 248: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3703 - binary_accuracy: 0.8300 - val_loss: 0.2755 - val_binary_accuracy: 0.8828\n",
      "Epoch 249/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3675 - binary_accuracy: 0.8348\n",
      "Epoch 249: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3676 - binary_accuracy: 0.8345 - val_loss: 0.2734 - val_binary_accuracy: 0.8842\n",
      "Epoch 250/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3717 - binary_accuracy: 0.8293\n",
      "Epoch 250: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3717 - binary_accuracy: 0.8293 - val_loss: 0.2827 - val_binary_accuracy: 0.8740\n",
      "Epoch 251/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3693 - binary_accuracy: 0.8294\n",
      "Epoch 251: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3691 - binary_accuracy: 0.8296 - val_loss: 0.2789 - val_binary_accuracy: 0.8806\n",
      "Epoch 252/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3693 - binary_accuracy: 0.8319\n",
      "Epoch 252: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3694 - binary_accuracy: 0.8321 - val_loss: 0.2793 - val_binary_accuracy: 0.8817\n",
      "Epoch 253/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3726 - binary_accuracy: 0.8313\n",
      "Epoch 253: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3725 - binary_accuracy: 0.8313 - val_loss: 0.2777 - val_binary_accuracy: 0.8834\n",
      "Epoch 254/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3732 - binary_accuracy: 0.8296\n",
      "Epoch 254: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 40ms/step - loss: 0.3730 - binary_accuracy: 0.8297 - val_loss: 0.2775 - val_binary_accuracy: 0.8806\n",
      "Epoch 255/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3676 - binary_accuracy: 0.8328\n",
      "Epoch 255: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3679 - binary_accuracy: 0.8328 - val_loss: 0.2806 - val_binary_accuracy: 0.8817\n",
      "Epoch 256/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3733 - binary_accuracy: 0.8286\n",
      "Epoch 256: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3733 - binary_accuracy: 0.8286 - val_loss: 0.2753 - val_binary_accuracy: 0.8809\n",
      "Epoch 257/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3680 - binary_accuracy: 0.8314\n",
      "Epoch 257: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3680 - binary_accuracy: 0.8314 - val_loss: 0.2762 - val_binary_accuracy: 0.8781\n",
      "Epoch 258/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3727 - binary_accuracy: 0.8307\n",
      "Epoch 258: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3727 - binary_accuracy: 0.8307 - val_loss: 0.2818 - val_binary_accuracy: 0.8776\n",
      "Epoch 259/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3673 - binary_accuracy: 0.8326\n",
      "Epoch 259: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3673 - binary_accuracy: 0.8326 - val_loss: 0.2794 - val_binary_accuracy: 0.8768\n",
      "Epoch 260/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3730 - binary_accuracy: 0.8322\n",
      "Epoch 260: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3730 - binary_accuracy: 0.8322 - val_loss: 0.2818 - val_binary_accuracy: 0.8748\n",
      "Epoch 261/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3690 - binary_accuracy: 0.8305\n",
      "Epoch 261: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3687 - binary_accuracy: 0.8309 - val_loss: 0.2805 - val_binary_accuracy: 0.8743\n",
      "Epoch 262/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3696 - binary_accuracy: 0.8349\n",
      "Epoch 262: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 44ms/step - loss: 0.3696 - binary_accuracy: 0.8349 - val_loss: 0.2759 - val_binary_accuracy: 0.8837\n",
      "Epoch 263/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3691 - binary_accuracy: 0.8321\n",
      "Epoch 263: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3691 - binary_accuracy: 0.8321 - val_loss: 0.2803 - val_binary_accuracy: 0.8765\n",
      "Epoch 264/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3661 - binary_accuracy: 0.8320\n",
      "Epoch 264: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3661 - binary_accuracy: 0.8321 - val_loss: 0.2852 - val_binary_accuracy: 0.8699\n",
      "Epoch 265/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3677 - binary_accuracy: 0.8311\n",
      "Epoch 265: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3679 - binary_accuracy: 0.8311 - val_loss: 0.2750 - val_binary_accuracy: 0.8834\n",
      "Epoch 266/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3675 - binary_accuracy: 0.8305\n",
      "Epoch 266: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3675 - binary_accuracy: 0.8305 - val_loss: 0.2797 - val_binary_accuracy: 0.8809\n",
      "Epoch 267/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3671 - binary_accuracy: 0.8331\n",
      "Epoch 267: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3671 - binary_accuracy: 0.8331 - val_loss: 0.2745 - val_binary_accuracy: 0.8834\n",
      "Epoch 268/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3736 - binary_accuracy: 0.8309\n",
      "Epoch 268: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3734 - binary_accuracy: 0.8310 - val_loss: 0.2731 - val_binary_accuracy: 0.8776\n",
      "Epoch 269/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3707 - binary_accuracy: 0.8329\n",
      "Epoch 269: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3716 - binary_accuracy: 0.8322 - val_loss: 0.2790 - val_binary_accuracy: 0.8795\n",
      "Epoch 270/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3675 - binary_accuracy: 0.8327\n",
      "Epoch 270: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3674 - binary_accuracy: 0.8329 - val_loss: 0.2750 - val_binary_accuracy: 0.8812\n",
      "Epoch 271/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3662 - binary_accuracy: 0.8343\n",
      "Epoch 271: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3661 - binary_accuracy: 0.8342 - val_loss: 0.2880 - val_binary_accuracy: 0.8748\n",
      "Epoch 272/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3708 - binary_accuracy: 0.8312\n",
      "Epoch 272: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3707 - binary_accuracy: 0.8313 - val_loss: 0.2783 - val_binary_accuracy: 0.8768\n",
      "Epoch 273/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3683 - binary_accuracy: 0.8330\n",
      "Epoch 273: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3685 - binary_accuracy: 0.8331 - val_loss: 0.2740 - val_binary_accuracy: 0.8817\n",
      "Epoch 274/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3678 - binary_accuracy: 0.8344\n",
      "Epoch 274: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3678 - binary_accuracy: 0.8344 - val_loss: 0.2784 - val_binary_accuracy: 0.8820\n",
      "Epoch 275/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3645 - binary_accuracy: 0.8347\n",
      "Epoch 275: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3642 - binary_accuracy: 0.8348 - val_loss: 0.2768 - val_binary_accuracy: 0.8781\n",
      "Epoch 276/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3648 - binary_accuracy: 0.8325\n",
      "Epoch 276: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3648 - binary_accuracy: 0.8325 - val_loss: 0.2758 - val_binary_accuracy: 0.8801\n",
      "Epoch 277/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3692 - binary_accuracy: 0.8303\n",
      "Epoch 277: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3693 - binary_accuracy: 0.8302 - val_loss: 0.2736 - val_binary_accuracy: 0.8820\n",
      "Epoch 278/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3693 - binary_accuracy: 0.8315\n",
      "Epoch 278: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3693 - binary_accuracy: 0.8315 - val_loss: 0.2758 - val_binary_accuracy: 0.8787\n",
      "Epoch 279/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3669 - binary_accuracy: 0.8321\n",
      "Epoch 279: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3668 - binary_accuracy: 0.8322 - val_loss: 0.2747 - val_binary_accuracy: 0.8787\n",
      "Epoch 280/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3650 - binary_accuracy: 0.8346\n",
      "Epoch 280: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 38ms/step - loss: 0.3650 - binary_accuracy: 0.8346 - val_loss: 0.2859 - val_binary_accuracy: 0.8723\n",
      "Epoch 281/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3673 - binary_accuracy: 0.8331\n",
      "Epoch 281: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3670 - binary_accuracy: 0.8331 - val_loss: 0.2835 - val_binary_accuracy: 0.8770\n",
      "Epoch 282/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3663 - binary_accuracy: 0.8325\n",
      "Epoch 282: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3666 - binary_accuracy: 0.8321 - val_loss: 0.2774 - val_binary_accuracy: 0.8820\n",
      "Epoch 283/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3612 - binary_accuracy: 0.8350\n",
      "Epoch 283: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3610 - binary_accuracy: 0.8353 - val_loss: 0.2752 - val_binary_accuracy: 0.8806\n",
      "Epoch 284/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3680 - binary_accuracy: 0.8337\n",
      "Epoch 284: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.3680 - binary_accuracy: 0.8336 - val_loss: 0.2822 - val_binary_accuracy: 0.8787\n",
      "Epoch 285/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3634 - binary_accuracy: 0.8357\n",
      "Epoch 285: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 3s 29ms/step - loss: 0.3642 - binary_accuracy: 0.8354 - val_loss: 0.2789 - val_binary_accuracy: 0.8762\n",
      "Epoch 286/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3661 - binary_accuracy: 0.8324\n",
      "Epoch 286: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.3662 - binary_accuracy: 0.8323 - val_loss: 0.2737 - val_binary_accuracy: 0.8787\n",
      "Epoch 287/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3680 - binary_accuracy: 0.8322\n",
      "Epoch 287: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3680 - binary_accuracy: 0.8324 - val_loss: 0.2764 - val_binary_accuracy: 0.8831\n",
      "Epoch 288/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3728 - binary_accuracy: 0.8285\n",
      "Epoch 288: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 48ms/step - loss: 0.3728 - binary_accuracy: 0.8285 - val_loss: 0.2766 - val_binary_accuracy: 0.8781\n",
      "Epoch 289/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3653 - binary_accuracy: 0.8333\n",
      "Epoch 289: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 42ms/step - loss: 0.3654 - binary_accuracy: 0.8333 - val_loss: 0.2725 - val_binary_accuracy: 0.8801\n",
      "Epoch 290/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3691 - binary_accuracy: 0.8331\n",
      "Epoch 290: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.3691 - binary_accuracy: 0.8330 - val_loss: 0.2761 - val_binary_accuracy: 0.8792\n",
      "Epoch 291/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3690 - binary_accuracy: 0.8323\n",
      "Epoch 291: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3691 - binary_accuracy: 0.8322 - val_loss: 0.2789 - val_binary_accuracy: 0.8765\n",
      "Epoch 292/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3650 - binary_accuracy: 0.8330\n",
      "Epoch 292: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3648 - binary_accuracy: 0.8331 - val_loss: 0.2760 - val_binary_accuracy: 0.8823\n",
      "Epoch 293/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3674 - binary_accuracy: 0.8323\n",
      "Epoch 293: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3675 - binary_accuracy: 0.8322 - val_loss: 0.2838 - val_binary_accuracy: 0.8718\n",
      "Epoch 294/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3646 - binary_accuracy: 0.8360\n",
      "Epoch 294: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3646 - binary_accuracy: 0.8361 - val_loss: 0.2726 - val_binary_accuracy: 0.8823\n",
      "Epoch 295/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3646 - binary_accuracy: 0.8329\n",
      "Epoch 295: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 0.3647 - binary_accuracy: 0.8327 - val_loss: 0.2711 - val_binary_accuracy: 0.8820\n",
      "Epoch 296/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3671 - binary_accuracy: 0.8321\n",
      "Epoch 296: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3671 - binary_accuracy: 0.8321 - val_loss: 0.2899 - val_binary_accuracy: 0.8655\n",
      "Epoch 297/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3607 - binary_accuracy: 0.8340\n",
      "Epoch 297: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 0.3609 - binary_accuracy: 0.8339 - val_loss: 0.2906 - val_binary_accuracy: 0.8690\n",
      "Epoch 298/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3689 - binary_accuracy: 0.8313\n",
      "Epoch 298: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3689 - binary_accuracy: 0.8313 - val_loss: 0.2736 - val_binary_accuracy: 0.8839\n",
      "Epoch 299/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3688 - binary_accuracy: 0.8339\n",
      "Epoch 299: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3689 - binary_accuracy: 0.8339 - val_loss: 0.2782 - val_binary_accuracy: 0.8792\n",
      "Epoch 300/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3653 - binary_accuracy: 0.8322\n",
      "Epoch 300: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3653 - binary_accuracy: 0.8321 - val_loss: 0.2849 - val_binary_accuracy: 0.8759\n",
      "Epoch 301/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3706 - binary_accuracy: 0.8284\n",
      "Epoch 301: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 0.3708 - binary_accuracy: 0.8283 - val_loss: 0.2763 - val_binary_accuracy: 0.8781\n",
      "Epoch 302/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3603 - binary_accuracy: 0.8343\n",
      "Epoch 302: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3604 - binary_accuracy: 0.8344 - val_loss: 0.2746 - val_binary_accuracy: 0.8817\n",
      "Epoch 303/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3641 - binary_accuracy: 0.8334\n",
      "Epoch 303: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3646 - binary_accuracy: 0.8333 - val_loss: 0.2734 - val_binary_accuracy: 0.8779\n",
      "Epoch 304/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3674 - binary_accuracy: 0.8335\n",
      "Epoch 304: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3675 - binary_accuracy: 0.8335 - val_loss: 0.2742 - val_binary_accuracy: 0.8820\n",
      "Epoch 305/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3603 - binary_accuracy: 0.8374\n",
      "Epoch 305: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3603 - binary_accuracy: 0.8374 - val_loss: 0.2725 - val_binary_accuracy: 0.8823\n",
      "Epoch 306/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3634 - binary_accuracy: 0.8357\n",
      "Epoch 306: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 39ms/step - loss: 0.3634 - binary_accuracy: 0.8357 - val_loss: 0.2716 - val_binary_accuracy: 0.8809\n",
      "Epoch 307/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3655 - binary_accuracy: 0.8335\n",
      "Epoch 307: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 39ms/step - loss: 0.3655 - binary_accuracy: 0.8335 - val_loss: 0.2725 - val_binary_accuracy: 0.8801\n",
      "Epoch 308/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3640 - binary_accuracy: 0.8350\n",
      "Epoch 308: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 40ms/step - loss: 0.3640 - binary_accuracy: 0.8350 - val_loss: 0.2707 - val_binary_accuracy: 0.8817\n",
      "Epoch 309/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3622 - binary_accuracy: 0.8360\n",
      "Epoch 309: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.3623 - binary_accuracy: 0.8358 - val_loss: 0.2717 - val_binary_accuracy: 0.8806\n",
      "Epoch 310/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3656 - binary_accuracy: 0.8346\n",
      "Epoch 310: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 0.3659 - binary_accuracy: 0.8344 - val_loss: 0.2715 - val_binary_accuracy: 0.8817\n",
      "Epoch 311/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3594 - binary_accuracy: 0.8381\n",
      "Epoch 311: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 45ms/step - loss: 0.3594 - binary_accuracy: 0.8381 - val_loss: 0.2709 - val_binary_accuracy: 0.8801\n",
      "Epoch 312/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3651 - binary_accuracy: 0.8350\n",
      "Epoch 312: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 43ms/step - loss: 0.3651 - binary_accuracy: 0.8350 - val_loss: 0.2746 - val_binary_accuracy: 0.8820\n",
      "Epoch 313/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3611 - binary_accuracy: 0.8356\n",
      "Epoch 313: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3610 - binary_accuracy: 0.8357 - val_loss: 0.2727 - val_binary_accuracy: 0.8825\n",
      "Epoch 314/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3630 - binary_accuracy: 0.8343\n",
      "Epoch 314: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3630 - binary_accuracy: 0.8343 - val_loss: 0.2744 - val_binary_accuracy: 0.8831\n",
      "Epoch 315/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3580 - binary_accuracy: 0.8366\n",
      "Epoch 315: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3580 - binary_accuracy: 0.8366 - val_loss: 0.2840 - val_binary_accuracy: 0.8734\n",
      "Epoch 316/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3620 - binary_accuracy: 0.8335\n",
      "Epoch 316: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3620 - binary_accuracy: 0.8335 - val_loss: 0.2729 - val_binary_accuracy: 0.8828\n",
      "Epoch 317/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3633 - binary_accuracy: 0.8338\n",
      "Epoch 317: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3636 - binary_accuracy: 0.8340 - val_loss: 0.2710 - val_binary_accuracy: 0.8848\n",
      "Epoch 318/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3609 - binary_accuracy: 0.8338\n",
      "Epoch 318: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 37ms/step - loss: 0.3609 - binary_accuracy: 0.8338 - val_loss: 0.2734 - val_binary_accuracy: 0.8784\n",
      "Epoch 319/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3620 - binary_accuracy: 0.8342\n",
      "Epoch 319: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3621 - binary_accuracy: 0.8342 - val_loss: 0.2759 - val_binary_accuracy: 0.8809\n",
      "Epoch 320/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3578 - binary_accuracy: 0.8348\n",
      "Epoch 320: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3578 - binary_accuracy: 0.8348 - val_loss: 0.2703 - val_binary_accuracy: 0.8803\n",
      "Epoch 321/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3617 - binary_accuracy: 0.8341\n",
      "Epoch 321: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3615 - binary_accuracy: 0.8341 - val_loss: 0.2789 - val_binary_accuracy: 0.8787\n",
      "Epoch 322/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3618 - binary_accuracy: 0.8369\n",
      "Epoch 322: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3619 - binary_accuracy: 0.8367 - val_loss: 0.2726 - val_binary_accuracy: 0.8795\n",
      "Epoch 323/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3590 - binary_accuracy: 0.8350\n",
      "Epoch 323: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3597 - binary_accuracy: 0.8345 - val_loss: 0.2739 - val_binary_accuracy: 0.8820\n",
      "Epoch 324/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3604 - binary_accuracy: 0.8352\n",
      "Epoch 324: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3601 - binary_accuracy: 0.8354 - val_loss: 0.2707 - val_binary_accuracy: 0.8839\n",
      "Epoch 325/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3574 - binary_accuracy: 0.8363\n",
      "Epoch 325: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 0.3574 - binary_accuracy: 0.8363 - val_loss: 0.2768 - val_binary_accuracy: 0.8798\n",
      "Epoch 326/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3614 - binary_accuracy: 0.8333\n",
      "Epoch 326: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3614 - binary_accuracy: 0.8333 - val_loss: 0.2802 - val_binary_accuracy: 0.8820\n",
      "Epoch 327/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3623 - binary_accuracy: 0.8325\n",
      "Epoch 327: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3624 - binary_accuracy: 0.8327 - val_loss: 0.2755 - val_binary_accuracy: 0.8792\n",
      "Epoch 328/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3568 - binary_accuracy: 0.8350\n",
      "Epoch 328: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3565 - binary_accuracy: 0.8352 - val_loss: 0.2720 - val_binary_accuracy: 0.8842\n",
      "Epoch 329/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3644 - binary_accuracy: 0.8340\n",
      "Epoch 329: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3645 - binary_accuracy: 0.8339 - val_loss: 0.2752 - val_binary_accuracy: 0.8828\n",
      "Epoch 330/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3577 - binary_accuracy: 0.8344\n",
      "Epoch 330: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3577 - binary_accuracy: 0.8345 - val_loss: 0.2733 - val_binary_accuracy: 0.8803\n",
      "Epoch 331/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3643 - binary_accuracy: 0.8313\n",
      "Epoch 331: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 0.3640 - binary_accuracy: 0.8313 - val_loss: 0.2775 - val_binary_accuracy: 0.8773\n",
      "Epoch 332/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3620 - binary_accuracy: 0.8352\n",
      "Epoch 332: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3620 - binary_accuracy: 0.8352 - val_loss: 0.2712 - val_binary_accuracy: 0.8820\n",
      "Epoch 333/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3625 - binary_accuracy: 0.8342\n",
      "Epoch 333: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3622 - binary_accuracy: 0.8345 - val_loss: 0.2799 - val_binary_accuracy: 0.8757\n",
      "Epoch 334/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3625 - binary_accuracy: 0.8329\n",
      "Epoch 334: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 0.3625 - binary_accuracy: 0.8329 - val_loss: 0.2759 - val_binary_accuracy: 0.8809\n",
      "Epoch 335/1000\n",
      "112/114 [============================>.] - ETA: 0s - loss: 0.3622 - binary_accuracy: 0.8339\n",
      "Epoch 335: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 0.3621 - binary_accuracy: 0.8340 - val_loss: 0.2782 - val_binary_accuracy: 0.8820\n",
      "Epoch 336/1000\n",
      "113/114 [============================>.] - ETA: 0s - loss: 0.3615 - binary_accuracy: 0.8362\n",
      "Epoch 336: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.3615 - binary_accuracy: 0.8362 - val_loss: 0.2724 - val_binary_accuracy: 0.8823\n",
      "Epoch 337/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.3544 - binary_accuracy: 0.8378\n",
      "Epoch 337: val_binary_accuracy did not improve from 0.88530\n",
      "114/114 [==============================] - 5s 46ms/step - loss: 0.3544 - binary_accuracy: 0.8378 - val_loss: 0.2807 - val_binary_accuracy: 0.8762\n",
      "Epoch 337: early stopping\n",
      "1527.173257999937\n",
      "Best validation model: epoch 236  - val_binary_accuracy 0.8853046298027039\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))\n",
    "print (time.perf_counter() - start)\n",
    "best_idx = int(np.argmax(history.history['val_binary_accuracy']))\n",
    "best_value = np.max(history.history['val_binary_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_binary_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=pd.DataFrame(history.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.savefig(f\"./images/loss{n_neurons_per_hlayer}_{seed}_{n_epochs}_{results.val_binary_accuracy.values[-1:][0]}_{tasa_dropout}_{lr}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_binary_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.354386</td>\n",
       "      <td>0.837814</td>\n",
       "      <td>0.280652</td>\n",
       "      <td>0.876206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  binary_accuracy  val_loss  val_binary_accuracy\n",
       "336  0.354386         0.837814  0.280652             0.876206"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the training set: 0.8378136157989502\n",
      "Accuracy for the development test set: 0.8762062191963196\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 0.3065 - binary_accuracy: 0.8734\n",
      "Accuracy for the test set: 0.8734491467475891\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\")\n",
    "print(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\"\n",
    ")\n",
    "print(f\"Accuracy for the test set: {model.evaluate(X_test, y_test)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 7ms/step - loss: 0.3065 - binary_accuracy: 0.8734\n"
     ]
    }
   ],
   "source": [
    "v = open(\"./history/DeepFeedforward.txt\", \"a\")\n",
    "v.write(f\"Epoque: {n_epochs}\\n\")\n",
    "v.write(f\"Learning Rate: {lr}\\n\")\n",
    "v.write(f\"Batch Size: {batch_size}\\n\")\n",
    "v.write(f\"Dropout: {tasa_dropout}\\n\")\n",
    "v.write(f\"Neurons per layer: {n_neurons_per_hlayer}\\n\")\n",
    "v.write(f\"Activation: elu\\n\")\n",
    "v.write(f\"Optimizer: Adam\\n\")\n",
    "v.write(f\"seed = {seed}\\n\")\n",
    "v.write(\n",
    "    \"model.add(keras.layers.Dense(neurons, kernel_initializer=he_normal, use_bias=False)); model.add(keras.layers.BatchNormalization()); model.add(keras.layers.Activation(elu)); model.add(tf.keras.layers.Dropout(tasa_dropout))\"\n",
    ")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.write(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\\n\")\n",
    "v.write(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\\n\"\n",
    ")\n",
    "v.write(f\"Best validation model: epoch {best_idx+1} - val_binary_accuracy {best_value}\\n\")\n",
    "v.write(f\"Accuracy for the test set: {model.evaluate(X_test, y_test)[1]}\\n\")\n",
    "v.write(f\"Time: {time.perf_counter() - start}\\n\")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 3s 11ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).round()\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
