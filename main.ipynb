{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETX = \"./data/prep/HotelReservationsPreparedCleanX.csv\"\n",
    "DATASETY = \"./data/prep/HotelReservationsY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(DATASETX)\n",
    "df_y = pd.read_csv(DATASETY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  \n",
       "0                    -0.6  \n",
       "1                    -1.0  \n",
       "2                    -1.0  \n",
       "3                    -1.0  \n",
       "4                    -1.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booking_status\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               0\n",
       "4               1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.990971</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.823928</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.624074</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603704</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548533</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.729630</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "0           0.0            -0.6             -1.000000          -0.882353   \n",
       "1           0.0            -1.0             -1.000000          -0.764706   \n",
       "2           0.0            -1.0             -0.428571          -0.764706   \n",
       "3           0.0            -1.0             -1.000000          -0.882353   \n",
       "4          -0.5            -1.0             -1.000000          -0.882353   \n",
       "\n",
       "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "0               -1.0                         1.0                -1.0   \n",
       "1                0.0                        -1.0                 0.0   \n",
       "2               -1.0                        -1.0                 0.0   \n",
       "3                0.0                        -1.0                 0.0   \n",
       "4                0.0                        -1.0                 0.0   \n",
       "\n",
       "   lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "0  -0.990971       0.454545     -0.933333                  0.0   \n",
       "1  -0.823928       0.272727     -0.133333                 -1.0   \n",
       "2  -0.936795       1.000000      0.733333                  0.0   \n",
       "3  -0.548533       0.636364      0.000000                 -1.0   \n",
       "4  -0.002257       0.272727     -0.266667                 -1.0   \n",
       "\n",
       "   repeated_guest  no_of_previous_cancellations  \\\n",
       "0            -1.0                          -1.0   \n",
       "1            -1.0                          -1.0   \n",
       "2            -1.0                          -1.0   \n",
       "3            -1.0                          -1.0   \n",
       "4            -1.0                          -1.0   \n",
       "\n",
       "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "0                                  -1.0           -0.044444   \n",
       "1                                  -1.0           -0.624074   \n",
       "2                                  -1.0           -0.603704   \n",
       "3                                  -1.0           -0.600000   \n",
       "4                                  -1.0           -0.729630   \n",
       "\n",
       "   no_of_special_requests  booking_status  \n",
       "0                    -0.6               1  \n",
       "1                    -1.0               1  \n",
       "2                    -1.0               1  \n",
       "3                    -1.0               0  \n",
       "4                    -1.0               1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_x, df_y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.665914</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.677778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.674944</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.647407</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.268623</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.576667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.503386</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.575444</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "16377          -0.5            -1.0             -1.000000          -0.764706   \n",
       "24639           0.0            -1.0             -0.428571          -0.764706   \n",
       "21974           0.0            -1.0             -0.428571          -1.000000   \n",
       "9205            0.0            -0.8             -1.000000          -0.647059   \n",
       "33303           0.0            -1.0             -0.428571          -0.411765   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "16377                0.0                        -1.0                 0.0   \n",
       "24639               -1.0                        -1.0                 0.0   \n",
       "21974               -1.0                         1.0                 1.0   \n",
       "9205                -1.0                        -1.0                 0.0   \n",
       "33303               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "16377  -0.665914       0.454545      0.133333                 -1.0   \n",
       "24639  -0.674944      -0.454545      0.466667                  0.0   \n",
       "21974  -0.268623      -0.090909      0.666667                  0.0   \n",
       "9205   -0.503386      -0.454545      0.733333                  0.0   \n",
       "33303  -0.399549       0.090909      0.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "16377            -1.0                          -1.0   \n",
       "24639            -1.0                          -1.0   \n",
       "21974            -1.0                          -1.0   \n",
       "9205             -1.0                          -1.0   \n",
       "33303            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "16377                                  -1.0           -0.677778   \n",
       "24639                                  -1.0           -0.647407   \n",
       "21974                                  -1.0           -0.576667   \n",
       "9205                                   -1.0           -0.643333   \n",
       "33303                                  -1.0           -0.575444   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "16377                    -1.0               1  \n",
       "24639                    -0.6               1  \n",
       "21974                    -0.6               0  \n",
       "9205                     -0.6               1  \n",
       "33303                    -0.6               1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df, random_state=seed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, temp_set = train_test_split(df, train_size=TRAIN_SIZE, random_state=seed)\n",
    "\n",
    "validation_set, test_set = train_test_split(temp_set, train_size=VALIDATION_SIZE / (VALIDATION_SIZE + TEST_SIZE), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('booking_status', axis=1)\n",
    "y_train = train_set['booking_status']\n",
    "\n",
    "X_val = validation_set.drop('booking_status', axis=1)\n",
    "y_val = validation_set['booking_status']\n",
    "\n",
    "X_test = test_set.drop('booking_status', axis=1)\n",
    "y_test = test_set['booking_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.462754</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.663333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.602709</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.304259</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399549</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.663148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.688488</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.740778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31249</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.128668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.552963</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "25003          -0.5            -1.0             -1.000000          -0.764706   \n",
       "34921          -0.5            -0.6             -0.714286          -0.529412   \n",
       "13102           0.0            -1.0             -1.000000          -0.647059   \n",
       "4487            0.0            -1.0             -1.000000          -0.647059   \n",
       "31249           0.5            -1.0             -1.000000          -0.529412   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "25003               -1.0                        -1.0                 0.0   \n",
       "34921               -1.0                        -1.0                -1.0   \n",
       "13102               -1.0                        -1.0                 1.0   \n",
       "4487                -1.0                        -1.0                 0.0   \n",
       "31249               -1.0                        -1.0                 1.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "25003  -0.462754      -0.454545     -0.066667                  0.0   \n",
       "34921  -0.602709      -0.272727      0.000000                  0.0   \n",
       "13102  -0.399549      -0.090909     -0.533333                  0.0   \n",
       "4487   -0.688488       0.636364      0.600000                  0.0   \n",
       "31249  -0.128668       1.000000      0.800000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "25003            -1.0                          -1.0   \n",
       "34921            -1.0                          -1.0   \n",
       "13102            -1.0                          -1.0   \n",
       "4487             -1.0                          -1.0   \n",
       "31249            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "25003                                  -1.0           -0.663333   \n",
       "34921                                  -1.0           -0.304259   \n",
       "13102                                  -1.0           -0.663148   \n",
       "4487                                   -1.0           -0.740778   \n",
       "31249                                  -1.0           -0.552963   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "25003                    -1.0               1  \n",
       "34921                    -0.6               0  \n",
       "13102                    -1.0               0  \n",
       "4487                     -1.0               0  \n",
       "31249                    -0.6               0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23403</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.673148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12427</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.959368</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557562</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.376667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.729120</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.496667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.092551</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.745185</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "23403           0.0            -1.0             -1.000000          -0.882353   \n",
       "12427          -0.5            -1.0             -0.714286          -1.000000   \n",
       "8230            0.5            -1.0             -1.000000          -0.764706   \n",
       "15344           0.0            -1.0             -1.000000          -0.647059   \n",
       "35059           0.0            -1.0             -1.000000          -0.529412   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "23403               -1.0                        -1.0                -1.0   \n",
       "12427                1.0                        -1.0                 0.0   \n",
       "8230                -1.0                        -1.0                 1.0   \n",
       "15344               -1.0                        -1.0                 0.0   \n",
       "35059               -1.0                        -1.0                 0.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "23403  -1.000000      -1.000000     -0.600000                  0.0   \n",
       "12427  -0.959368      -0.636364     -0.600000                  0.0   \n",
       "8230   -0.557562      -0.272727      0.066667                  0.0   \n",
       "15344  -0.729120       0.454545      0.733333                  0.0   \n",
       "35059  -0.092551       0.818182     -1.000000                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "23403            -1.0                          -1.0   \n",
       "12427            -1.0                          -1.0   \n",
       "8230             -1.0                          -1.0   \n",
       "15344            -1.0                          -1.0   \n",
       "35059            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "23403                                  -1.0           -0.673148   \n",
       "12427                                  -1.0           -0.666667   \n",
       "8230                                   -1.0           -0.376667   \n",
       "15344                                  -1.0           -0.496667   \n",
       "35059                                  -1.0           -0.745185   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "23403                    -1.0               1  \n",
       "12427                    -0.6               1  \n",
       "8230                     -1.0               0  \n",
       "15344                    -1.0               0  \n",
       "35059                     0.2               1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_children</th>\n",
       "      <th>no_of_weekend_nights</th>\n",
       "      <th>no_of_week_nights</th>\n",
       "      <th>type_of_meal_plan</th>\n",
       "      <th>required_car_parking_space</th>\n",
       "      <th>room_type_reserved</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>market_segment_type</th>\n",
       "      <th>repeated_guest</th>\n",
       "      <th>no_of_previous_cancellations</th>\n",
       "      <th>no_of_previous_bookings_not_canceled</th>\n",
       "      <th>avg_price_per_room</th>\n",
       "      <th>no_of_special_requests</th>\n",
       "      <th>booking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.855530</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.643333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33371</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.720090</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.610000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.562077</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.759333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.720090</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.820000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.760722</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.603333</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
       "20955           0.0            -1.0             -0.714286          -0.764706   \n",
       "33371           0.0            -1.0             -0.428571          -0.882353   \n",
       "22008           0.0            -1.0             -0.714286          -0.647059   \n",
       "35320           0.0            -1.0             -0.714286          -0.882353   \n",
       "25986           0.0            -1.0             -0.714286          -0.882353   \n",
       "\n",
       "       type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
       "20955               -1.0                        -1.0                 0.0   \n",
       "33371               -1.0                        -1.0                 0.0   \n",
       "22008               -1.0                        -1.0                 0.0   \n",
       "35320                1.0                        -1.0                 0.0   \n",
       "25986               -1.0                        -1.0                 0.0   \n",
       "\n",
       "       lead_time  arrival_month  arrival_date  market_segment_type  \\\n",
       "20955  -0.855530       0.636364      0.200000                  0.0   \n",
       "33371  -0.720090       0.090909      0.066667                  0.0   \n",
       "22008  -0.562077      -0.636364      0.533333                  0.0   \n",
       "35320  -0.720090      -0.818182     -0.066667                  0.0   \n",
       "25986  -0.760722       0.636364      0.866667                  0.0   \n",
       "\n",
       "       repeated_guest  no_of_previous_cancellations  \\\n",
       "20955            -1.0                          -1.0   \n",
       "33371            -1.0                          -1.0   \n",
       "22008            -1.0                          -1.0   \n",
       "35320            -1.0                          -1.0   \n",
       "25986            -1.0                          -1.0   \n",
       "\n",
       "       no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
       "20955                                  -1.0           -0.643333   \n",
       "33371                                  -1.0           -0.610000   \n",
       "22008                                  -1.0           -0.759333   \n",
       "35320                                  -1.0           -0.820000   \n",
       "25986                                  -1.0           -0.603333   \n",
       "\n",
       "       no_of_special_requests  booking_status  \n",
       "20955                    -0.6               1  \n",
       "33371                    -0.6               1  \n",
       "22008                    -0.6               1  \n",
       "35320                    -1.0               1  \n",
       "25986                    -0.6               0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (29016, 16)\n",
      "Test:  (3627, 16)\n",
      "Validation:  (3627, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: \", X_train.shape)\n",
    "print(\"Test: \", X_test.shape)\n",
    "print(\"Validation: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = X_train.shape[1]\n",
    "OUTPUTS = 1\n",
    "NUM_TRAINING_EXAMPLES = int(round(X_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int (round (X_val.shape[0]/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.001\n",
    "batch_size = 512\n",
    "tasa_dropout = 0.2\n",
    "n_neurons_per_hlayer = [1024, 512, 256, 124, 62, 31, 16, 8, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"DeepFeedforward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepFeedforward\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 1024)              16384     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               524288    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131072    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 124)               31744     \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 124)               496       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 124)               0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 124)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 62)                7688      \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 62)                248       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 62)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 62)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 31)                1922      \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 31)                124       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 31)                0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 31)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                496       \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 8)                 128       \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 32        \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 4)                 16        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 4)                 0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 8         \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 2)                 8         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 721921 (2.75 MB)\n",
      "Trainable params: 717843 (2.74 MB)\n",
      "Non-trainable params: 4078 (15.93 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.InputLayer(input_shape=(INPUTS,), batch_size=None))\n",
    "\n",
    "for neurons in n_neurons_per_hlayer:\n",
    "  model.add(keras.layers.Dense(neurons, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "  model.add(keras.layers.BatchNormalization())\n",
    "  model.add(keras.layers.Activation(\"elu\"))\n",
    "  model.add(tf.keras.layers.Dropout(tasa_dropout))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint = ModelCheckpoint('model.hdf5', monitor='val_binary_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_binary_accuracy', factor=0.1, patience=45, min_lr=0.0001, verbose=1)\n",
    "early_stop = EarlyStopping('val_binary_accuracy', patience=101, verbose=1)\n",
    "\n",
    "callbacks = [model_checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "#opt = SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"binary_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/57 [============================>.] - ETA: 0s - loss: 0.5925 - binary_accuracy: 0.7077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.76317, saving model to model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\DL\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 16s 89ms/step - loss: 0.5920 - binary_accuracy: 0.7081 - val_loss: 0.5367 - val_binary_accuracy: 0.7632\n",
      "Epoch 2/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.5236 - binary_accuracy: 0.7677\n",
      "Epoch 2: val_binary_accuracy improved from 0.76317 to 0.80948, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.5236 - binary_accuracy: 0.7677 - val_loss: 0.4724 - val_binary_accuracy: 0.8095\n",
      "Epoch 3/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.5001 - binary_accuracy: 0.7814\n",
      "Epoch 3: val_binary_accuracy improved from 0.80948 to 0.81720, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.4999 - binary_accuracy: 0.7817 - val_loss: 0.4220 - val_binary_accuracy: 0.8172\n",
      "Epoch 4/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4833 - binary_accuracy: 0.7885\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.81720\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.4830 - binary_accuracy: 0.7885 - val_loss: 0.4216 - val_binary_accuracy: 0.8150\n",
      "Epoch 5/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4677 - binary_accuracy: 0.7943\n",
      "Epoch 5: val_binary_accuracy improved from 0.81720 to 0.82575, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.4677 - binary_accuracy: 0.7943 - val_loss: 0.4072 - val_binary_accuracy: 0.8258\n",
      "Epoch 6/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4617 - binary_accuracy: 0.7987\n",
      "Epoch 6: val_binary_accuracy improved from 0.82575 to 0.82934, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.4617 - binary_accuracy: 0.7987 - val_loss: 0.3993 - val_binary_accuracy: 0.8293\n",
      "Epoch 7/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4596 - binary_accuracy: 0.7955\n",
      "Epoch 7: val_binary_accuracy did not improve from 0.82934\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.4592 - binary_accuracy: 0.7958 - val_loss: 0.3987 - val_binary_accuracy: 0.8246\n",
      "Epoch 8/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4488 - binary_accuracy: 0.8019\n",
      "Epoch 8: val_binary_accuracy did not improve from 0.82934\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.4491 - binary_accuracy: 0.8017 - val_loss: 0.3888 - val_binary_accuracy: 0.8269\n",
      "Epoch 9/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4467 - binary_accuracy: 0.8003\n",
      "Epoch 9: val_binary_accuracy improved from 0.82934 to 0.83182, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.4464 - binary_accuracy: 0.8007 - val_loss: 0.3864 - val_binary_accuracy: 0.8318\n",
      "Epoch 10/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4422 - binary_accuracy: 0.8037\n",
      "Epoch 10: val_binary_accuracy improved from 0.83182 to 0.83347, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.4423 - binary_accuracy: 0.8039 - val_loss: 0.3890 - val_binary_accuracy: 0.8335\n",
      "Epoch 11/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4399 - binary_accuracy: 0.8055\n",
      "Epoch 11: val_binary_accuracy did not improve from 0.83347\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.4400 - binary_accuracy: 0.8054 - val_loss: 0.3758 - val_binary_accuracy: 0.8335\n",
      "Epoch 12/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4371 - binary_accuracy: 0.8035\n",
      "Epoch 12: val_binary_accuracy did not improve from 0.83347\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.4367 - binary_accuracy: 0.8037 - val_loss: 0.3762 - val_binary_accuracy: 0.8313\n",
      "Epoch 13/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4323 - binary_accuracy: 0.8081\n",
      "Epoch 13: val_binary_accuracy improved from 0.83347 to 0.83871, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.4318 - binary_accuracy: 0.8083 - val_loss: 0.3730 - val_binary_accuracy: 0.8387\n",
      "Epoch 14/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4304 - binary_accuracy: 0.8111\n",
      "Epoch 14: val_binary_accuracy did not improve from 0.83871\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.4305 - binary_accuracy: 0.8106 - val_loss: 0.3754 - val_binary_accuracy: 0.8351\n",
      "Epoch 15/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4282 - binary_accuracy: 0.8111\n",
      "Epoch 15: val_binary_accuracy improved from 0.83871 to 0.83981, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.4276 - binary_accuracy: 0.8111 - val_loss: 0.3723 - val_binary_accuracy: 0.8398\n",
      "Epoch 16/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4272 - binary_accuracy: 0.8107\n",
      "Epoch 16: val_binary_accuracy improved from 0.83981 to 0.84560, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.4268 - binary_accuracy: 0.8109 - val_loss: 0.3670 - val_binary_accuracy: 0.8456\n",
      "Epoch 17/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4268 - binary_accuracy: 0.8100\n",
      "Epoch 17: val_binary_accuracy did not improve from 0.84560\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.4266 - binary_accuracy: 0.8099 - val_loss: 0.3673 - val_binary_accuracy: 0.8348\n",
      "Epoch 18/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4244 - binary_accuracy: 0.8104\n",
      "Epoch 18: val_binary_accuracy did not improve from 0.84560\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.4244 - binary_accuracy: 0.8104 - val_loss: 0.3718 - val_binary_accuracy: 0.8404\n",
      "Epoch 19/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4174 - binary_accuracy: 0.8156\n",
      "Epoch 19: val_binary_accuracy did not improve from 0.84560\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.4178 - binary_accuracy: 0.8155 - val_loss: 0.3652 - val_binary_accuracy: 0.8428\n",
      "Epoch 20/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4187 - binary_accuracy: 0.8141\n",
      "Epoch 20: val_binary_accuracy did not improve from 0.84560\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.4184 - binary_accuracy: 0.8145 - val_loss: 0.3640 - val_binary_accuracy: 0.8426\n",
      "Epoch 21/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4158 - binary_accuracy: 0.8183\n",
      "Epoch 21: val_binary_accuracy did not improve from 0.84560\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.4156 - binary_accuracy: 0.8183 - val_loss: 0.3580 - val_binary_accuracy: 0.8415\n",
      "Epoch 22/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4125 - binary_accuracy: 0.8204\n",
      "Epoch 22: val_binary_accuracy did not improve from 0.84560\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.4124 - binary_accuracy: 0.8207 - val_loss: 0.3573 - val_binary_accuracy: 0.8451\n",
      "Epoch 23/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4117 - binary_accuracy: 0.8197\n",
      "Epoch 23: val_binary_accuracy improved from 0.84560 to 0.84781, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.4121 - binary_accuracy: 0.8197 - val_loss: 0.3565 - val_binary_accuracy: 0.8478\n",
      "Epoch 24/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4104 - binary_accuracy: 0.8205\n",
      "Epoch 24: val_binary_accuracy did not improve from 0.84781\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.4102 - binary_accuracy: 0.8205 - val_loss: 0.3699 - val_binary_accuracy: 0.8434\n",
      "Epoch 25/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4098 - binary_accuracy: 0.8205\n",
      "Epoch 25: val_binary_accuracy did not improve from 0.84781\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.4099 - binary_accuracy: 0.8203 - val_loss: 0.3585 - val_binary_accuracy: 0.8382\n",
      "Epoch 26/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4072 - binary_accuracy: 0.8218\n",
      "Epoch 26: val_binary_accuracy did not improve from 0.84781\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.4069 - binary_accuracy: 0.8219 - val_loss: 0.3642 - val_binary_accuracy: 0.8428\n",
      "Epoch 27/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4051 - binary_accuracy: 0.8245\n",
      "Epoch 27: val_binary_accuracy did not improve from 0.84781\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.4049 - binary_accuracy: 0.8246 - val_loss: 0.3554 - val_binary_accuracy: 0.8434\n",
      "Epoch 28/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4070 - binary_accuracy: 0.8222\n",
      "Epoch 28: val_binary_accuracy did not improve from 0.84781\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.4073 - binary_accuracy: 0.8218 - val_loss: 0.3589 - val_binary_accuracy: 0.8462\n",
      "Epoch 29/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4001 - binary_accuracy: 0.8268\n",
      "Epoch 29: val_binary_accuracy improved from 0.84781 to 0.84946, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.4002 - binary_accuracy: 0.8268 - val_loss: 0.3477 - val_binary_accuracy: 0.8495\n",
      "Epoch 30/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4027 - binary_accuracy: 0.8242\n",
      "Epoch 30: val_binary_accuracy did not improve from 0.84946\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.4032 - binary_accuracy: 0.8240 - val_loss: 0.3462 - val_binary_accuracy: 0.8484\n",
      "Epoch 31/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.4006 - binary_accuracy: 0.8273\n",
      "Epoch 31: val_binary_accuracy did not improve from 0.84946\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.4004 - binary_accuracy: 0.8273 - val_loss: 0.3564 - val_binary_accuracy: 0.8451\n",
      "Epoch 32/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3962 - binary_accuracy: 0.8272\n",
      "Epoch 32: val_binary_accuracy improved from 0.84946 to 0.85250, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3964 - binary_accuracy: 0.8271 - val_loss: 0.3447 - val_binary_accuracy: 0.8525\n",
      "Epoch 33/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.4012 - binary_accuracy: 0.8253\n",
      "Epoch 33: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.4012 - binary_accuracy: 0.8253 - val_loss: 0.3460 - val_binary_accuracy: 0.8517\n",
      "Epoch 34/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3941 - binary_accuracy: 0.8297\n",
      "Epoch 34: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3941 - binary_accuracy: 0.8296 - val_loss: 0.3436 - val_binary_accuracy: 0.8453\n",
      "Epoch 35/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3948 - binary_accuracy: 0.8265\n",
      "Epoch 35: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3944 - binary_accuracy: 0.8268 - val_loss: 0.3457 - val_binary_accuracy: 0.8514\n",
      "Epoch 36/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3992 - binary_accuracy: 0.8239\n",
      "Epoch 36: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3992 - binary_accuracy: 0.8237 - val_loss: 0.3517 - val_binary_accuracy: 0.8473\n",
      "Epoch 37/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3956 - binary_accuracy: 0.8266\n",
      "Epoch 37: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3958 - binary_accuracy: 0.8265 - val_loss: 0.3438 - val_binary_accuracy: 0.8481\n",
      "Epoch 38/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3914 - binary_accuracy: 0.8280\n",
      "Epoch 38: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3910 - binary_accuracy: 0.8282 - val_loss: 0.3412 - val_binary_accuracy: 0.8492\n",
      "Epoch 39/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3921 - binary_accuracy: 0.8282\n",
      "Epoch 39: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3919 - binary_accuracy: 0.8285 - val_loss: 0.3394 - val_binary_accuracy: 0.8506\n",
      "Epoch 40/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3905 - binary_accuracy: 0.8287\n",
      "Epoch 40: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3907 - binary_accuracy: 0.8287 - val_loss: 0.3391 - val_binary_accuracy: 0.8511\n",
      "Epoch 41/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3919 - binary_accuracy: 0.8283\n",
      "Epoch 41: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3915 - binary_accuracy: 0.8283 - val_loss: 0.3531 - val_binary_accuracy: 0.8492\n",
      "Epoch 42/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3875 - binary_accuracy: 0.8327\n",
      "Epoch 42: val_binary_accuracy did not improve from 0.85250\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3866 - binary_accuracy: 0.8332 - val_loss: 0.3371 - val_binary_accuracy: 0.8503\n",
      "Epoch 43/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3912 - binary_accuracy: 0.8289\n",
      "Epoch 43: val_binary_accuracy improved from 0.85250 to 0.85470, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3913 - binary_accuracy: 0.8289 - val_loss: 0.3357 - val_binary_accuracy: 0.8547\n",
      "Epoch 44/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3845 - binary_accuracy: 0.8316\n",
      "Epoch 44: val_binary_accuracy did not improve from 0.85470\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3846 - binary_accuracy: 0.8317 - val_loss: 0.3347 - val_binary_accuracy: 0.8514\n",
      "Epoch 45/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3845 - binary_accuracy: 0.8330\n",
      "Epoch 45: val_binary_accuracy improved from 0.85470 to 0.85525, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3843 - binary_accuracy: 0.8332 - val_loss: 0.3373 - val_binary_accuracy: 0.8553\n",
      "Epoch 46/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3822 - binary_accuracy: 0.8338\n",
      "Epoch 46: val_binary_accuracy did not improve from 0.85525\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3822 - binary_accuracy: 0.8337 - val_loss: 0.3301 - val_binary_accuracy: 0.8519\n",
      "Epoch 47/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3856 - binary_accuracy: 0.8313\n",
      "Epoch 47: val_binary_accuracy did not improve from 0.85525\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3857 - binary_accuracy: 0.8314 - val_loss: 0.3306 - val_binary_accuracy: 0.8539\n",
      "Epoch 48/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3835 - binary_accuracy: 0.8301\n",
      "Epoch 48: val_binary_accuracy did not improve from 0.85525\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3835 - binary_accuracy: 0.8305 - val_loss: 0.3298 - val_binary_accuracy: 0.8539\n",
      "Epoch 49/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3856 - binary_accuracy: 0.8321\n",
      "Epoch 49: val_binary_accuracy improved from 0.85525 to 0.85718, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3856 - binary_accuracy: 0.8321 - val_loss: 0.3301 - val_binary_accuracy: 0.8572\n",
      "Epoch 50/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3811 - binary_accuracy: 0.8350\n",
      "Epoch 50: val_binary_accuracy did not improve from 0.85718\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3811 - binary_accuracy: 0.8351 - val_loss: 0.3360 - val_binary_accuracy: 0.8519\n",
      "Epoch 51/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3822 - binary_accuracy: 0.8316\n",
      "Epoch 51: val_binary_accuracy did not improve from 0.85718\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3820 - binary_accuracy: 0.8315 - val_loss: 0.3254 - val_binary_accuracy: 0.8561\n",
      "Epoch 52/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3808 - binary_accuracy: 0.8327\n",
      "Epoch 52: val_binary_accuracy did not improve from 0.85718\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3808 - binary_accuracy: 0.8326 - val_loss: 0.3288 - val_binary_accuracy: 0.8572\n",
      "Epoch 53/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3819 - binary_accuracy: 0.8340\n",
      "Epoch 53: val_binary_accuracy did not improve from 0.85718\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3812 - binary_accuracy: 0.8345 - val_loss: 0.3262 - val_binary_accuracy: 0.8536\n",
      "Epoch 54/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3770 - binary_accuracy: 0.8359\n",
      "Epoch 54: val_binary_accuracy improved from 0.85718 to 0.85994, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3776 - binary_accuracy: 0.8356 - val_loss: 0.3275 - val_binary_accuracy: 0.8599\n",
      "Epoch 55/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3809 - binary_accuracy: 0.8358\n",
      "Epoch 55: val_binary_accuracy did not improve from 0.85994\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3809 - binary_accuracy: 0.8358 - val_loss: 0.3331 - val_binary_accuracy: 0.8525\n",
      "Epoch 56/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3795 - binary_accuracy: 0.8334\n",
      "Epoch 56: val_binary_accuracy improved from 0.85994 to 0.86325, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.3794 - binary_accuracy: 0.8335 - val_loss: 0.3255 - val_binary_accuracy: 0.8632\n",
      "Epoch 57/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3793 - binary_accuracy: 0.8338\n",
      "Epoch 57: val_binary_accuracy did not improve from 0.86325\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3790 - binary_accuracy: 0.8341 - val_loss: 0.3258 - val_binary_accuracy: 0.8541\n",
      "Epoch 58/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3768 - binary_accuracy: 0.8345\n",
      "Epoch 58: val_binary_accuracy did not improve from 0.86325\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3768 - binary_accuracy: 0.8345 - val_loss: 0.3283 - val_binary_accuracy: 0.8541\n",
      "Epoch 59/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3789 - binary_accuracy: 0.8351\n",
      "Epoch 59: val_binary_accuracy did not improve from 0.86325\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3782 - binary_accuracy: 0.8355 - val_loss: 0.3213 - val_binary_accuracy: 0.8577\n",
      "Epoch 60/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3770 - binary_accuracy: 0.8358\n",
      "Epoch 60: val_binary_accuracy did not improve from 0.86325\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3765 - binary_accuracy: 0.8363 - val_loss: 0.3255 - val_binary_accuracy: 0.8550\n",
      "Epoch 61/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3760 - binary_accuracy: 0.8380\n",
      "Epoch 61: val_binary_accuracy did not improve from 0.86325\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3763 - binary_accuracy: 0.8376 - val_loss: 0.3277 - val_binary_accuracy: 0.8602\n",
      "Epoch 62/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3736 - binary_accuracy: 0.8367\n",
      "Epoch 62: val_binary_accuracy did not improve from 0.86325\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3737 - binary_accuracy: 0.8368 - val_loss: 0.3206 - val_binary_accuracy: 0.8619\n",
      "Epoch 63/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3728 - binary_accuracy: 0.8392\n",
      "Epoch 63: val_binary_accuracy improved from 0.86325 to 0.86408, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3722 - binary_accuracy: 0.8394 - val_loss: 0.3181 - val_binary_accuracy: 0.8641\n",
      "Epoch 64/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3750 - binary_accuracy: 0.8367\n",
      "Epoch 64: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3748 - binary_accuracy: 0.8365 - val_loss: 0.3258 - val_binary_accuracy: 0.8553\n",
      "Epoch 65/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3731 - binary_accuracy: 0.8374\n",
      "Epoch 65: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3729 - binary_accuracy: 0.8375 - val_loss: 0.3231 - val_binary_accuracy: 0.8569\n",
      "Epoch 66/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3732 - binary_accuracy: 0.8369\n",
      "Epoch 66: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3732 - binary_accuracy: 0.8368 - val_loss: 0.3198 - val_binary_accuracy: 0.8624\n",
      "Epoch 67/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3722 - binary_accuracy: 0.8388\n",
      "Epoch 67: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3723 - binary_accuracy: 0.8388 - val_loss: 0.3164 - val_binary_accuracy: 0.8577\n",
      "Epoch 68/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3715 - binary_accuracy: 0.8396\n",
      "Epoch 68: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3720 - binary_accuracy: 0.8392 - val_loss: 0.3234 - val_binary_accuracy: 0.8539\n",
      "Epoch 69/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3679 - binary_accuracy: 0.8411\n",
      "Epoch 69: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3683 - binary_accuracy: 0.8407 - val_loss: 0.3215 - val_binary_accuracy: 0.8597\n",
      "Epoch 70/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3696 - binary_accuracy: 0.8405\n",
      "Epoch 70: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3702 - binary_accuracy: 0.8400 - val_loss: 0.3210 - val_binary_accuracy: 0.8599\n",
      "Epoch 71/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3686 - binary_accuracy: 0.8384\n",
      "Epoch 71: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3687 - binary_accuracy: 0.8383 - val_loss: 0.3243 - val_binary_accuracy: 0.8624\n",
      "Epoch 72/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3657 - binary_accuracy: 0.8403\n",
      "Epoch 72: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3657 - binary_accuracy: 0.8404 - val_loss: 0.3232 - val_binary_accuracy: 0.8616\n",
      "Epoch 73/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3670 - binary_accuracy: 0.8411\n",
      "Epoch 73: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3667 - binary_accuracy: 0.8413 - val_loss: 0.3154 - val_binary_accuracy: 0.8610\n",
      "Epoch 74/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3667 - binary_accuracy: 0.8405\n",
      "Epoch 74: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3672 - binary_accuracy: 0.8401 - val_loss: 0.3193 - val_binary_accuracy: 0.8635\n",
      "Epoch 75/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3628 - binary_accuracy: 0.8438\n",
      "Epoch 75: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3636 - binary_accuracy: 0.8434 - val_loss: 0.3126 - val_binary_accuracy: 0.8619\n",
      "Epoch 76/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3660 - binary_accuracy: 0.8419\n",
      "Epoch 76: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3660 - binary_accuracy: 0.8419 - val_loss: 0.3244 - val_binary_accuracy: 0.8577\n",
      "Epoch 77/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3673 - binary_accuracy: 0.8390\n",
      "Epoch 77: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3670 - binary_accuracy: 0.8392 - val_loss: 0.3181 - val_binary_accuracy: 0.8635\n",
      "Epoch 78/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3671 - binary_accuracy: 0.8403\n",
      "Epoch 78: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3669 - binary_accuracy: 0.8404 - val_loss: 0.3165 - val_binary_accuracy: 0.8627\n",
      "Epoch 79/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3637 - binary_accuracy: 0.8420\n",
      "Epoch 79: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3637 - binary_accuracy: 0.8420 - val_loss: 0.3158 - val_binary_accuracy: 0.8605\n",
      "Epoch 80/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3657 - binary_accuracy: 0.8410\n",
      "Epoch 80: val_binary_accuracy did not improve from 0.86408\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3655 - binary_accuracy: 0.8411 - val_loss: 0.3175 - val_binary_accuracy: 0.8597\n",
      "Epoch 81/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3643 - binary_accuracy: 0.8426\n",
      "Epoch 81: val_binary_accuracy improved from 0.86408 to 0.86656, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3643 - binary_accuracy: 0.8424 - val_loss: 0.3117 - val_binary_accuracy: 0.8666\n",
      "Epoch 82/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3597 - binary_accuracy: 0.8458\n",
      "Epoch 82: val_binary_accuracy did not improve from 0.86656\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3592 - binary_accuracy: 0.8459 - val_loss: 0.3173 - val_binary_accuracy: 0.8632\n",
      "Epoch 83/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3626 - binary_accuracy: 0.8405\n",
      "Epoch 83: val_binary_accuracy did not improve from 0.86656\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3623 - binary_accuracy: 0.8406 - val_loss: 0.3152 - val_binary_accuracy: 0.8621\n",
      "Epoch 84/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3669 - binary_accuracy: 0.8395\n",
      "Epoch 84: val_binary_accuracy did not improve from 0.86656\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3666 - binary_accuracy: 0.8398 - val_loss: 0.3106 - val_binary_accuracy: 0.8635\n",
      "Epoch 85/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3602 - binary_accuracy: 0.8439\n",
      "Epoch 85: val_binary_accuracy improved from 0.86656 to 0.86876, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3602 - binary_accuracy: 0.8439 - val_loss: 0.3116 - val_binary_accuracy: 0.8688\n",
      "Epoch 86/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3586 - binary_accuracy: 0.8438\n",
      "Epoch 86: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3587 - binary_accuracy: 0.8439 - val_loss: 0.3065 - val_binary_accuracy: 0.8644\n",
      "Epoch 87/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3590 - binary_accuracy: 0.8446\n",
      "Epoch 87: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3593 - binary_accuracy: 0.8444 - val_loss: 0.3090 - val_binary_accuracy: 0.8599\n",
      "Epoch 88/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3605 - binary_accuracy: 0.8413\n",
      "Epoch 88: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3605 - binary_accuracy: 0.8413 - val_loss: 0.3127 - val_binary_accuracy: 0.8594\n",
      "Epoch 89/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3584 - binary_accuracy: 0.8448\n",
      "Epoch 89: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3582 - binary_accuracy: 0.8451 - val_loss: 0.3110 - val_binary_accuracy: 0.8621\n",
      "Epoch 90/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3572 - binary_accuracy: 0.8441\n",
      "Epoch 90: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3581 - binary_accuracy: 0.8435 - val_loss: 0.3065 - val_binary_accuracy: 0.8641\n",
      "Epoch 91/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3603 - binary_accuracy: 0.8435\n",
      "Epoch 91: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3604 - binary_accuracy: 0.8434 - val_loss: 0.3091 - val_binary_accuracy: 0.8674\n",
      "Epoch 92/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3588 - binary_accuracy: 0.8424\n",
      "Epoch 92: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3591 - binary_accuracy: 0.8421 - val_loss: 0.3091 - val_binary_accuracy: 0.8666\n",
      "Epoch 93/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3561 - binary_accuracy: 0.8435\n",
      "Epoch 93: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3564 - binary_accuracy: 0.8435 - val_loss: 0.3092 - val_binary_accuracy: 0.8610\n",
      "Epoch 94/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3565 - binary_accuracy: 0.8449\n",
      "Epoch 94: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3558 - binary_accuracy: 0.8453 - val_loss: 0.3131 - val_binary_accuracy: 0.8586\n",
      "Epoch 95/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3590 - binary_accuracy: 0.8427\n",
      "Epoch 95: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3586 - binary_accuracy: 0.8428 - val_loss: 0.3110 - val_binary_accuracy: 0.8679\n",
      "Epoch 96/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3527 - binary_accuracy: 0.8455\n",
      "Epoch 96: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3534 - binary_accuracy: 0.8453 - val_loss: 0.3098 - val_binary_accuracy: 0.8630\n",
      "Epoch 97/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3554 - binary_accuracy: 0.8435\n",
      "Epoch 97: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3545 - binary_accuracy: 0.8440 - val_loss: 0.3075 - val_binary_accuracy: 0.8655\n",
      "Epoch 98/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3571 - binary_accuracy: 0.8460\n",
      "Epoch 98: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3570 - binary_accuracy: 0.8459 - val_loss: 0.3139 - val_binary_accuracy: 0.8608\n",
      "Epoch 99/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3519 - binary_accuracy: 0.8473\n",
      "Epoch 99: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3521 - binary_accuracy: 0.8471 - val_loss: 0.3111 - val_binary_accuracy: 0.8621\n",
      "Epoch 100/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3533 - binary_accuracy: 0.8447\n",
      "Epoch 100: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3532 - binary_accuracy: 0.8449 - val_loss: 0.3062 - val_binary_accuracy: 0.8616\n",
      "Epoch 101/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3561 - binary_accuracy: 0.8425\n",
      "Epoch 101: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3566 - binary_accuracy: 0.8422 - val_loss: 0.3043 - val_binary_accuracy: 0.8682\n",
      "Epoch 102/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3568 - binary_accuracy: 0.8440\n",
      "Epoch 102: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3567 - binary_accuracy: 0.8439 - val_loss: 0.3061 - val_binary_accuracy: 0.8644\n",
      "Epoch 103/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3544 - binary_accuracy: 0.8441\n",
      "Epoch 103: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3539 - binary_accuracy: 0.8446 - val_loss: 0.3096 - val_binary_accuracy: 0.8608\n",
      "Epoch 104/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3509 - binary_accuracy: 0.8445\n",
      "Epoch 104: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3511 - binary_accuracy: 0.8444 - val_loss: 0.3133 - val_binary_accuracy: 0.8630\n",
      "Epoch 105/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3536 - binary_accuracy: 0.8441\n",
      "Epoch 105: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3536 - binary_accuracy: 0.8441 - val_loss: 0.3103 - val_binary_accuracy: 0.8608\n",
      "Epoch 106/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3529 - binary_accuracy: 0.8467\n",
      "Epoch 106: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3527 - binary_accuracy: 0.8468 - val_loss: 0.3064 - val_binary_accuracy: 0.8621\n",
      "Epoch 107/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3549 - binary_accuracy: 0.8441\n",
      "Epoch 107: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3546 - binary_accuracy: 0.8444 - val_loss: 0.3013 - val_binary_accuracy: 0.8685\n",
      "Epoch 108/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3531 - binary_accuracy: 0.8448\n",
      "Epoch 108: val_binary_accuracy did not improve from 0.86876\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3525 - binary_accuracy: 0.8452 - val_loss: 0.3081 - val_binary_accuracy: 0.8635\n",
      "Epoch 109/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3477 - binary_accuracy: 0.8494\n",
      "Epoch 109: val_binary_accuracy improved from 0.86876 to 0.86904, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3477 - binary_accuracy: 0.8494 - val_loss: 0.3053 - val_binary_accuracy: 0.8690\n",
      "Epoch 110/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3498 - binary_accuracy: 0.8479\n",
      "Epoch 110: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3502 - binary_accuracy: 0.8480 - val_loss: 0.3031 - val_binary_accuracy: 0.8638\n",
      "Epoch 111/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3491 - binary_accuracy: 0.8484\n",
      "Epoch 111: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3486 - binary_accuracy: 0.8488 - val_loss: 0.3011 - val_binary_accuracy: 0.8663\n",
      "Epoch 112/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3511 - binary_accuracy: 0.8478\n",
      "Epoch 112: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3513 - binary_accuracy: 0.8478 - val_loss: 0.3018 - val_binary_accuracy: 0.8688\n",
      "Epoch 113/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3469 - binary_accuracy: 0.8493\n",
      "Epoch 113: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3465 - binary_accuracy: 0.8495 - val_loss: 0.2998 - val_binary_accuracy: 0.8608\n",
      "Epoch 114/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3477 - binary_accuracy: 0.8465\n",
      "Epoch 114: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3476 - binary_accuracy: 0.8466 - val_loss: 0.3097 - val_binary_accuracy: 0.8677\n",
      "Epoch 115/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3503 - binary_accuracy: 0.8485\n",
      "Epoch 115: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3507 - binary_accuracy: 0.8483 - val_loss: 0.3120 - val_binary_accuracy: 0.8591\n",
      "Epoch 116/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3486 - binary_accuracy: 0.8466\n",
      "Epoch 116: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3491 - binary_accuracy: 0.8463 - val_loss: 0.3033 - val_binary_accuracy: 0.8666\n",
      "Epoch 117/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3497 - binary_accuracy: 0.8471\n",
      "Epoch 117: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3495 - binary_accuracy: 0.8473 - val_loss: 0.3011 - val_binary_accuracy: 0.8685\n",
      "Epoch 118/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3484 - binary_accuracy: 0.8474\n",
      "Epoch 118: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3480 - binary_accuracy: 0.8477 - val_loss: 0.3017 - val_binary_accuracy: 0.8679\n",
      "Epoch 119/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3478 - binary_accuracy: 0.8466\n",
      "Epoch 119: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3476 - binary_accuracy: 0.8465 - val_loss: 0.3034 - val_binary_accuracy: 0.8649\n",
      "Epoch 120/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3450 - binary_accuracy: 0.8511\n",
      "Epoch 120: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3452 - binary_accuracy: 0.8509 - val_loss: 0.3086 - val_binary_accuracy: 0.8660\n",
      "Epoch 121/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3472 - binary_accuracy: 0.8473\n",
      "Epoch 121: val_binary_accuracy did not improve from 0.86904\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3470 - binary_accuracy: 0.8472 - val_loss: 0.3076 - val_binary_accuracy: 0.8632\n",
      "Epoch 122/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3456 - binary_accuracy: 0.8496\n",
      "Epoch 122: val_binary_accuracy improved from 0.86904 to 0.87345, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3453 - binary_accuracy: 0.8498 - val_loss: 0.2975 - val_binary_accuracy: 0.8734\n",
      "Epoch 123/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3453 - binary_accuracy: 0.8489\n",
      "Epoch 123: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3453 - binary_accuracy: 0.8490 - val_loss: 0.3016 - val_binary_accuracy: 0.8723\n",
      "Epoch 124/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3417 - binary_accuracy: 0.8510\n",
      "Epoch 124: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3416 - binary_accuracy: 0.8510 - val_loss: 0.2977 - val_binary_accuracy: 0.8677\n",
      "Epoch 125/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3462 - binary_accuracy: 0.8503\n",
      "Epoch 125: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3464 - binary_accuracy: 0.8500 - val_loss: 0.3030 - val_binary_accuracy: 0.8668\n",
      "Epoch 126/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3413 - binary_accuracy: 0.8484\n",
      "Epoch 126: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3412 - binary_accuracy: 0.8484 - val_loss: 0.2981 - val_binary_accuracy: 0.8701\n",
      "Epoch 127/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3389 - binary_accuracy: 0.8524\n",
      "Epoch 127: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3395 - binary_accuracy: 0.8520 - val_loss: 0.2989 - val_binary_accuracy: 0.8693\n",
      "Epoch 128/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3416 - binary_accuracy: 0.8506\n",
      "Epoch 128: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3419 - binary_accuracy: 0.8506 - val_loss: 0.2955 - val_binary_accuracy: 0.8723\n",
      "Epoch 129/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3450 - binary_accuracy: 0.8494\n",
      "Epoch 129: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3451 - binary_accuracy: 0.8495 - val_loss: 0.2987 - val_binary_accuracy: 0.8693\n",
      "Epoch 130/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3418 - binary_accuracy: 0.8494\n",
      "Epoch 130: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3418 - binary_accuracy: 0.8495 - val_loss: 0.2999 - val_binary_accuracy: 0.8688\n",
      "Epoch 131/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3451 - binary_accuracy: 0.8498\n",
      "Epoch 131: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3450 - binary_accuracy: 0.8499 - val_loss: 0.2997 - val_binary_accuracy: 0.8710\n",
      "Epoch 132/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3434 - binary_accuracy: 0.8484\n",
      "Epoch 132: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3433 - binary_accuracy: 0.8484 - val_loss: 0.2987 - val_binary_accuracy: 0.8704\n",
      "Epoch 133/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3431 - binary_accuracy: 0.8526\n",
      "Epoch 133: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3432 - binary_accuracy: 0.8525 - val_loss: 0.2957 - val_binary_accuracy: 0.8712\n",
      "Epoch 134/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3430 - binary_accuracy: 0.8489\n",
      "Epoch 134: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3437 - binary_accuracy: 0.8484 - val_loss: 0.2961 - val_binary_accuracy: 0.8718\n",
      "Epoch 135/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3415 - binary_accuracy: 0.8533\n",
      "Epoch 135: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3414 - binary_accuracy: 0.8533 - val_loss: 0.2949 - val_binary_accuracy: 0.8712\n",
      "Epoch 136/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3418 - binary_accuracy: 0.8504\n",
      "Epoch 136: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3415 - binary_accuracy: 0.8506 - val_loss: 0.2978 - val_binary_accuracy: 0.8663\n",
      "Epoch 137/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3396 - binary_accuracy: 0.8527\n",
      "Epoch 137: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3395 - binary_accuracy: 0.8528 - val_loss: 0.2994 - val_binary_accuracy: 0.8723\n",
      "Epoch 138/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3402 - binary_accuracy: 0.8508\n",
      "Epoch 138: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3398 - binary_accuracy: 0.8510 - val_loss: 0.2943 - val_binary_accuracy: 0.8701\n",
      "Epoch 139/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3382 - binary_accuracy: 0.8506\n",
      "Epoch 139: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3380 - binary_accuracy: 0.8507 - val_loss: 0.2929 - val_binary_accuracy: 0.8696\n",
      "Epoch 140/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3396 - binary_accuracy: 0.8506\n",
      "Epoch 140: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3400 - binary_accuracy: 0.8504 - val_loss: 0.2977 - val_binary_accuracy: 0.8704\n",
      "Epoch 141/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3382 - binary_accuracy: 0.8523\n",
      "Epoch 141: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3379 - binary_accuracy: 0.8527 - val_loss: 0.2952 - val_binary_accuracy: 0.8721\n",
      "Epoch 142/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3373 - binary_accuracy: 0.8525\n",
      "Epoch 142: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3374 - binary_accuracy: 0.8525 - val_loss: 0.2930 - val_binary_accuracy: 0.8666\n",
      "Epoch 143/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3413 - binary_accuracy: 0.8524\n",
      "Epoch 143: val_binary_accuracy did not improve from 0.87345\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3417 - binary_accuracy: 0.8521 - val_loss: 0.2908 - val_binary_accuracy: 0.8693\n",
      "Epoch 144/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3421 - binary_accuracy: 0.8494\n",
      "Epoch 144: val_binary_accuracy improved from 0.87345 to 0.87593, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.3419 - binary_accuracy: 0.8496 - val_loss: 0.2953 - val_binary_accuracy: 0.8759\n",
      "Epoch 145/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3400 - binary_accuracy: 0.8513\n",
      "Epoch 145: val_binary_accuracy did not improve from 0.87593\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3395 - binary_accuracy: 0.8515 - val_loss: 0.2932 - val_binary_accuracy: 0.8729\n",
      "Epoch 146/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3361 - binary_accuracy: 0.8536\n",
      "Epoch 146: val_binary_accuracy did not improve from 0.87593\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3361 - binary_accuracy: 0.8536 - val_loss: 0.2910 - val_binary_accuracy: 0.8712\n",
      "Epoch 147/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3357 - binary_accuracy: 0.8542\n",
      "Epoch 147: val_binary_accuracy improved from 0.87593 to 0.87648, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.3359 - binary_accuracy: 0.8540 - val_loss: 0.2978 - val_binary_accuracy: 0.8765\n",
      "Epoch 148/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3347 - binary_accuracy: 0.8521\n",
      "Epoch 148: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3347 - binary_accuracy: 0.8521 - val_loss: 0.2931 - val_binary_accuracy: 0.8671\n",
      "Epoch 149/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3358 - binary_accuracy: 0.8530\n",
      "Epoch 149: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.3355 - binary_accuracy: 0.8529 - val_loss: 0.2910 - val_binary_accuracy: 0.8674\n",
      "Epoch 150/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3358 - binary_accuracy: 0.8522\n",
      "Epoch 150: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3360 - binary_accuracy: 0.8520 - val_loss: 0.2948 - val_binary_accuracy: 0.8710\n",
      "Epoch 151/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3329 - binary_accuracy: 0.8559\n",
      "Epoch 151: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3324 - binary_accuracy: 0.8562 - val_loss: 0.2973 - val_binary_accuracy: 0.8740\n",
      "Epoch 152/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3364 - binary_accuracy: 0.8546\n",
      "Epoch 152: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3358 - binary_accuracy: 0.8548 - val_loss: 0.2936 - val_binary_accuracy: 0.8723\n",
      "Epoch 153/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3310 - binary_accuracy: 0.8560\n",
      "Epoch 153: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3308 - binary_accuracy: 0.8561 - val_loss: 0.2929 - val_binary_accuracy: 0.8734\n",
      "Epoch 154/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3367 - binary_accuracy: 0.8497\n",
      "Epoch 154: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3370 - binary_accuracy: 0.8496 - val_loss: 0.2899 - val_binary_accuracy: 0.8751\n",
      "Epoch 155/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3302 - binary_accuracy: 0.8560\n",
      "Epoch 155: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3299 - binary_accuracy: 0.8561 - val_loss: 0.2942 - val_binary_accuracy: 0.8693\n",
      "Epoch 156/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3347 - binary_accuracy: 0.8531\n",
      "Epoch 156: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3347 - binary_accuracy: 0.8530 - val_loss: 0.2928 - val_binary_accuracy: 0.8757\n",
      "Epoch 157/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3370 - binary_accuracy: 0.8516\n",
      "Epoch 157: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3368 - binary_accuracy: 0.8513 - val_loss: 0.2876 - val_binary_accuracy: 0.8759\n",
      "Epoch 158/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3321 - binary_accuracy: 0.8540\n",
      "Epoch 158: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3322 - binary_accuracy: 0.8539 - val_loss: 0.2911 - val_binary_accuracy: 0.8718\n",
      "Epoch 159/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3299 - binary_accuracy: 0.8551\n",
      "Epoch 159: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3303 - binary_accuracy: 0.8548 - val_loss: 0.2909 - val_binary_accuracy: 0.8693\n",
      "Epoch 160/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3324 - binary_accuracy: 0.8548\n",
      "Epoch 160: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3322 - binary_accuracy: 0.8548 - val_loss: 0.2882 - val_binary_accuracy: 0.8754\n",
      "Epoch 161/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3293 - binary_accuracy: 0.8556\n",
      "Epoch 161: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3290 - binary_accuracy: 0.8557 - val_loss: 0.2907 - val_binary_accuracy: 0.8746\n",
      "Epoch 162/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3274 - binary_accuracy: 0.8572\n",
      "Epoch 162: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3271 - binary_accuracy: 0.8573 - val_loss: 0.2913 - val_binary_accuracy: 0.8710\n",
      "Epoch 163/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3315 - binary_accuracy: 0.8554\n",
      "Epoch 163: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3317 - binary_accuracy: 0.8554 - val_loss: 0.2908 - val_binary_accuracy: 0.8746\n",
      "Epoch 164/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3326 - binary_accuracy: 0.8548\n",
      "Epoch 164: val_binary_accuracy did not improve from 0.87648\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3327 - binary_accuracy: 0.8548 - val_loss: 0.2887 - val_binary_accuracy: 0.8748\n",
      "Epoch 165/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3320 - binary_accuracy: 0.8534\n",
      "Epoch 165: val_binary_accuracy improved from 0.87648 to 0.87676, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3321 - binary_accuracy: 0.8532 - val_loss: 0.2885 - val_binary_accuracy: 0.8768\n",
      "Epoch 166/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3294 - binary_accuracy: 0.8568\n",
      "Epoch 166: val_binary_accuracy did not improve from 0.87676\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3295 - binary_accuracy: 0.8569 - val_loss: 0.2915 - val_binary_accuracy: 0.8734\n",
      "Epoch 167/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3296 - binary_accuracy: 0.8541\n",
      "Epoch 167: val_binary_accuracy did not improve from 0.87676\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3295 - binary_accuracy: 0.8540 - val_loss: 0.2850 - val_binary_accuracy: 0.8754\n",
      "Epoch 168/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3295 - binary_accuracy: 0.8556\n",
      "Epoch 168: val_binary_accuracy did not improve from 0.87676\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3295 - binary_accuracy: 0.8555 - val_loss: 0.2920 - val_binary_accuracy: 0.8762\n",
      "Epoch 169/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3279 - binary_accuracy: 0.8560\n",
      "Epoch 169: val_binary_accuracy improved from 0.87676 to 0.87703, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.3284 - binary_accuracy: 0.8560 - val_loss: 0.2867 - val_binary_accuracy: 0.8770\n",
      "Epoch 170/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3267 - binary_accuracy: 0.8562\n",
      "Epoch 170: val_binary_accuracy did not improve from 0.87703\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3269 - binary_accuracy: 0.8561 - val_loss: 0.2884 - val_binary_accuracy: 0.8726\n",
      "Epoch 171/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3234 - binary_accuracy: 0.8588\n",
      "Epoch 171: val_binary_accuracy did not improve from 0.87703\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3239 - binary_accuracy: 0.8586 - val_loss: 0.2837 - val_binary_accuracy: 0.8768\n",
      "Epoch 172/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3286 - binary_accuracy: 0.8561\n",
      "Epoch 172: val_binary_accuracy improved from 0.87703 to 0.88034, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.3282 - binary_accuracy: 0.8566 - val_loss: 0.2878 - val_binary_accuracy: 0.8803\n",
      "Epoch 173/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3303 - binary_accuracy: 0.8564\n",
      "Epoch 173: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3302 - binary_accuracy: 0.8565 - val_loss: 0.2860 - val_binary_accuracy: 0.8784\n",
      "Epoch 174/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3290 - binary_accuracy: 0.8568\n",
      "Epoch 174: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3291 - binary_accuracy: 0.8568 - val_loss: 0.2865 - val_binary_accuracy: 0.8795\n",
      "Epoch 175/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3270 - binary_accuracy: 0.8568\n",
      "Epoch 175: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3268 - binary_accuracy: 0.8570 - val_loss: 0.2866 - val_binary_accuracy: 0.8790\n",
      "Epoch 176/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3237 - binary_accuracy: 0.8602\n",
      "Epoch 176: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3235 - binary_accuracy: 0.8604 - val_loss: 0.2875 - val_binary_accuracy: 0.8773\n",
      "Epoch 177/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3281 - binary_accuracy: 0.8565\n",
      "Epoch 177: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3280 - binary_accuracy: 0.8565 - val_loss: 0.2907 - val_binary_accuracy: 0.8721\n",
      "Epoch 178/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3286 - binary_accuracy: 0.8561\n",
      "Epoch 178: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3287 - binary_accuracy: 0.8561 - val_loss: 0.2864 - val_binary_accuracy: 0.8757\n",
      "Epoch 179/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3282 - binary_accuracy: 0.8566\n",
      "Epoch 179: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3276 - binary_accuracy: 0.8569 - val_loss: 0.2858 - val_binary_accuracy: 0.8748\n",
      "Epoch 180/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3249 - binary_accuracy: 0.8578\n",
      "Epoch 180: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3248 - binary_accuracy: 0.8577 - val_loss: 0.2790 - val_binary_accuracy: 0.8784\n",
      "Epoch 181/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3212 - binary_accuracy: 0.8602\n",
      "Epoch 181: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3213 - binary_accuracy: 0.8604 - val_loss: 0.2876 - val_binary_accuracy: 0.8746\n",
      "Epoch 182/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3256 - binary_accuracy: 0.8571\n",
      "Epoch 182: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3256 - binary_accuracy: 0.8571 - val_loss: 0.2860 - val_binary_accuracy: 0.8754\n",
      "Epoch 183/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3275 - binary_accuracy: 0.8555\n",
      "Epoch 183: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.3270 - binary_accuracy: 0.8558 - val_loss: 0.2912 - val_binary_accuracy: 0.8726\n",
      "Epoch 184/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3250 - binary_accuracy: 0.8602\n",
      "Epoch 184: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3247 - binary_accuracy: 0.8604 - val_loss: 0.2873 - val_binary_accuracy: 0.8723\n",
      "Epoch 185/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3257 - binary_accuracy: 0.8562\n",
      "Epoch 185: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3259 - binary_accuracy: 0.8560 - val_loss: 0.2862 - val_binary_accuracy: 0.8765\n",
      "Epoch 186/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3271 - binary_accuracy: 0.8551\n",
      "Epoch 186: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3265 - binary_accuracy: 0.8555 - val_loss: 0.2947 - val_binary_accuracy: 0.8715\n",
      "Epoch 187/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3242 - binary_accuracy: 0.8585\n",
      "Epoch 187: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3242 - binary_accuracy: 0.8585 - val_loss: 0.2879 - val_binary_accuracy: 0.8768\n",
      "Epoch 188/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3237 - binary_accuracy: 0.8601\n",
      "Epoch 188: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3237 - binary_accuracy: 0.8601 - val_loss: 0.2846 - val_binary_accuracy: 0.8781\n",
      "Epoch 189/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3208 - binary_accuracy: 0.8604\n",
      "Epoch 189: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3215 - binary_accuracy: 0.8599 - val_loss: 0.2893 - val_binary_accuracy: 0.8773\n",
      "Epoch 190/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3249 - binary_accuracy: 0.8577\n",
      "Epoch 190: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3249 - binary_accuracy: 0.8576 - val_loss: 0.2848 - val_binary_accuracy: 0.8751\n",
      "Epoch 191/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3196 - binary_accuracy: 0.8593\n",
      "Epoch 191: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3200 - binary_accuracy: 0.8593 - val_loss: 0.2843 - val_binary_accuracy: 0.8798\n",
      "Epoch 192/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3233 - binary_accuracy: 0.8581\n",
      "Epoch 192: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3231 - binary_accuracy: 0.8581 - val_loss: 0.2876 - val_binary_accuracy: 0.8743\n",
      "Epoch 193/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3215 - binary_accuracy: 0.8600\n",
      "Epoch 193: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3215 - binary_accuracy: 0.8600 - val_loss: 0.2830 - val_binary_accuracy: 0.8770\n",
      "Epoch 194/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3236 - binary_accuracy: 0.8562\n",
      "Epoch 194: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3237 - binary_accuracy: 0.8564 - val_loss: 0.2838 - val_binary_accuracy: 0.8776\n",
      "Epoch 195/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3221 - binary_accuracy: 0.8570\n",
      "Epoch 195: val_binary_accuracy did not improve from 0.88034\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3218 - binary_accuracy: 0.8573 - val_loss: 0.2855 - val_binary_accuracy: 0.8790\n",
      "Epoch 196/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3213 - binary_accuracy: 0.8595\n",
      "Epoch 196: val_binary_accuracy improved from 0.88034 to 0.88227, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3209 - binary_accuracy: 0.8598 - val_loss: 0.2793 - val_binary_accuracy: 0.8823\n",
      "Epoch 197/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3219 - binary_accuracy: 0.8585\n",
      "Epoch 197: val_binary_accuracy did not improve from 0.88227\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3217 - binary_accuracy: 0.8585 - val_loss: 0.2845 - val_binary_accuracy: 0.8773\n",
      "Epoch 198/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3204 - binary_accuracy: 0.8577\n",
      "Epoch 198: val_binary_accuracy did not improve from 0.88227\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3207 - binary_accuracy: 0.8576 - val_loss: 0.2857 - val_binary_accuracy: 0.8729\n",
      "Epoch 199/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3198 - binary_accuracy: 0.8589\n",
      "Epoch 199: val_binary_accuracy did not improve from 0.88227\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3198 - binary_accuracy: 0.8589 - val_loss: 0.2867 - val_binary_accuracy: 0.8748\n",
      "Epoch 200/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3206 - binary_accuracy: 0.8587\n",
      "Epoch 200: val_binary_accuracy improved from 0.88227 to 0.88365, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3207 - binary_accuracy: 0.8586 - val_loss: 0.2829 - val_binary_accuracy: 0.8837\n",
      "Epoch 201/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3189 - binary_accuracy: 0.8590\n",
      "Epoch 201: val_binary_accuracy did not improve from 0.88365\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3189 - binary_accuracy: 0.8588 - val_loss: 0.2801 - val_binary_accuracy: 0.8809\n",
      "Epoch 202/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3189 - binary_accuracy: 0.8605\n",
      "Epoch 202: val_binary_accuracy did not improve from 0.88365\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3196 - binary_accuracy: 0.8604 - val_loss: 0.2839 - val_binary_accuracy: 0.8751\n",
      "Epoch 203/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3199 - binary_accuracy: 0.8601\n",
      "Epoch 203: val_binary_accuracy did not improve from 0.88365\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3194 - binary_accuracy: 0.8605 - val_loss: 0.2818 - val_binary_accuracy: 0.8792\n",
      "Epoch 204/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3214 - binary_accuracy: 0.8566\n",
      "Epoch 204: val_binary_accuracy improved from 0.88365 to 0.88503, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3209 - binary_accuracy: 0.8568 - val_loss: 0.2770 - val_binary_accuracy: 0.8850\n",
      "Epoch 205/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3194 - binary_accuracy: 0.8598\n",
      "Epoch 205: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3193 - binary_accuracy: 0.8599 - val_loss: 0.2821 - val_binary_accuracy: 0.8809\n",
      "Epoch 206/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3213 - binary_accuracy: 0.8572\n",
      "Epoch 206: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3209 - binary_accuracy: 0.8576 - val_loss: 0.2820 - val_binary_accuracy: 0.8809\n",
      "Epoch 207/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3170 - binary_accuracy: 0.8605\n",
      "Epoch 207: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3170 - binary_accuracy: 0.8605 - val_loss: 0.2847 - val_binary_accuracy: 0.8707\n",
      "Epoch 208/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3179 - binary_accuracy: 0.8612\n",
      "Epoch 208: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3180 - binary_accuracy: 0.8612 - val_loss: 0.2795 - val_binary_accuracy: 0.8806\n",
      "Epoch 209/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3173 - binary_accuracy: 0.8595\n",
      "Epoch 209: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3177 - binary_accuracy: 0.8594 - val_loss: 0.2843 - val_binary_accuracy: 0.8787\n",
      "Epoch 210/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3185 - binary_accuracy: 0.8593\n",
      "Epoch 210: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3185 - binary_accuracy: 0.8595 - val_loss: 0.2807 - val_binary_accuracy: 0.8795\n",
      "Epoch 211/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3192 - binary_accuracy: 0.8573\n",
      "Epoch 211: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3189 - binary_accuracy: 0.8577 - val_loss: 0.2809 - val_binary_accuracy: 0.8798\n",
      "Epoch 212/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3151 - binary_accuracy: 0.8633\n",
      "Epoch 212: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3156 - binary_accuracy: 0.8630 - val_loss: 0.2816 - val_binary_accuracy: 0.8798\n",
      "Epoch 213/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3134 - binary_accuracy: 0.8630\n",
      "Epoch 213: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3136 - binary_accuracy: 0.8628 - val_loss: 0.2788 - val_binary_accuracy: 0.8839\n",
      "Epoch 214/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3187 - binary_accuracy: 0.8615\n",
      "Epoch 214: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3189 - binary_accuracy: 0.8616 - val_loss: 0.2787 - val_binary_accuracy: 0.8820\n",
      "Epoch 215/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3169 - binary_accuracy: 0.8605\n",
      "Epoch 215: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3167 - binary_accuracy: 0.8607 - val_loss: 0.2809 - val_binary_accuracy: 0.8803\n",
      "Epoch 216/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3183 - binary_accuracy: 0.8597\n",
      "Epoch 216: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3189 - binary_accuracy: 0.8597 - val_loss: 0.2792 - val_binary_accuracy: 0.8801\n",
      "Epoch 217/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3158 - binary_accuracy: 0.8620\n",
      "Epoch 217: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3159 - binary_accuracy: 0.8621 - val_loss: 0.2799 - val_binary_accuracy: 0.8765\n",
      "Epoch 218/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3141 - binary_accuracy: 0.8642\n",
      "Epoch 218: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3143 - binary_accuracy: 0.8639 - val_loss: 0.2813 - val_binary_accuracy: 0.8803\n",
      "Epoch 219/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3167 - binary_accuracy: 0.8590\n",
      "Epoch 219: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3166 - binary_accuracy: 0.8589 - val_loss: 0.2789 - val_binary_accuracy: 0.8784\n",
      "Epoch 220/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3158 - binary_accuracy: 0.8635\n",
      "Epoch 220: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3158 - binary_accuracy: 0.8635 - val_loss: 0.2813 - val_binary_accuracy: 0.8792\n",
      "Epoch 221/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3102 - binary_accuracy: 0.8628\n",
      "Epoch 221: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.3103 - binary_accuracy: 0.8627 - val_loss: 0.2871 - val_binary_accuracy: 0.8757\n",
      "Epoch 222/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3164 - binary_accuracy: 0.8601\n",
      "Epoch 222: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.3164 - binary_accuracy: 0.8601 - val_loss: 0.2814 - val_binary_accuracy: 0.8781\n",
      "Epoch 223/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3140 - binary_accuracy: 0.8655\n",
      "Epoch 223: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3138 - binary_accuracy: 0.8656 - val_loss: 0.2797 - val_binary_accuracy: 0.8770\n",
      "Epoch 224/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3146 - binary_accuracy: 0.8625\n",
      "Epoch 224: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3149 - binary_accuracy: 0.8624 - val_loss: 0.2845 - val_binary_accuracy: 0.8726\n",
      "Epoch 225/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3124 - binary_accuracy: 0.8645\n",
      "Epoch 225: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.3128 - binary_accuracy: 0.8644 - val_loss: 0.2833 - val_binary_accuracy: 0.8762\n",
      "Epoch 226/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3142 - binary_accuracy: 0.8649\n",
      "Epoch 226: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3134 - binary_accuracy: 0.8655 - val_loss: 0.2896 - val_binary_accuracy: 0.8757\n",
      "Epoch 227/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3157 - binary_accuracy: 0.8612\n",
      "Epoch 227: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3157 - binary_accuracy: 0.8612 - val_loss: 0.2797 - val_binary_accuracy: 0.8787\n",
      "Epoch 228/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3169 - binary_accuracy: 0.8616\n",
      "Epoch 228: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3164 - binary_accuracy: 0.8618 - val_loss: 0.2759 - val_binary_accuracy: 0.8837\n",
      "Epoch 229/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3154 - binary_accuracy: 0.8638\n",
      "Epoch 229: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3154 - binary_accuracy: 0.8636 - val_loss: 0.2795 - val_binary_accuracy: 0.8842\n",
      "Epoch 230/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3118 - binary_accuracy: 0.8627\n",
      "Epoch 230: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3115 - binary_accuracy: 0.8629 - val_loss: 0.2779 - val_binary_accuracy: 0.8823\n",
      "Epoch 231/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3111 - binary_accuracy: 0.8631\n",
      "Epoch 231: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3109 - binary_accuracy: 0.8629 - val_loss: 0.2803 - val_binary_accuracy: 0.8795\n",
      "Epoch 232/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3119 - binary_accuracy: 0.8633\n",
      "Epoch 232: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.3135 - binary_accuracy: 0.8631 - val_loss: 0.2780 - val_binary_accuracy: 0.8801\n",
      "Epoch 233/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3140 - binary_accuracy: 0.8622\n",
      "Epoch 233: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 66ms/step - loss: 0.3139 - binary_accuracy: 0.8623 - val_loss: 0.2775 - val_binary_accuracy: 0.8814\n",
      "Epoch 234/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3100 - binary_accuracy: 0.8653\n",
      "Epoch 234: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3100 - binary_accuracy: 0.8652 - val_loss: 0.2776 - val_binary_accuracy: 0.8825\n",
      "Epoch 235/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3058 - binary_accuracy: 0.8663\n",
      "Epoch 235: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3065 - binary_accuracy: 0.8659 - val_loss: 0.2733 - val_binary_accuracy: 0.8834\n",
      "Epoch 236/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3125 - binary_accuracy: 0.8645\n",
      "Epoch 236: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3127 - binary_accuracy: 0.8645 - val_loss: 0.2787 - val_binary_accuracy: 0.8817\n",
      "Epoch 237/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3095 - binary_accuracy: 0.8628\n",
      "Epoch 237: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3095 - binary_accuracy: 0.8628 - val_loss: 0.2797 - val_binary_accuracy: 0.8801\n",
      "Epoch 238/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3088 - binary_accuracy: 0.8655\n",
      "Epoch 238: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3088 - binary_accuracy: 0.8655 - val_loss: 0.2780 - val_binary_accuracy: 0.8790\n",
      "Epoch 239/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3091 - binary_accuracy: 0.8632\n",
      "Epoch 239: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3093 - binary_accuracy: 0.8629 - val_loss: 0.2809 - val_binary_accuracy: 0.8787\n",
      "Epoch 240/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3078 - binary_accuracy: 0.8646\n",
      "Epoch 240: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3078 - binary_accuracy: 0.8646 - val_loss: 0.2761 - val_binary_accuracy: 0.8848\n",
      "Epoch 241/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3072 - binary_accuracy: 0.8657\n",
      "Epoch 241: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3075 - binary_accuracy: 0.8657 - val_loss: 0.2762 - val_binary_accuracy: 0.8825\n",
      "Epoch 242/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3096 - binary_accuracy: 0.8628\n",
      "Epoch 242: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3100 - binary_accuracy: 0.8627 - val_loss: 0.2803 - val_binary_accuracy: 0.8792\n",
      "Epoch 243/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3078 - binary_accuracy: 0.8621\n",
      "Epoch 243: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3075 - binary_accuracy: 0.8625 - val_loss: 0.2766 - val_binary_accuracy: 0.8814\n",
      "Epoch 244/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3085 - binary_accuracy: 0.8631\n",
      "Epoch 244: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3081 - binary_accuracy: 0.8633 - val_loss: 0.2761 - val_binary_accuracy: 0.8806\n",
      "Epoch 245/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3093 - binary_accuracy: 0.8647\n",
      "Epoch 245: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3093 - binary_accuracy: 0.8647 - val_loss: 0.2754 - val_binary_accuracy: 0.8831\n",
      "Epoch 246/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3073 - binary_accuracy: 0.8642\n",
      "Epoch 246: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3072 - binary_accuracy: 0.8644 - val_loss: 0.2779 - val_binary_accuracy: 0.8779\n",
      "Epoch 247/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3063 - binary_accuracy: 0.8675\n",
      "Epoch 247: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3070 - binary_accuracy: 0.8672 - val_loss: 0.2794 - val_binary_accuracy: 0.8817\n",
      "Epoch 248/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3058 - binary_accuracy: 0.8642\n",
      "Epoch 248: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3061 - binary_accuracy: 0.8641 - val_loss: 0.2757 - val_binary_accuracy: 0.8839\n",
      "Epoch 249/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3079 - binary_accuracy: 0.8644\n",
      "Epoch 249: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3077 - binary_accuracy: 0.8645 - val_loss: 0.2733 - val_binary_accuracy: 0.8842\n",
      "Epoch 250/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3079 - binary_accuracy: 0.8672\n",
      "Epoch 250: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3076 - binary_accuracy: 0.8673 - val_loss: 0.2766 - val_binary_accuracy: 0.8817\n",
      "Epoch 251/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3076 - binary_accuracy: 0.8662\n",
      "Epoch 251: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3076 - binary_accuracy: 0.8662 - val_loss: 0.2819 - val_binary_accuracy: 0.8787\n",
      "Epoch 252/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3070 - binary_accuracy: 0.8636\n",
      "Epoch 252: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3075 - binary_accuracy: 0.8634 - val_loss: 0.2817 - val_binary_accuracy: 0.8792\n",
      "Epoch 253/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3076 - binary_accuracy: 0.8642\n",
      "Epoch 253: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3075 - binary_accuracy: 0.8641 - val_loss: 0.2770 - val_binary_accuracy: 0.8831\n",
      "Epoch 254/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3064 - binary_accuracy: 0.8651\n",
      "Epoch 254: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3062 - binary_accuracy: 0.8649 - val_loss: 0.2773 - val_binary_accuracy: 0.8839\n",
      "Epoch 255/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3071 - binary_accuracy: 0.8657\n",
      "Epoch 255: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3064 - binary_accuracy: 0.8660 - val_loss: 0.2768 - val_binary_accuracy: 0.8817\n",
      "Epoch 256/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3036 - binary_accuracy: 0.8656\n",
      "Epoch 256: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3031 - binary_accuracy: 0.8661 - val_loss: 0.2753 - val_binary_accuracy: 0.8795\n",
      "Epoch 257/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3063 - binary_accuracy: 0.8650\n",
      "Epoch 257: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3063 - binary_accuracy: 0.8650 - val_loss: 0.2783 - val_binary_accuracy: 0.8845\n",
      "Epoch 258/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3043 - binary_accuracy: 0.8666\n",
      "Epoch 258: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3048 - binary_accuracy: 0.8663 - val_loss: 0.2760 - val_binary_accuracy: 0.8837\n",
      "Epoch 259/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3023 - binary_accuracy: 0.8661\n",
      "Epoch 259: val_binary_accuracy did not improve from 0.88503\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3025 - binary_accuracy: 0.8661 - val_loss: 0.2786 - val_binary_accuracy: 0.8823\n",
      "Epoch 260/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3044 - binary_accuracy: 0.8674\n",
      "Epoch 260: val_binary_accuracy improved from 0.88503 to 0.88558, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3052 - binary_accuracy: 0.8671 - val_loss: 0.2723 - val_binary_accuracy: 0.8856\n",
      "Epoch 261/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3061 - binary_accuracy: 0.8640\n",
      "Epoch 261: val_binary_accuracy did not improve from 0.88558\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3058 - binary_accuracy: 0.8641 - val_loss: 0.2756 - val_binary_accuracy: 0.8839\n",
      "Epoch 262/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3097 - binary_accuracy: 0.8636\n",
      "Epoch 262: val_binary_accuracy improved from 0.88558 to 0.88696, saving model to model.hdf5\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.3097 - binary_accuracy: 0.8636 - val_loss: 0.2724 - val_binary_accuracy: 0.8870\n",
      "Epoch 263/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3070 - binary_accuracy: 0.8659\n",
      "Epoch 263: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.3070 - binary_accuracy: 0.8659 - val_loss: 0.2758 - val_binary_accuracy: 0.8820\n",
      "Epoch 264/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3055 - binary_accuracy: 0.8658\n",
      "Epoch 264: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 6s 99ms/step - loss: 0.3055 - binary_accuracy: 0.8658 - val_loss: 0.2733 - val_binary_accuracy: 0.8809\n",
      "Epoch 265/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3017 - binary_accuracy: 0.8696\n",
      "Epoch 265: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3017 - binary_accuracy: 0.8696 - val_loss: 0.2786 - val_binary_accuracy: 0.8781\n",
      "Epoch 266/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3039 - binary_accuracy: 0.8673\n",
      "Epoch 266: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.3047 - binary_accuracy: 0.8668 - val_loss: 0.2765 - val_binary_accuracy: 0.8823\n",
      "Epoch 267/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3013 - binary_accuracy: 0.8669\n",
      "Epoch 267: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.3013 - binary_accuracy: 0.8669 - val_loss: 0.2773 - val_binary_accuracy: 0.8820\n",
      "Epoch 268/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3062 - binary_accuracy: 0.8670\n",
      "Epoch 268: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3059 - binary_accuracy: 0.8673 - val_loss: 0.2769 - val_binary_accuracy: 0.8814\n",
      "Epoch 269/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3051 - binary_accuracy: 0.8678\n",
      "Epoch 269: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3053 - binary_accuracy: 0.8678 - val_loss: 0.2770 - val_binary_accuracy: 0.8803\n",
      "Epoch 270/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3015 - binary_accuracy: 0.8667\n",
      "Epoch 270: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.3017 - binary_accuracy: 0.8666 - val_loss: 0.2750 - val_binary_accuracy: 0.8817\n",
      "Epoch 271/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3017 - binary_accuracy: 0.8674\n",
      "Epoch 271: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.3022 - binary_accuracy: 0.8674 - val_loss: 0.2778 - val_binary_accuracy: 0.8812\n",
      "Epoch 272/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3016 - binary_accuracy: 0.8665\n",
      "Epoch 272: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3014 - binary_accuracy: 0.8667 - val_loss: 0.2761 - val_binary_accuracy: 0.8792\n",
      "Epoch 273/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2996 - binary_accuracy: 0.8688\n",
      "Epoch 273: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 64ms/step - loss: 0.2996 - binary_accuracy: 0.8688 - val_loss: 0.2763 - val_binary_accuracy: 0.8839\n",
      "Epoch 274/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3002 - binary_accuracy: 0.8699\n",
      "Epoch 274: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3005 - binary_accuracy: 0.8697 - val_loss: 0.2750 - val_binary_accuracy: 0.8831\n",
      "Epoch 275/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3003 - binary_accuracy: 0.8670\n",
      "Epoch 275: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3006 - binary_accuracy: 0.8671 - val_loss: 0.2761 - val_binary_accuracy: 0.8845\n",
      "Epoch 276/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3007 - binary_accuracy: 0.8679\n",
      "Epoch 276: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3011 - binary_accuracy: 0.8676 - val_loss: 0.2746 - val_binary_accuracy: 0.8798\n",
      "Epoch 277/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3004 - binary_accuracy: 0.8679\n",
      "Epoch 277: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.3007 - binary_accuracy: 0.8679 - val_loss: 0.2721 - val_binary_accuracy: 0.8848\n",
      "Epoch 278/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3021 - binary_accuracy: 0.8680\n",
      "Epoch 278: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3023 - binary_accuracy: 0.8678 - val_loss: 0.2765 - val_binary_accuracy: 0.8831\n",
      "Epoch 279/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2970 - binary_accuracy: 0.8697\n",
      "Epoch 279: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.2972 - binary_accuracy: 0.8696 - val_loss: 0.2769 - val_binary_accuracy: 0.8762\n",
      "Epoch 280/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3015 - binary_accuracy: 0.8693\n",
      "Epoch 280: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 85ms/step - loss: 0.3015 - binary_accuracy: 0.8693 - val_loss: 0.2752 - val_binary_accuracy: 0.8837\n",
      "Epoch 281/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.3015 - binary_accuracy: 0.8676\n",
      "Epoch 281: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.3015 - binary_accuracy: 0.8676 - val_loss: 0.2762 - val_binary_accuracy: 0.8803\n",
      "Epoch 282/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3013 - binary_accuracy: 0.8675\n",
      "Epoch 282: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 65ms/step - loss: 0.3009 - binary_accuracy: 0.8675 - val_loss: 0.2724 - val_binary_accuracy: 0.8837\n",
      "Epoch 283/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2983 - binary_accuracy: 0.8691\n",
      "Epoch 283: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.2979 - binary_accuracy: 0.8691 - val_loss: 0.2717 - val_binary_accuracy: 0.8842\n",
      "Epoch 284/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2998 - binary_accuracy: 0.8700\n",
      "Epoch 284: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3000 - binary_accuracy: 0.8700 - val_loss: 0.2784 - val_binary_accuracy: 0.8801\n",
      "Epoch 285/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3005 - binary_accuracy: 0.8673\n",
      "Epoch 285: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 62ms/step - loss: 0.3003 - binary_accuracy: 0.8672 - val_loss: 0.2738 - val_binary_accuracy: 0.8834\n",
      "Epoch 286/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3011 - binary_accuracy: 0.8681\n",
      "Epoch 286: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 3s 61ms/step - loss: 0.3014 - binary_accuracy: 0.8679 - val_loss: 0.2729 - val_binary_accuracy: 0.8853\n",
      "Epoch 287/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.3028 - binary_accuracy: 0.8675\n",
      "Epoch 287: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 3s 60ms/step - loss: 0.3028 - binary_accuracy: 0.8676 - val_loss: 0.2769 - val_binary_accuracy: 0.8848\n",
      "Epoch 288/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2975 - binary_accuracy: 0.8702\n",
      "Epoch 288: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 0.2976 - binary_accuracy: 0.8700 - val_loss: 0.2730 - val_binary_accuracy: 0.8842\n",
      "Epoch 289/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2949 - binary_accuracy: 0.8695\n",
      "Epoch 289: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2949 - binary_accuracy: 0.8695 - val_loss: 0.2743 - val_binary_accuracy: 0.8781\n",
      "Epoch 290/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2965 - binary_accuracy: 0.8688\n",
      "Epoch 290: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.2965 - binary_accuracy: 0.8688 - val_loss: 0.2730 - val_binary_accuracy: 0.8842\n",
      "Epoch 291/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2973 - binary_accuracy: 0.8694\n",
      "Epoch 291: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 92ms/step - loss: 0.2973 - binary_accuracy: 0.8694 - val_loss: 0.2747 - val_binary_accuracy: 0.8806\n",
      "Epoch 292/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2976 - binary_accuracy: 0.8678\n",
      "Epoch 292: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.2976 - binary_accuracy: 0.8678 - val_loss: 0.2769 - val_binary_accuracy: 0.8809\n",
      "Epoch 293/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2969 - binary_accuracy: 0.8708\n",
      "Epoch 293: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2970 - binary_accuracy: 0.8704 - val_loss: 0.2740 - val_binary_accuracy: 0.8842\n",
      "Epoch 294/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2994 - binary_accuracy: 0.8677\n",
      "Epoch 294: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 0.3001 - binary_accuracy: 0.8676 - val_loss: 0.2715 - val_binary_accuracy: 0.8817\n",
      "Epoch 295/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2938 - binary_accuracy: 0.8715\n",
      "Epoch 295: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2938 - binary_accuracy: 0.8715 - val_loss: 0.2715 - val_binary_accuracy: 0.8825\n",
      "Epoch 296/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2997 - binary_accuracy: 0.8678\n",
      "Epoch 296: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2997 - binary_accuracy: 0.8678 - val_loss: 0.2768 - val_binary_accuracy: 0.8812\n",
      "Epoch 297/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2979 - binary_accuracy: 0.8690\n",
      "Epoch 297: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2979 - binary_accuracy: 0.8690 - val_loss: 0.2758 - val_binary_accuracy: 0.8812\n",
      "Epoch 298/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2987 - binary_accuracy: 0.8681\n",
      "Epoch 298: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2988 - binary_accuracy: 0.8682 - val_loss: 0.2736 - val_binary_accuracy: 0.8825\n",
      "Epoch 299/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2962 - binary_accuracy: 0.8703\n",
      "Epoch 299: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2958 - binary_accuracy: 0.8703 - val_loss: 0.2735 - val_binary_accuracy: 0.8848\n",
      "Epoch 300/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2955 - binary_accuracy: 0.8691\n",
      "Epoch 300: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2957 - binary_accuracy: 0.8692 - val_loss: 0.2813 - val_binary_accuracy: 0.8845\n",
      "Epoch 301/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2969 - binary_accuracy: 0.8683\n",
      "Epoch 301: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2967 - binary_accuracy: 0.8686 - val_loss: 0.2717 - val_binary_accuracy: 0.8839\n",
      "Epoch 302/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2935 - binary_accuracy: 0.8710\n",
      "Epoch 302: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2938 - binary_accuracy: 0.8710 - val_loss: 0.2765 - val_binary_accuracy: 0.8787\n",
      "Epoch 303/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2970 - binary_accuracy: 0.8705\n",
      "Epoch 303: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2967 - binary_accuracy: 0.8706 - val_loss: 0.2773 - val_binary_accuracy: 0.8792\n",
      "Epoch 304/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2946 - binary_accuracy: 0.8698\n",
      "Epoch 304: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2950 - binary_accuracy: 0.8697 - val_loss: 0.2761 - val_binary_accuracy: 0.8798\n",
      "Epoch 305/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2975 - binary_accuracy: 0.8698\n",
      "Epoch 305: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2975 - binary_accuracy: 0.8698 - val_loss: 0.2724 - val_binary_accuracy: 0.8848\n",
      "Epoch 306/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2914 - binary_accuracy: 0.8726\n",
      "Epoch 306: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2918 - binary_accuracy: 0.8725 - val_loss: 0.2722 - val_binary_accuracy: 0.8848\n",
      "Epoch 307/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2967 - binary_accuracy: 0.8695\n",
      "Epoch 307: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2968 - binary_accuracy: 0.8695 - val_loss: 0.2780 - val_binary_accuracy: 0.8850\n",
      "Epoch 308/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2917 - binary_accuracy: 0.8721\n",
      "Epoch 308: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2918 - binary_accuracy: 0.8722 - val_loss: 0.2765 - val_binary_accuracy: 0.8784\n",
      "Epoch 309/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2949 - binary_accuracy: 0.8697\n",
      "Epoch 309: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2949 - binary_accuracy: 0.8697 - val_loss: 0.2727 - val_binary_accuracy: 0.8848\n",
      "Epoch 310/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2941 - binary_accuracy: 0.8708\n",
      "Epoch 310: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2941 - binary_accuracy: 0.8708 - val_loss: 0.2747 - val_binary_accuracy: 0.8837\n",
      "Epoch 311/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2930 - binary_accuracy: 0.8735\n",
      "Epoch 311: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2937 - binary_accuracy: 0.8732 - val_loss: 0.2757 - val_binary_accuracy: 0.8823\n",
      "Epoch 312/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2941 - binary_accuracy: 0.8724\n",
      "Epoch 312: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2941 - binary_accuracy: 0.8724 - val_loss: 0.2730 - val_binary_accuracy: 0.8861\n",
      "Epoch 313/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2935 - binary_accuracy: 0.8727\n",
      "Epoch 313: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2935 - binary_accuracy: 0.8727 - val_loss: 0.2789 - val_binary_accuracy: 0.8781\n",
      "Epoch 314/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2929 - binary_accuracy: 0.8721\n",
      "Epoch 314: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2929 - binary_accuracy: 0.8721 - val_loss: 0.2758 - val_binary_accuracy: 0.8801\n",
      "Epoch 315/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2940 - binary_accuracy: 0.8698\n",
      "Epoch 315: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2940 - binary_accuracy: 0.8698 - val_loss: 0.2709 - val_binary_accuracy: 0.8853\n",
      "Epoch 316/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2895 - binary_accuracy: 0.8738\n",
      "Epoch 316: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2895 - binary_accuracy: 0.8738 - val_loss: 0.2777 - val_binary_accuracy: 0.8845\n",
      "Epoch 317/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2917 - binary_accuracy: 0.8724\n",
      "Epoch 317: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2917 - binary_accuracy: 0.8724 - val_loss: 0.2721 - val_binary_accuracy: 0.8853\n",
      "Epoch 318/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2952 - binary_accuracy: 0.8722\n",
      "Epoch 318: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2964 - binary_accuracy: 0.8719 - val_loss: 0.2718 - val_binary_accuracy: 0.8837\n",
      "Epoch 319/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2918 - binary_accuracy: 0.8697\n",
      "Epoch 319: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2918 - binary_accuracy: 0.8697 - val_loss: 0.2743 - val_binary_accuracy: 0.8837\n",
      "Epoch 320/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2888 - binary_accuracy: 0.8708\n",
      "Epoch 320: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2893 - binary_accuracy: 0.8704 - val_loss: 0.2727 - val_binary_accuracy: 0.8845\n",
      "Epoch 321/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2933 - binary_accuracy: 0.8705\n",
      "Epoch 321: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2928 - binary_accuracy: 0.8708 - val_loss: 0.2722 - val_binary_accuracy: 0.8864\n",
      "Epoch 322/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2935 - binary_accuracy: 0.8707\n",
      "Epoch 322: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2935 - binary_accuracy: 0.8707 - val_loss: 0.2741 - val_binary_accuracy: 0.8870\n",
      "Epoch 323/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2915 - binary_accuracy: 0.8720\n",
      "Epoch 323: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2915 - binary_accuracy: 0.8720 - val_loss: 0.2708 - val_binary_accuracy: 0.8831\n",
      "Epoch 324/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2883 - binary_accuracy: 0.8729\n",
      "Epoch 324: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.2883 - binary_accuracy: 0.8729 - val_loss: 0.2775 - val_binary_accuracy: 0.8842\n",
      "Epoch 325/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2909 - binary_accuracy: 0.8726\n",
      "Epoch 325: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 7s 115ms/step - loss: 0.2909 - binary_accuracy: 0.8726 - val_loss: 0.2734 - val_binary_accuracy: 0.8825\n",
      "Epoch 326/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2906 - binary_accuracy: 0.8705\n",
      "Epoch 326: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 86ms/step - loss: 0.2906 - binary_accuracy: 0.8705 - val_loss: 0.2691 - val_binary_accuracy: 0.8834\n",
      "Epoch 327/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2888 - binary_accuracy: 0.8709\n",
      "Epoch 327: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.2888 - binary_accuracy: 0.8709 - val_loss: 0.2719 - val_binary_accuracy: 0.8848\n",
      "Epoch 328/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2912 - binary_accuracy: 0.8701\n",
      "Epoch 328: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.2912 - binary_accuracy: 0.8701 - val_loss: 0.2749 - val_binary_accuracy: 0.8825\n",
      "Epoch 329/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2873 - binary_accuracy: 0.8747\n",
      "Epoch 329: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2873 - binary_accuracy: 0.8745 - val_loss: 0.2731 - val_binary_accuracy: 0.8853\n",
      "Epoch 330/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2930 - binary_accuracy: 0.8711\n",
      "Epoch 330: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2930 - binary_accuracy: 0.8711 - val_loss: 0.2727 - val_binary_accuracy: 0.8834\n",
      "Epoch 331/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2925 - binary_accuracy: 0.8720\n",
      "Epoch 331: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2925 - binary_accuracy: 0.8723 - val_loss: 0.2707 - val_binary_accuracy: 0.8859\n",
      "Epoch 332/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2885 - binary_accuracy: 0.8753\n",
      "Epoch 332: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2885 - binary_accuracy: 0.8751 - val_loss: 0.2764 - val_binary_accuracy: 0.8817\n",
      "Epoch 333/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2919 - binary_accuracy: 0.8712\n",
      "Epoch 333: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 6s 113ms/step - loss: 0.2916 - binary_accuracy: 0.8711 - val_loss: 0.2731 - val_binary_accuracy: 0.8823\n",
      "Epoch 334/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2857 - binary_accuracy: 0.8752\n",
      "Epoch 334: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 68ms/step - loss: 0.2857 - binary_accuracy: 0.8752 - val_loss: 0.2754 - val_binary_accuracy: 0.8803\n",
      "Epoch 335/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2899 - binary_accuracy: 0.8729\n",
      "Epoch 335: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 6s 114ms/step - loss: 0.2906 - binary_accuracy: 0.8726 - val_loss: 0.2701 - val_binary_accuracy: 0.8864\n",
      "Epoch 336/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2903 - binary_accuracy: 0.8719\n",
      "Epoch 336: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2900 - binary_accuracy: 0.8722 - val_loss: 0.2785 - val_binary_accuracy: 0.8834\n",
      "Epoch 337/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2913 - binary_accuracy: 0.8715\n",
      "Epoch 337: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 69ms/step - loss: 0.2917 - binary_accuracy: 0.8711 - val_loss: 0.2739 - val_binary_accuracy: 0.8828\n",
      "Epoch 338/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2851 - binary_accuracy: 0.8736\n",
      "Epoch 338: val_binary_accuracy did not improve from 0.88696\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.2851 - binary_accuracy: 0.8736 - val_loss: 0.2750 - val_binary_accuracy: 0.8850\n",
      "Epoch 339/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2908 - binary_accuracy: 0.8720\n",
      "Epoch 339: val_binary_accuracy improved from 0.88696 to 0.88751, saving model to model.hdf5\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.2908 - binary_accuracy: 0.8720 - val_loss: 0.2751 - val_binary_accuracy: 0.8875\n",
      "Epoch 340/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2890 - binary_accuracy: 0.8737\n",
      "Epoch 340: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2890 - binary_accuracy: 0.8737 - val_loss: 0.2784 - val_binary_accuracy: 0.8842\n",
      "Epoch 341/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2891 - binary_accuracy: 0.8740\n",
      "Epoch 341: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2891 - binary_accuracy: 0.8740 - val_loss: 0.2708 - val_binary_accuracy: 0.8853\n",
      "Epoch 342/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2857 - binary_accuracy: 0.8736\n",
      "Epoch 342: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2860 - binary_accuracy: 0.8736 - val_loss: 0.2715 - val_binary_accuracy: 0.8839\n",
      "Epoch 343/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2875 - binary_accuracy: 0.8721\n",
      "Epoch 343: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2876 - binary_accuracy: 0.8721 - val_loss: 0.2734 - val_binary_accuracy: 0.8848\n",
      "Epoch 344/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2862 - binary_accuracy: 0.8747\n",
      "Epoch 344: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2862 - binary_accuracy: 0.8747 - val_loss: 0.2739 - val_binary_accuracy: 0.8848\n",
      "Epoch 345/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2878 - binary_accuracy: 0.8733\n",
      "Epoch 345: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.2878 - binary_accuracy: 0.8733 - val_loss: 0.2735 - val_binary_accuracy: 0.8870\n",
      "Epoch 346/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2869 - binary_accuracy: 0.8742\n",
      "Epoch 346: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.2869 - binary_accuracy: 0.8742 - val_loss: 0.2744 - val_binary_accuracy: 0.8817\n",
      "Epoch 347/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2873 - binary_accuracy: 0.8721\n",
      "Epoch 347: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.2873 - binary_accuracy: 0.8721 - val_loss: 0.2784 - val_binary_accuracy: 0.8875\n",
      "Epoch 348/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2887 - binary_accuracy: 0.8761\n",
      "Epoch 348: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.2887 - binary_accuracy: 0.8761 - val_loss: 0.2719 - val_binary_accuracy: 0.8872\n",
      "Epoch 349/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2885 - binary_accuracy: 0.8756\n",
      "Epoch 349: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2878 - binary_accuracy: 0.8759 - val_loss: 0.2768 - val_binary_accuracy: 0.8812\n",
      "Epoch 350/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2899 - binary_accuracy: 0.8714\n",
      "Epoch 350: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 7s 119ms/step - loss: 0.2899 - binary_accuracy: 0.8714 - val_loss: 0.2775 - val_binary_accuracy: 0.8845\n",
      "Epoch 351/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2847 - binary_accuracy: 0.8739\n",
      "Epoch 351: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2847 - binary_accuracy: 0.8739 - val_loss: 0.2701 - val_binary_accuracy: 0.8823\n",
      "Epoch 352/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2902 - binary_accuracy: 0.8702\n",
      "Epoch 352: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.2902 - binary_accuracy: 0.8702 - val_loss: 0.2751 - val_binary_accuracy: 0.8848\n",
      "Epoch 353/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2857 - binary_accuracy: 0.8733\n",
      "Epoch 353: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.2857 - binary_accuracy: 0.8733 - val_loss: 0.2745 - val_binary_accuracy: 0.8837\n",
      "Epoch 354/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2870 - binary_accuracy: 0.8715\n",
      "Epoch 354: val_binary_accuracy did not improve from 0.88751\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.2870 - binary_accuracy: 0.8715 - val_loss: 0.2737 - val_binary_accuracy: 0.8837\n",
      "Epoch 355/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2848 - binary_accuracy: 0.8757\n",
      "Epoch 355: val_binary_accuracy improved from 0.88751 to 0.88999, saving model to model.hdf5\n",
      "57/57 [==============================] - 7s 128ms/step - loss: 0.2848 - binary_accuracy: 0.8757 - val_loss: 0.2703 - val_binary_accuracy: 0.8900\n",
      "Epoch 356/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2857 - binary_accuracy: 0.8747\n",
      "Epoch 356: val_binary_accuracy improved from 0.88999 to 0.89027, saving model to model.hdf5\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.2857 - binary_accuracy: 0.8747 - val_loss: 0.2713 - val_binary_accuracy: 0.8903\n",
      "Epoch 357/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2814 - binary_accuracy: 0.8752\n",
      "Epoch 357: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.2814 - binary_accuracy: 0.8752 - val_loss: 0.2738 - val_binary_accuracy: 0.8881\n",
      "Epoch 358/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2822 - binary_accuracy: 0.8758\n",
      "Epoch 358: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.2817 - binary_accuracy: 0.8760 - val_loss: 0.2744 - val_binary_accuracy: 0.8842\n",
      "Epoch 359/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2860 - binary_accuracy: 0.8750\n",
      "Epoch 359: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2860 - binary_accuracy: 0.8750 - val_loss: 0.2713 - val_binary_accuracy: 0.8903\n",
      "Epoch 360/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2826 - binary_accuracy: 0.8753\n",
      "Epoch 360: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2828 - binary_accuracy: 0.8753 - val_loss: 0.2717 - val_binary_accuracy: 0.8861\n",
      "Epoch 361/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2833 - binary_accuracy: 0.8769\n",
      "Epoch 361: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2834 - binary_accuracy: 0.8767 - val_loss: 0.2703 - val_binary_accuracy: 0.8870\n",
      "Epoch 362/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2833 - binary_accuracy: 0.8754\n",
      "Epoch 362: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2833 - binary_accuracy: 0.8754 - val_loss: 0.2736 - val_binary_accuracy: 0.8823\n",
      "Epoch 363/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2814 - binary_accuracy: 0.8757\n",
      "Epoch 363: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2809 - binary_accuracy: 0.8761 - val_loss: 0.2738 - val_binary_accuracy: 0.8845\n",
      "Epoch 364/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2881 - binary_accuracy: 0.8743\n",
      "Epoch 364: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.2881 - binary_accuracy: 0.8743 - val_loss: 0.2741 - val_binary_accuracy: 0.8848\n",
      "Epoch 365/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2838 - binary_accuracy: 0.8754\n",
      "Epoch 365: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.2838 - binary_accuracy: 0.8754 - val_loss: 0.2716 - val_binary_accuracy: 0.8870\n",
      "Epoch 366/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2848 - binary_accuracy: 0.8747\n",
      "Epoch 366: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2848 - binary_accuracy: 0.8747 - val_loss: 0.2677 - val_binary_accuracy: 0.8861\n",
      "Epoch 367/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2798 - binary_accuracy: 0.8762\n",
      "Epoch 367: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2797 - binary_accuracy: 0.8762 - val_loss: 0.2759 - val_binary_accuracy: 0.8848\n",
      "Epoch 368/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2800 - binary_accuracy: 0.8767\n",
      "Epoch 368: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.2800 - binary_accuracy: 0.8767 - val_loss: 0.2744 - val_binary_accuracy: 0.8861\n",
      "Epoch 369/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2838 - binary_accuracy: 0.8741\n",
      "Epoch 369: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2838 - binary_accuracy: 0.8741 - val_loss: 0.2730 - val_binary_accuracy: 0.8859\n",
      "Epoch 370/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2829 - binary_accuracy: 0.8758\n",
      "Epoch 370: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2834 - binary_accuracy: 0.8754 - val_loss: 0.2787 - val_binary_accuracy: 0.8828\n",
      "Epoch 371/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2821 - binary_accuracy: 0.8761\n",
      "Epoch 371: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2821 - binary_accuracy: 0.8761 - val_loss: 0.2722 - val_binary_accuracy: 0.8817\n",
      "Epoch 372/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2767 - binary_accuracy: 0.8776\n",
      "Epoch 372: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.2767 - binary_accuracy: 0.8776 - val_loss: 0.2743 - val_binary_accuracy: 0.8809\n",
      "Epoch 373/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2850 - binary_accuracy: 0.8750\n",
      "Epoch 373: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2856 - binary_accuracy: 0.8748 - val_loss: 0.2707 - val_binary_accuracy: 0.8823\n",
      "Epoch 374/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2841 - binary_accuracy: 0.8738\n",
      "Epoch 374: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2838 - binary_accuracy: 0.8738 - val_loss: 0.2758 - val_binary_accuracy: 0.8839\n",
      "Epoch 375/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2802 - binary_accuracy: 0.8762\n",
      "Epoch 375: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2802 - binary_accuracy: 0.8762 - val_loss: 0.2780 - val_binary_accuracy: 0.8839\n",
      "Epoch 376/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2803 - binary_accuracy: 0.8749\n",
      "Epoch 376: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2807 - binary_accuracy: 0.8747 - val_loss: 0.2782 - val_binary_accuracy: 0.8801\n",
      "Epoch 377/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2801 - binary_accuracy: 0.8757\n",
      "Epoch 377: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2801 - binary_accuracy: 0.8757 - val_loss: 0.2748 - val_binary_accuracy: 0.8820\n",
      "Epoch 378/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2795 - binary_accuracy: 0.8783\n",
      "Epoch 378: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.2795 - binary_accuracy: 0.8783 - val_loss: 0.2771 - val_binary_accuracy: 0.8878\n",
      "Epoch 379/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2800 - binary_accuracy: 0.8772\n",
      "Epoch 379: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.2800 - binary_accuracy: 0.8772 - val_loss: 0.2749 - val_binary_accuracy: 0.8814\n",
      "Epoch 380/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2785 - binary_accuracy: 0.8765\n",
      "Epoch 380: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2785 - binary_accuracy: 0.8765 - val_loss: 0.2731 - val_binary_accuracy: 0.8823\n",
      "Epoch 381/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2756 - binary_accuracy: 0.8774\n",
      "Epoch 381: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 5s 86ms/step - loss: 0.2756 - binary_accuracy: 0.8774 - val_loss: 0.2760 - val_binary_accuracy: 0.8889\n",
      "Epoch 382/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2801 - binary_accuracy: 0.8763\n",
      "Epoch 382: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2799 - binary_accuracy: 0.8764 - val_loss: 0.2746 - val_binary_accuracy: 0.8812\n",
      "Epoch 383/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2812 - binary_accuracy: 0.8755\n",
      "Epoch 383: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2812 - binary_accuracy: 0.8755 - val_loss: 0.2776 - val_binary_accuracy: 0.8817\n",
      "Epoch 384/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2770 - binary_accuracy: 0.8779\n",
      "Epoch 384: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 5s 84ms/step - loss: 0.2770 - binary_accuracy: 0.8779 - val_loss: 0.2707 - val_binary_accuracy: 0.8870\n",
      "Epoch 385/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2787 - binary_accuracy: 0.8757\n",
      "Epoch 385: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 5s 96ms/step - loss: 0.2787 - binary_accuracy: 0.8757 - val_loss: 0.2724 - val_binary_accuracy: 0.8842\n",
      "Epoch 386/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2774 - binary_accuracy: 0.8791\n",
      "Epoch 386: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.2774 - binary_accuracy: 0.8791 - val_loss: 0.2705 - val_binary_accuracy: 0.8839\n",
      "Epoch 387/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2850 - binary_accuracy: 0.8740\n",
      "Epoch 387: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2846 - binary_accuracy: 0.8742 - val_loss: 0.2730 - val_binary_accuracy: 0.8823\n",
      "Epoch 388/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2785 - binary_accuracy: 0.8773\n",
      "Epoch 388: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.2785 - binary_accuracy: 0.8773 - val_loss: 0.2782 - val_binary_accuracy: 0.8839\n",
      "Epoch 389/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2784 - binary_accuracy: 0.8779\n",
      "Epoch 389: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.2785 - binary_accuracy: 0.8779 - val_loss: 0.2730 - val_binary_accuracy: 0.8845\n",
      "Epoch 390/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2818 - binary_accuracy: 0.8737\n",
      "Epoch 390: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2818 - binary_accuracy: 0.8737 - val_loss: 0.2760 - val_binary_accuracy: 0.8812\n",
      "Epoch 391/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2776 - binary_accuracy: 0.8762\n",
      "Epoch 391: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2774 - binary_accuracy: 0.8765 - val_loss: 0.2749 - val_binary_accuracy: 0.8839\n",
      "Epoch 392/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2773 - binary_accuracy: 0.8789\n",
      "Epoch 392: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2773 - binary_accuracy: 0.8789 - val_loss: 0.2781 - val_binary_accuracy: 0.8872\n",
      "Epoch 393/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2745 - binary_accuracy: 0.8780\n",
      "Epoch 393: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2745 - binary_accuracy: 0.8780 - val_loss: 0.2729 - val_binary_accuracy: 0.8856\n",
      "Epoch 394/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2768 - binary_accuracy: 0.8805\n",
      "Epoch 394: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 79ms/step - loss: 0.2768 - binary_accuracy: 0.8805 - val_loss: 0.2777 - val_binary_accuracy: 0.8864\n",
      "Epoch 395/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2792 - binary_accuracy: 0.8785\n",
      "Epoch 395: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2790 - binary_accuracy: 0.8784 - val_loss: 0.2745 - val_binary_accuracy: 0.8850\n",
      "Epoch 396/400\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.2796 - binary_accuracy: 0.8773\n",
      "Epoch 396: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2798 - binary_accuracy: 0.8772 - val_loss: 0.2757 - val_binary_accuracy: 0.8837\n",
      "Epoch 397/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2761 - binary_accuracy: 0.8776\n",
      "Epoch 397: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.2761 - binary_accuracy: 0.8776 - val_loss: 0.2760 - val_binary_accuracy: 0.8853\n",
      "Epoch 398/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2782 - binary_accuracy: 0.8761\n",
      "Epoch 398: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.2782 - binary_accuracy: 0.8761 - val_loss: 0.2797 - val_binary_accuracy: 0.8878\n",
      "Epoch 399/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2796 - binary_accuracy: 0.8773\n",
      "Epoch 399: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2796 - binary_accuracy: 0.8773 - val_loss: 0.2718 - val_binary_accuracy: 0.8853\n",
      "Epoch 400/400\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2785 - binary_accuracy: 0.8773\n",
      "Epoch 400: val_binary_accuracy did not improve from 0.89027\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.2785 - binary_accuracy: 0.8773 - val_loss: 0.2706 - val_binary_accuracy: 0.8889\n",
      "1559.0500780000584\n",
      "Best validation model: epoch 356  - val_binary_accuracy 0.8902674317359924\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))\n",
    "print (time.perf_counter() - start)\n",
    "best_idx = int(np.argmax(history.history['val_binary_accuracy']))\n",
    "best_value = np.max(history.history['val_binary_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_binary_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=pd.DataFrame(history.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.savefig(f\"./images/loss{n_neurons_per_hlayer}_{seed}_{n_epochs}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_binary_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.278461</td>\n",
       "      <td>0.877309</td>\n",
       "      <td>0.270615</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  binary_accuracy  val_loss  val_binary_accuracy\n",
       "399  0.278461         0.877309  0.270615             0.888889"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the training set: 0.8773090839385986\n",
      "Accuracy for the development test set: 0.8888888955116272\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\")\n",
    "print(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = open(\"./history/DeepFeedforward.txt\", \"a\")\n",
    "v.write(f\"Epoque: {n_epochs}\\n\")\n",
    "v.write(f\"Learning Rate: {lr}\\n\")\n",
    "v.write(f\"Batch Size: {batch_size}\\n\")\n",
    "v.write(f\"Dropout: {tasa_dropout}\\n\")\n",
    "v.write(f\"Neurons per layer: {n_neurons_per_hlayer}\\n\")\n",
    "v.write(f\"Activation: elu\\n\")\n",
    "v.write(f\"Optimizer: Adam\\n\")\n",
    "v.write(f\"seed = {seed}\\n\")\n",
    "v.write(\n",
    "    \"model.add(keras.layers.Dense(neurons, kernel_initializer=he_normal, use_bias=False)); model.add(keras.layers.BatchNormalization()); model.add(keras.layers.Activation(elu)); model.add(tf.keras.layers.Dropout(tasa_dropout))\"\n",
    ")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.write(f\"Accuracy for the training set: {results.binary_accuracy.values[-1:][0]}\\n\")\n",
    "v.write(\n",
    "    f\"Accuracy for the development test set: {results.val_binary_accuracy.values[-1:][0]}\\n\"\n",
    ")\n",
    "v.write(f\"Best validation model: epoch {best_idx+1} - val_binary_accuracy {best_value}\\n\")\n",
    "v.write(f\"Time: {time.perf_counter() - start}\\n\")\n",
    "v.write(\n",
    "    \"--------------------------------------------------------------------------------------------\\n\"\n",
    ")\n",
    "v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).round()\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
